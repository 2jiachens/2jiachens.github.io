<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="我的博客">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="我的博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="宋嘉晨">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>我的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">我的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-guestbook">

    <a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言板</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/10/WGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/10/WGAN/" class="post-title-link" itemprop="url">WGAN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-10 09:20:32" itemprop="dateCreated datePublished" datetime="2025-03-10T09:20:32+08:00">2025-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-11 12:31:41" itemprop="dateModified" datetime="2025-03-11T12:31:41+08:00">2025-03-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文精读</span></a>
                </span>
            </span>

          
            <span id="/2025/03/10/WGAN/" class="post-meta-item leancloud_visitors" data-flag-title="WGAN" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/03/10/WGAN/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/10/WGAN/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="wasserstein-gan">Wasserstein GAN</h1>
<h2 id="介绍">介绍</h2>
<p>本文关心的问题是无监督学习问题。主要是，学习一个概率分布意味着什么？对此，经典的答案是学习一个概率密度。这通常是通过定义一个参数密度族<span class="math inline">\(\left(P_\theta\right)_{\theta \in
\mathbb{R}^d}\)</span>，并在数据上找到使似然函数最大化的密度族：如果有真实的数据样本<span class="math inline">\(\left\{x^{(i)}\right\}_{i-1}^m\)</span>，我们就可以解决这个问题。</p>
<p><span class="math display">\[
\max _{\theta \in \mathbb{R}^d} \frac{1}{m} \sum_{i=1}^m \log
P_\theta\left(x^{(i)}\right)
\]</span></p>
<p>如果真实数据分布<span class="math inline">\(\mathbb{P}_r\)</span>具有密度，<span class="math inline">\(\mathbb{P}_\theta\)</span>是参数化密度<span class="math inline">\(P_\theta\)</span>的分布，那么渐近地，这相当于最小化Kullback-Leibler散度<span class="math inline">\(KL\left(\mathbb{P}_r \|
\mathbb{P}_\theta\right)\)</span></p>
<p>为此，需要模型密度<span class="math inline">\(P_\theta\)</span>存在。在我们处理由低维流形支撑的分布时，这种情况并不常见。那么模型流形和真实分布的支撑集不太可能有不可忽略（测度大于0）的交集[1]，这意味着KL距离没有定义(或者简单地无限)。</p>
<p>典型的补救方法是在模型分布中加入噪声项。这就是为什么在经典的机器学习文献中描述的几乎所有的生成模型都包含了噪声成分。在最简单的情况下，为了覆盖所有的例子，假设高斯噪声具有相对较大的带宽。然而，在图像生成模型中，这种噪声会降低样本的质量，使其变得模糊。例如，在文献[23]中，我们可以看到当生成图像的像素已经归一化到<span class="math inline">\([0,1]\)</span>范围内时，当做最大似然估计时，添加到模型中的噪声的最佳标准偏差是0.1。
这是一个非常高的噪声量，以至于当论文报告他们的模型的样本时，他们没有添加他们报告似然数的噪声项。换句话说，添加的噪声项对于问题显然是不正确的，但需要使最大似然方法发挥作用。</p>
<p>我们可以定义一个固定分布<span class="math inline">\(p(z)\)</span>的随机变量<span class="math inline">\(Z\)</span>，并通过一个参数函数<span class="math inline">\(g_\theta: \mathcal{Z} \rightarrow
\mathcal{X}\)</span>
(通常是某种类型的神经网络)，直接生成服从某一分布<span class="math inline">\(\mathbb{P}_\theta\)</span>的样本，而不是估计可能不存在的<span class="math inline">\(\mathbb{P}_r\)</span>的密度。通过改变<span class="math inline">\(\theta\)</span>，可以改变这种分布，使其接近真实的数据分布<span class="math inline">\(\mathbb{P}_r\)</span>。这在两个方面是有用的。首先，与密度不同，这种方法可以表示限制在低维流形上的分布。其次，容易生成样本的能力往往比知道密度的数值更有用。
一般而言，给定任意高维密度生成样本在计算上是困难的[16]。</p>
<p>变分自编码器(Variational Auto-Encoders，VAEs)
[9]和生成式对抗网络(Generative Adversarial Networks，GANs)
[4]就是这种方法的典型代表。由于VAEs关注的是样本的近似似然，因此它们具有标准模型的局限性，需要处理额外的噪声项。GANs在目标函数的定义上提供了更多的灵活性，包括Jensen-Shannon
[4]，所有的f-散度[17]以及一些奇异组合[6]。另一方面，训练GANs是众所周知的微妙和不稳定的，原因在理论上研究[1]。</p>
<p>在这篇文章中，我们将注意力集中在用不同的方法来度量模型分布和真实分布的接近程度，或者等价地，用不同的方法来定义一个距离或散度<span class="math inline">\(\rho\left(\mathbb{P}_\theta,
\mathbb{P}_r\right)\)</span>。这种距离之间最根本的区别在于它们对概率分布序列收敛性的影响。一个分布列<span class="math inline">\(\left(\mathbb{P}_t\right)_{t \in
\mathbb{N}}\)</span>收敛当且仅当存在一个分布<span class="math inline">\(\mathbb{P}_{\infty}\)</span>，使得<span class="math inline">\(\rho\left(\mathbb{P}_t,
\mathbb{P}_{\infty}\right)\)</span>趋向于零，这取决于距离<span class="math inline">\(\rho\)</span>定义的精确程度。不严格的说，当距离<span class="math inline">\(\rho\)</span>使得一个分布序列更容易收敛时，就会产生一个较弱的拓扑结构。（当<span class="math inline">\(\rho&#39;\)</span>下的收敛序列集是<span class="math inline">\(\rho\)</span>下收敛序列集的子集时，<span class="math inline">\(\rho\)</span>诱导的拓扑比<span class="math inline">\(\rho&#39;\)</span>诱导的拓扑弱）第2节阐明了常用概率距离在这方面的差异。</p>
<p>为了优化参数<span class="math inline">\(\theta\)</span>，需要使映射<span class="math inline">\(\theta \mapsto
\mathbb{P}_\theta\)</span>以连续的方式定义我们的模型分布<span class="math inline">\(\mathbb{P}_\theta\)</span>。连续性是指当参数序列<span class="math inline">\(\theta_t\)</span>收敛于<span class="math inline">\(\theta\)</span>时，分布<span class="math inline">\(\mathbb{P}_{\theta_t}\)</span>也收敛于<span class="math inline">\(\mathbb{P}_\theta\)</span>。然而，需要注意的是，分布<span class="math inline">\(\mathbb{P}_{\theta_t}\)</span>收敛的概念取决于我们计算分布之间距离的方式。这个距离越弱，就越容易定义一个从<span class="math inline">\(\theta\)</span>空间到<span class="math inline">\(\mathbb{P}_\theta\)</span>空间的连续映射，因为它的分布就越容易收敛。我们关心映射<span class="math inline">\(\theta \mapsto
\mathbb{P}_\theta\)</span>连续的主要原因如下. 如果<span class="math inline">\(\rho\)</span>是我们关于两个分布之间的距离的概念，我们希望有一个连续的损失函数<span class="math inline">\(\theta \mapsto \rho\left(\mathbb{P}_\theta,
\mathbb{P}_r\right)\)</span>，这相当于在使用分布之间的距离<span class="math inline">\(\rho\)</span>时，映射<span class="math inline">\(\theta \mapsto
\mathbb{P}_\theta\)</span>是连续的。</p>
<p>本文的贡献在于：
1.我们提供了一个全面的理论分析，与学习分布中使用的流行概率距离和散度相比，Earth
Mover（EM）距离是如何表现的</p>
<p>2.在第3节中，我们定义了一种称为Wasserstein
GAN的GAN形式，该GAN使EM距离的合理有效近似最小化，我们从理论上证明了相应的优化问题是合理的</p>
<p>3.在第4节中，我们实证表明，WGAN解决了GAN的主要训练问题。特别是，训练WGAN不需要在训练鉴别器和生成器时保持谨慎的平衡，也不需要仔细设计网络架构。GAN中典型的模式下降现象也大大减少。WGAN最引人注目的实际好处之一是能够通过将鉴别器训练到最优性来连续估计EM距离。绘制这些学习曲线不仅有助于调试和超参数搜索，而且与观察到的样本质量有很好的相关性。</p>
<h3 id="towards-principled-methods-for-training-generative-adversarial-networks">[1]《Towards
principled methods for training generative adversarial networks》</h3>
<h4 id="介绍-1">介绍</h4>
<p>传统的生成式建模方法依赖于最大化似然，或者等价地最小化未知数据分布<span class="math inline">\(\mathbb{P}_r\)</span>和生成器分布<span class="math inline">\(\mathbb{P}_g\)</span>之间的KL散度。如果假设两个分布都是密度为<span class="math inline">\(P_r\)</span>和<span class="math inline">\(P_g\)</span>的连续分布，那么这些方法都试图最小化
<span class="math display">\[
K L\left(\mathbb{P}_r \| \mathbb{P}_g\right)=\int_{\mathcal{X}} P_r(x)
\log \frac{P_r(x)}{P_g(x)} \mathrm{d} x
\]</span></p>
<p>该代价函数具有在<span class="math inline">\(\mathbb{P}_g=\mathbb{P}_r\)</span>处有唯一最小值的良好性质，并且不需要知道未知的<span class="math inline">\(P_r(x)\)</span>来优化它(仅需要样本)。然而，它的发散在<span class="math inline">\(\mathbb{P}_r\)</span>r和<span class="math inline">\(\mathbb{P}_g\)</span>之间不对称（<span class="math inline">\(K L\left(\mathbb{P}_r \| \mathbb{P}_g\right) \not
= K L\left(\mathbb{P}_g \| \mathbb{P}_r\right)\)</span>）：</p>
<p>1.若<span class="math inline">\(P_r(x)&gt;P_g(x)\)</span>，则<span class="math inline">\(x\)</span>是来自数据的概率高于生成样本的点。这就是通常所说的'mode
dropping'现象的核心：存在大片区域满足<span class="math inline">\(P_r\)</span>值较大的，而<span class="math inline">\(P_g\)</span>值较小或为零。需要注意的是，当<span class="math inline">\(P_r(x)&gt;0\)</span>但<span class="math inline">\(P_g(x) \rightarrow
0\)</span>时，KL内的被积函数迅速增长到无穷大，这意味着该成本函数为没有覆盖部分数据的生成器分布分配了极高的成本（如果生成器不能完全生成全部真实样本数据，那么cost将很高）</p>
<p>2.如果<span class="math inline">\(P_r(x)&lt;P_g(x)\)</span>，则<span class="math inline">\(x\)</span>作为数据点的概率较低，但由我们的模型生成的概率很高。当我们看到生成器输出的图像看起来不真实时，就是这种情况。在这种情况下，当<span class="math inline">\(P_r(x) \rightarrow 0\)</span>并且<span class="math inline">\(P_g(x)&gt;0\)</span>时，看到KL内的值变为0，这意味着这个成本函数将为生成看起来很假的样本支付极低的成本。</p>
<p>相反，如果我们将<span class="math inline">\(K L\left(\mathbb{P}_g \|
\mathbb{P}_r\right)\)</span>最小化，这些误差的权重将被逆转，这意味着这个代价函数将付出很高的成本来生成看上去不真实的图片。生成对抗网络已经被证明可以优化这两个代价函数的对称中间点，即Jensen-shannon散度
<span class="math display">\[
J S D\left(\mathbb{P}_r \| \mathbb{P}_g\right)=\frac{1}{2} K
L\left(\mathbb{P}_r \| \mathbb{P}_A\right)+\frac{1}{2} K
L\left(\mathbb{P}_g \| \mathbb{P}_A\right)
\]</span></p>
<p>其中，<span class="math inline">\(\mathbb{P}_A\)</span>为"平均"分布，密度为<span class="math inline">\(\frac{P_r+P_g}{2}\)</span>
。可以推测，GANs成功地生成真实图像的原因是由于从传统的最大似然方法转换而来的。然而，还有问题</p>
<p>生成对抗网络的构建分为两个步骤。我们首先训练一个判别器<span class="math inline">\(D\)</span>使其最大化 <span class="math display">\[
L\left(D, g_\theta\right)=\mathbb{E}_{x \sim \mathbf{P}_r}[\log
D(x)]+\mathbb{E}_{x \sim \mathbf{P}_g}[\log (1-D(x))] \qquad(1)
\]</span></p>
<p>很容易地证明最优判别器具有形状（L对D（x）求导等于0） <span class="math display">\[
D^*(x)=\frac{P_r(x)}{P_r(x)+P_g(x)}
\]</span></p>
<p>且 <span class="math inline">\(L\left(D^*, g_\theta\right)=2 J S
D\left(\mathbb{P}_r \| \mathbb{P}_g\right)-2 \log 2\)</span>,
因此，当判别器是最优的时，最小化作为<span class="math inline">\(\theta\)</span>的函数的方程(1)等价于最小化Jensen-Shannon散度。因此，在理论上，人们期望我们首先训练尽可能接近最优的判别器(因此,
<span class="math inline">\(\theta\)</span>上的代价函数更好地逼近JSD
)，然后在<span class="math inline">\(\theta\)</span>上做梯度步骤，交替这两种情况
，然而，这并不奏效。在实际应用中，随着判别器的变好，生成器的更新也不断变差。</p>
<h4 id="不稳定的来源">不稳定的来源</h4>
<p>该理论告诉我们，训练好的判别器最好也就是<span class="math inline">\(2
\log 2-2 J S D\left(\mathbb{P}_r \|
\mathbb{P}_g\right)\)</span>。然而，在实际中，如果我们仅训练<span class="math inline">\(D\)</span>直到收敛，它的误差将趋于0，但是如图一所示，这表明它们之间的JSD是最大的。唯一可能的情况是，两个分布不是连续的，或者它们有不相交的支撑。
<img src="/2025/03/10/WGAN/WGAN\4.png" alt="image"></p>
<p>原因总结：真实数据分布的支撑集往往形成流形，而生成器的输出分布也往往是流形，而两个流形相交部分测度为0。</p>
<p>定理2.3 设<span class="math inline">\(\mathbb{P}_r\)</span>和<span class="math inline">\(\mathbb{P}_g\)</span>是两个分布，其支撑点位于两个流形<span class="math inline">\(\mathcal{M}\)</span>和<span class="math inline">\(\mathcal{P}\)</span>中，这两个流形不具有全维且不完全对齐。我们进一步假设<span class="math inline">\(\mathbb{P}_r\)</span>和<span class="math inline">\(\mathbb{P}_g\)</span>在各自的流形上是连续的，则有
<span class="math display">\[
\begin{aligned}
J S D\left(\mathbb{P}_r \| \mathbb{P}_g\right) &amp; =\log 2 \\
K L\left(\mathbb{P}_r \| \mathbb{P}_g\right) &amp; =+\infty \\
K L\left(\mathbb{P}_g \| \mathbb{P}_r\right) &amp; =+\infty
\end{aligned}
\]</span></p>
<p>值得注意的是，即使两个流形任意靠近，这些分歧也会被放大。我们的生成器的样本可能看起来很好，但两个KL散度都是无穷大，JSD为常数log2，这在训练的时候会发生梯度消失。</p>
<p>为什么log2？ <img src="/2025/03/10/WGAN/WGAN\1.jpg" alt="image"></p>
<p>回到WGAN</p>
<h2 id="不同距离">不同距离</h2>
<p>设<span class="math inline">\(\mathcal{X}\)</span>是紧致度量集(如图像的空间<span class="math inline">\([0,1]^d\)</span>)，<span class="math inline">\(\Sigma\)</span>表示<span class="math inline">\(\mathcal{X}\)</span>的所有Borel子集构成的集合，<span class="math inline">\(\operatorname{Prob}(\mathcal{X})\)</span>表示定义在<span class="math inline">\(\mathcal{X}\)</span>上的概率测度空间。定义两个分布<span class="math inline">\(\mathbb{P}_r, \mathbb{P}_g \in
\operatorname{Prob}(\mathcal{X})\)</span>之间的距离： - 全变分 (TV)
距离</p>
<p><span class="math display">\[
\delta\left(\mathbb{P}_r, \mathbb{P}_g\right)=\sup _{A \in
\Sigma}\left|\mathbb{P}_r(A)-\mathbb{P}_g(A)\right|
\]</span></p>
<ul>
<li><p>The Kullback-Leibler (KL) 散度 <span class="math display">\[
K L\left(\mathbb{P}_r \| \mathbb{P}_g\right)=\int \log
\left(\frac{P_r(x)}{P_g(x)}\right) P_r(x) d \mu(x)
\]</span></p></li>
<li><p>The Jensen-Shannon (JS) 散度 <span class="math display">\[
J S\left(\mathbb{P}_r, \mathbb{P}_g\right)=K L\left(\mathbb{P}_r \|
\mathbb{P}_m\right)+K L\left(\mathbb{P}_g \| \mathbb{P}_m\right)
\]</span></p></li>
</ul>
<p>其中 <span class="math inline">\(\mathbb{P}_m\)</span>是<span class="math inline">\(\left(\mathbb{P}_r+\mathbb{P}_g\right) /
2\)</span>的混合</p>
<ul>
<li>The Earth-Mover (EM) 距离 （Wasserstein-1 距离） <span class="math display">\[
W\left(\mathbb{P}_r, \mathbb{P}_g\right)=\inf _{\gamma \in
\Pi\left(\mathbb{P}_r, \mathbb{P}_g\right)} \mathbb{E}_{(x, y) \sim
\gamma}[\|x-y\|] \qquad(1)
\]</span></li>
</ul>
<p>其中，<span class="math inline">\(\Pi\left(\mathbb{P}_r,
\mathbb{P}_g\right)\)</span>表示所有边际分布分别为<span class="math inline">\(\mathbb{P}_r\)</span>和<span class="math inline">\(\mathbb{P}_g\)</span>的联合分布<span class="math inline">\(\gamma(x, y)\)</span>的集合。直观上，<span class="math inline">\(\gamma(x, y)\)</span>表示为了将分布<span class="math inline">\(\mathbb{P}_r\)</span>转化为分布<span class="math inline">\(\mathbb{P}_g\)</span>需要从<span class="math inline">\(x\)</span>输送到<span class="math inline">\(y\)</span>的"质量"。那么EM距离就是最优运输方案的"成本"。</p>
<p>下面的例子说明了简单的概率分布序列在EM距离下是收敛的，而在上面定义的其他距离和散度下是不收敛的。</p>
<p>例1 令<span class="math inline">\(Z \sim
U[0,1]\)</span>为单位区间上的均匀分布。设<span class="math inline">\(\mathbb{P}_0\)</span>为<span class="math inline">\((0, Z) \in
\mathbb{R}^2\)</span>上的分布(0在x轴上,随机变量<span class="math inline">\(Z\)</span>在y轴上)，在过原点的竖直线上均匀分布。现令<span class="math inline">\(g_\theta(z)=(\theta, z)\)</span>，<span class="math inline">\(\theta\)</span>为单一实参数。不难看出，在这种情况下
- <span class="math inline">\(W\left(\mathbb{P}_0,
\mathbb{P}_\theta\right)=|\theta|\)</span>, - <span class="math inline">\(J S\left(\mathbb{P}_0, \mathbb{P}_\theta\right)=
\begin{cases}\log 2 &amp; \text { if } \theta \neq 0, \\ 0 &amp; \text {
if } \theta=0,\end{cases}\)</span> - <span class="math inline">\(K
L\left(\mathbb{P}_\theta \| \mathbb{P}_0\right)=K L\left(\mathbb{P}_0 \|
\mathbb{P}_\theta\right)= \begin{cases}+\infty &amp; \text { if } \theta
\neq 0, \\ 0 &amp; \text { if } \theta=0,\end{cases}\)</span> - and
<span class="math inline">\(\delta\left(\mathbb{P}_0,
\mathbb{P}_\theta\right)= \begin{cases}1 &amp; \text { if } \theta \neq
0, \\ 0 &amp; \text { if } \theta=0 .\end{cases}\)</span></p>
<p>当<span class="math inline">\(\theta_t \rightarrow
0\)</span>时，序列<span class="math inline">\(\left(\mathbb{P}_{\theta_t}\right)_{t \in
\mathbb{N}}\)</span>在EM距离下收敛于<span class="math inline">\(\mathbb{P}_0\)</span>，但在JS、KL、反向KL或TV散度下均不收敛。图1说明了EM和JS距离的情况。
<img src="/2025/03/10/WGAN/WGAN\2.png" alt="image"></p>
<p>示例1提供了一个案例，可以通过在EM距离上进行梯度下降来学习低维流形上的概率分布。这无法通过其他距离和散度来实现，因为由此产生的损失函数甚至不是连续的。尽管这个简单的例子以具有不相交支撑的分布为特征，但当支撑在一组零测集中包含非空交集时，同样的结论成立。</p>
<p>由于Wasserstein距离比JS距离更弱，我们现在可以问，在适当的假设下，<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>是否是<span class="math inline">\(\theta\)</span>上的连续损失函数</p>
<p><strong>定理1</strong> 设<span class="math inline">\(\mathbb{P}_r\)</span>是<span class="math inline">\(\mathcal{X}\)</span>上的一个固定分布，<span class="math inline">\(Z\)</span>是另一空间<span class="math inline">\(\mathcal{Z}\)</span>上的随机变量(例如 Gaussian)
。设<span class="math inline">\(g: \mathcal{Z} \times \mathbb{R}^d
\rightarrow \mathcal{X}\)</span>是一个函数，记为<span class="math inline">\(g_\theta(z)\)</span>，其中<span class="math inline">\(z\)</span>是第一坐标，<span class="math inline">\(\theta\)</span>是第二坐标。令<span class="math inline">\(\mathbb{P}_\theta\)</span>表示<span class="math inline">\(g_\theta(Z)\)</span>的分布。那么</p>
<ol type="1">
<li><p>如果<span class="math inline">\(g\)</span>在关于<span class="math inline">\(\theta\)</span>是连续的，则<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>也是连续的</p></li>
<li><p>如果<span class="math inline">\(g\)</span>是局部Lipschitz并满足假设1，那么<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>处处连续，并且几乎处处可微</p></li>
<li><p>1-2对于Jensen-Shannon散度<span class="math inline">\(J
S\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>和所有KL都不成立</p></li>
</ol>
<p>以下推论告诉我们，通过最小化
EM距离进行学习对于神经网络来说是有意义的（至少在理论上是这样）</p>
<p><strong>推论1</strong> 设<span class="math inline">\(g_\theta\)</span>为由<span class="math inline">\(\theta\)</span>参数化的任何前馈神经网络，<span class="math inline">\(p(z)\)</span>为<span class="math inline">\(z\)</span>上的先验分布，使得 <span class="math inline">\(\mathbb{E}_{z \sim
p(z)}[\|z\|]&lt;\infty\)</span>（例如高斯、均匀等）。然后假设 1
得到满足，因此<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>处处连续，几乎处处可微</p>
<p>所有这些都表明，对于研究的问题来说，EM是一个至少比
Jensen-Shannon散度更合理的成本函数。以下定理描述了由这些距离和散度诱导的拓扑的相对强度，其中KL最强，其次是JS和TV，EM最弱</p>
<p><strong>定理2</strong> 设<span class="math inline">\(\mathbb{P}\)</span>是紧空间<span class="math inline">\(\mathcal{X}\)</span>上的分布，<span class="math inline">\(\left(\mathbb{P}_n\right)_{n \in
\mathbb{N}}\)</span>是<span class="math inline">\(\mathcal{X}\)</span>上的分布序列。然后，<span class="math inline">\(n \rightarrow \infty\)</span>，</p>
<ol type="1">
<li>以下语句是等价的</li>
</ol>
<ul>
<li><span class="math inline">\(\delta\left(\mathbb{P}_n,
\mathbb{P}\right) \rightarrow 0\)</span></li>
<li>JS <span class="math inline">\(\left(\mathbb{P}_n, \mathbb{P}\right)
\rightarrow 0\)</span></li>
</ul>
<ol start="2" type="1">
<li>以下语句是等价的</li>
</ol>
<ul>
<li><span class="math inline">\(W\left(\mathbb{P}_n, \mathbb{P}\right)
\rightarrow 0\)</span></li>
<li><span class="math inline">\(\mathbb{P}_n \xrightarrow{\mathcal{D}}
\mathbb{P}\)</span> 其中 <span class="math inline">\(\xrightarrow{\mathcal{D}}\)</span>
表示随机变量的分布收敛</li>
</ul>
<ol start="3" type="1">
<li><p><span class="math inline">\(K L\left(\mathbb{P}_n \|
\mathbb{P}\right) \rightarrow 0\)</span> 或 <span class="math inline">\(K L\left(\mathbb{P} \| \mathbb{P}_n\right)
\rightarrow 0\)</span>能推出（1）</p></li>
<li><p>（1）能推出（2）</p></li>
</ol>
<p>这突出了这样一个事实，即在学习低维流形支持的分布时，KL、JS 和 TV
距离不是合理的代价函数。但是，EM
距离是合理的。下面将介绍优化EM距离的实际近似值</p>
<h2 id="wasserstein-gan-1">Wasserstein GAN</h2>
<p>定理2指出<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>在优化时可能比<span class="math inline">\(J S\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>具有更好的性质。然而，(1)中的下确界是非常难解的。另一方面，由Kantorovich-Rubinstein对偶[22]</p>
<p><span class="math display">\[
W\left(\mathbb{P}_r, \mathbb{P}_\theta\right)=\sup _{\|f\|_L \leq 1}
\mathbb{E}_{x \sim \mathbb{P}_r}[f(x)]-\mathbb{E}_{x \sim
\mathbb{P}_\theta}[f(x)] \qquad(2)
\]</span></p>
<p>其中，上确界在所有1-Lipschitz函数<span class="math inline">\(f:
\mathcal{X} \rightarrow \mathbb{R}\)</span>中寻找。请注意，如果我们将
<span class="math inline">\(\|f\|_L \leq 1\)</span>替换为<span class="math inline">\(\|f\|_L \leq
K\)</span>（某个常数K的K-Lipschitz），那么我们最终得到<span class="math inline">\(K \cdot W\left(\mathbb{P}_r,
\mathbb{P}_g\right)\)</span>。因此，如果有一个参数化的函数族<span class="math inline">\(\left\{f_w\right\}_{w \in
\mathcal{W}}\)</span>都是K-Lipschitz，那么可以考虑解决这个问题</p>
<p><span class="math display">\[
\max _{w \in \mathcal{W}} \mathbb{E}_{x \sim
\mathbb{P}_r}\left[f_w(x)\right]-\mathbb{E}_{z \sim
p(z)}\left[f_w\left(g_\theta(z)\right]\right.  \qquad(3)
\]</span></p>
<p>如果（2）中的上确界是由某个 <span class="math inline">\(w \in
\mathcal{W}\)</span>（这是一个非常强的假设，类似于证明估计器一致性时的假设）而得到的，那么这个过程将会使<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>的计算到达一个常数。此外，我们可以考虑通过估计<span class="math inline">\(\mathbb{E}_{z \sim p(z)}\left[\nabla_\theta
f_w\left(g_\theta(z)\right)\right]\)</span>通过方程（2）进行反推来对<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>求导数（同样，直到一个常数）。虽然这都是直觉，但现在证明，在最优性假设下，这个过程是有原则的。</p>
<p><strong>定理3</strong> 设<span class="math inline">\(\mathbb{P}_r\)</span>为任意分布。设<span class="math inline">\(\mathbb{P}_\theta\)</span>为<span class="math inline">\(g_\theta(Z)\)</span>的分布，<span class="math inline">\(Z\)</span>为密度为<span class="math inline">\(p\)</span>的随机变量，<span class="math inline">\(g_\theta\)</span>为满足假设1的函数。那么，下面问题有一个解<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[
\max _{\|f\|_L \leq 1} \mathbb{E}_{x \sim
\mathbb{P}_r}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_\theta}[f(x)]
\]</span> 并且有 <span class="math display">\[
\nabla_\theta W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)=-\mathbb{E}_{z \sim p(z)}\left[\nabla_\theta
f\left(g_\theta(z)\right)\right]
\]</span></p>
<p>现在的问题是找到解决方程（2）中最大化问题的函数<span class="math inline">\(f\)</span>。为了大致近似这一点，我们可以做的是训练一个参数化的神经网络，其中权重<span class="math inline">\(w\)</span>位于紧空间<span class="math inline">\(\mathcal{W}\)</span>中，然后通过<span class="math inline">\(\mathbb{E}_{z \sim p(z)}\left[\nabla_\theta
f_w\left(g_\theta(z)\right)\right]\)</span>进行反向传播，就像我们使用典型的GAN一样。注意，<span class="math inline">\(\mathcal{W}\)</span>是紧的这一事实意味着所有函数<span class="math inline">\(f_w\)</span>都是K-Lipschitz，它只取决于<span class="math inline">\(\mathcal{W}\)</span>，而不取决于单个权重，因此近似（2）到一个不相关的比例因子和“critic”<span class="math inline">\(f_w\)</span>的容量。为了使参数<span class="math inline">\(w\)</span>位于紧空间中，我们可以做的一件简单的事情是在每次梯度更新后将权重clamp到一个固定的范围中（比如<span class="math inline">\(\mathcal{W}=[-0.01,0.01]^l\)</span>）。Wasserstein生成对抗网络（WGAN）过程在算法1中进行了描述</p>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法1} \quad \text{WGAN} \\
   \hline
   \textbf{给定：}\alpha:学习率。c:裁剪参数。m:批量大小。 \\
   \qquad n_\text{critic}:每次生成器迭代，迭代\text{critic}的次数\\
   \textbf{给定：}w_0：初始 critic 参数。\theta_0：初始生成器的参数。
   \\1:
   \quad \textbf{while} \,\theta 尚未收敛 \, \textbf{do} \\2:
   \qquad \textbf{for} \, t=0,...,n_\text{critic} \, \textbf{do} \\3:
   \qquad \quad
从真实数据的批次采样一个批次\left\{x^{(i)}\right\}_{i=1}^m \sim
\mathbb{P}_r \\4:
   \qquad \quad 采样一批先验样本 \left\{z^{(i)}\right\}_{i=1}^m \sim
p(z) \\5:
   \qquad \quad g_w \leftarrow \nabla_w\left[\frac{1}{m} \sum_{i=1}^m
f_w\left(x^{(i)}\right)-\frac{1}{m} \sum_{i=1}^m
f_w\left(g_\theta\left(z^{(i)}\right)\right)\right]\\6:
   \qquad \quad w \leftarrow w+\alpha \cdot
\operatorname{RMSProp}\left(w, g_w\right)\\7:
   \qquad \quad w \leftarrow \operatorname{clip}(w,-c, c) \\8:
   \qquad \textbf{end for}\\9:
   \qquad 采样一批先验样本 \left\{z^{(i)}\right\}_{i=1}^m \sim p(z)
\\10:
   \qquad g_\theta \leftarrow-\nabla_\theta \frac{1}{m} \sum_{i=1}^m
f_w\left(g_\theta\left(z^{(i)}\right)\right) \\11:
   \qquad \theta \leftarrow \theta-\alpha \cdot
\operatorname{RMSProp}\left(\theta, g_\theta\right) \\12:
   \quad \textbf{end while} \\
   \hline
\end{array}
\]</span></p>
<p>权重裁剪显然是强制实施Lipschitz约束的一种糟糕的方法。如果裁剪参数很大，则任何权重可能需要很长时间才能达到其极限，从而使critic更难训练直到最优。如果裁剪很小，则当层数较大或未使用批量归一化（例如在RNN中）时，这很容易导致渐变消失。尝试了简单的变体（例如将权重投影到球体），几乎没有区别，并且由于它的简单性和已经很好的性能，作者坚持使用权重裁剪。作者将在神经网络设置中强制执行Lipschitz约束的主题留给进一步研究，并且积极鼓励感兴趣的研究人员改进这种方法。</p>
<p>EM距离是连续且可微的事实意味着可以（并且应该）训练critic直到最优。论点很简单，训练critic的次数越多，得到的Wasserstein梯度就越可靠，这实际上很有用，因为Wasserstein几乎在所有地方都是可导的对于JS，随着判别器变得更好，梯度变得更可靠，但真正的梯度为0，因为JS是局部饱和的，得到的梯度消失。图2展示了一个概念验证，训练一个GAN判别器和一个WGAN
critic，直到最优。判别器学习非常快，可以很好地区分假和真，并且正如预期的那样，没有提供可靠的梯度信息。然而，critic无法饱和，并收敛到一个线性函数，该函数在任何地方都给出了非常干净的渐变。我们约束权重的事实限制了函数在空间的不同部分最多是线性的，迫使最优critic具有这种行为。
<img src="/2025/03/10/WGAN/WGAN\3.png" alt="image"></p>
<p>也许更重要的是，可以训练critic直到最优，这使得这样做时不会发生collapse
modes。这是因为collapse
modes来自这样一个事实，即固定判别器的最佳生成器是判别器分配最高值的点的增量之和，如[4]所观察到并在[11]中突出显示的那样。</p>
<h2 id="实验结果">实验结果</h2>
<p>使用Wasserstein-GAN算法运行图像生成实验，并表明使用它比标准GAN中使用的公式有很大的实际好处</p>
<p>声明主要有两个好处 1.
一个与生成器的收敛性和样本质量相关的有意义的损失度量 2.
提高了优化过程的稳定性</p>
<h3 id="实验设置">实验设置</h3>
<p>作者对图像生成进行实验。要学习的目标分布是LSUN-Bedrooms数据集[24]
--一个室内卧室自然图像的集合。baseline比较是DCGAN[18]，这是一个卷积结构的GAN，使用标准的GAN程序使用<span class="math inline">\(-log
D\)</span>技巧[4]进行训练。生成的样本为3通道，大小为64×64像素的图像。我们在所有的实验中都使用了算法1中规定的超参数。</p>
<h3 id="有意义的损失度量">有意义的损失度量</h3>
<p>由于WGAN算法试图在每个生成器更新(算法1中的第10行)之前相对较好地训练critic
<span class="math inline">\(f\)</span>(算法1中第2-8行)，此时的损失函数是EM距离的估计，直到与我们约束f的Lipschitz常数的方式相关的常数。</p>
<p>我们的第一个实验说明了这种估计如何与生成样本的质量很好地相关。除了卷积DCGAN架构，我们还进行了实验，使用512个隐藏单元的4层ReLU-MLP替换生成器或同时替换生成器和critic。</p>
<p>图3为3种架构的WGAN训练过程中EM距离的WGAN估计(3)的演化情况。从图中可以清楚地看出，这些曲线与生成样本的视觉质量有很好的相关性。
<img src="/2025/03/10/WGAN/WGAN\5.png" alt="image"></p>
<p>据我们所知，这是GAN文献中第一次显示这样的属性，GAN的损失显示了收敛的属性。在对抗网络中进行研究时，此属性非常有用，因为人们不需要盯着生成的样本来找出故障模式，也不需要获得哪些模型比其他模型做得更好的信息。</p>
<p>然而，我们并不认为这是一种定量评估生成模型的新方法。依赖于critic架构的恒定比例因子意味着很难比较不同critic的模型。更重要的是，在实践中critic没有无限能力的事实使得我们很难知道我们估计的EM距离究竟有多接近。也就是说，我们成功地使用了损失度量来反复验证我们的实验，并且没有失败，我们认为这是对以前没有这种设施的训练GAN的一个巨大的改进。</p>
<p>相反，图4描绘了GAN训练过程中JS距离的GAN估计的演变。更准确地说，在GAN训练过程中，对判别器进行训练最大化
<span class="math display">\[
\left.\left.L\left(D, g_\theta\right)=\mathbb{E}_{x \sim \mathbb{P}_r}
\log D(x)\right]+\mathbb{E}_{x \sim \mathbb{P}_\theta} \log
(1-D(x))\right]
\]</span></p>
<p>这是<span class="math inline">\(2 J S\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)-2 \log
2\)</span>的下界。在图中，我们画出了量<span class="math inline">\(\frac{1}{2} L\left(D, g_\theta\right)+\log
2\)</span>，这是JS距离的一个下界 <img src="/2025/03/10/WGAN/WGAN\6.png" alt="image"></p>
<p>这个数量显然与样品质量相关性很差。还要注意，JS估计值通常保持不变，或者上升而不是下降。事实上，它通常非常接近<span class="math inline">\(log
2≈0.69\)</span>，这是JS距离的最高值。换句话说，JS距离饱和，鉴别器无损耗，生成的样本在某些情况下是有意义的（DCGAN生成器，右上图），在其他情况下会折叠成一个无意义的图像[4]。最后一个现象在[1]中得到了理论解释，并在[11]中得到了强调。</p>
<p>当使用<span class="math inline">\(- log
D\)</span>技巧[4]时，判别器损失和生成器损失是不同的。附录E中的图8报告了同样的图用于GAN训练，但是使用生成器损失代替判别器损失。这并不改变结论。</p>
<p>最后，作为一个负面的结果，当一个人使用基于动量的优化器(如Adam [8] (
<span class="math inline">\(\beta_1&gt;0\)</span> )
)时，或者当一个人使用高学习率时，WGAN训练变得不稳定。由于critic的损失是非平稳的，基于动量的方法似乎表现得更差。我们认为动量是一个潜在的原因，因为随着损失的增加和样本的恶化，Adam步和梯度之间的余弦通常会变成负值。这个余弦为负的唯一地方是在这些不稳定的情况下。因此，我们切换到RMSprop算法[21]，即使在非平稳问题RMSprop算法也表现良好[13]。</p>
<h3 id="提高了稳定性">提高了稳定性</h3>
<p>WGAN的一个好处是它允许我们训练critic直到最优性。当critic训练完毕时，它只是给生成器提供了一个损失，我们可以像任何其他神经网络一样进行训练。这告诉我们，我们不再需要适当地平衡生成器和判别器的容量。critic越好，我们用来训练生成器的梯度质量越高。</p>
<p>我们观察到，当生成器的结构选择发生变化时，WGANs比GANs更加稳定。我们通过在三个生成器架构上运行实验来说明这一点：(1)一个卷积DCGAN生成器，(2)一个没有批量归一化和具有固定数量过滤器的卷积DCGAN生成器，(3)一个具有512个隐藏单元的4层ReLU-MLP。最后两个已知在GANs中表现很差。我们为WGAN
critic或GAN判别器保留了卷积DCGAN架构</p>
<p>图5、图6和图7显示了同时使用WGAN和GAN算法为这三种架构生成的样本。我们将生成的样本的完整片参考附录F。样品未经过Cherry-Picked处理。</p>
<p>在没任何实验中我们都未看到WGAN算法发生模式崩溃的证据 <img src="/2025/03/10/WGAN/WGAN\7.png" alt="image">
图5：使用DCGAN生成器训练的算法。左：Wgan算法。右：标准GAN配方。两种算法都产生了高质量的样本
<img src="/2025/03/10/WGAN/WGAN\8.png" alt="image">
图6：算法用一个没有批归一化的生成器和每层固定数量的过滤器进行训练。除了去掉批量归一化，参数的数量也因此减少了一个数量级以上。左：Wgan算法。右：标准GAN配方。正如我们可以看到标准的GAN没有学习到，而WGAN仍然能够产生样本。
<img src="/2025/03/10/WGAN/WGAN\9.png" alt="image">
图7：用4层512单元ReLU非线性的MLP生成器训练的算法。参数数量与DCGAN类似，但对图像生成缺乏较强的诱导偏差。左：Wgan算法。右：标准GAN配方。WGAN方法仍然能够产生样品，质量低于DCGAN，质量高于标准GAN的MLP。注意到GAN
MLP中模式崩溃的显著程度 ## 结论</p>
<p>引入了WGAN算法，它是传统GAN训练的一种替代。在这个新的模型中，表明可以提高学习的稳定性，摆脱模式崩溃等问题，并提供有意义的学习曲线，用于调试和超参数搜索。此外，文章表明，相应的优化问题是健全的，并提供了广泛的理论工作，突出了与其他距离分布之间的深层联系。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/04/Numerical-Solution-of-Inverse-Problems-by-Weak-Adversarial-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/04/Numerical-Solution-of-Inverse-Problems-by-Weak-Adversarial-Networks/" class="post-title-link" itemprop="url">Numerical Solution of Inverse Problems by Weak Adversarial  Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-04 17:19:32" itemprop="dateCreated datePublished" datetime="2025-03-04T17:19:32+08:00">2025-03-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-16 10:48:22" itemprop="dateModified" datetime="2025-03-16T10:48:22+08:00">2025-03-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文精读</span></a>
                </span>
            </span>

          
            <span id="/2025/03/04/Numerical-Solution-of-Inverse-Problems-by-Weak-Adversarial-Networks/" class="post-meta-item leancloud_visitors" data-flag-title="Numerical Solution of Inverse Problems by Weak Adversarial  Networks" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/03/04/Numerical-Solution-of-Inverse-Problems-by-Weak-Adversarial-Networks/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/04/Numerical-Solution-of-Inverse-Problems-by-Weak-Adversarial-Networks/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>21k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="numerical-solution-of-inverse-problems-by-weak-adversarial-networks弱对抗网络对反问题的数值求解">Numerical
Solution of Inverse Problems by Weak Adversarial
Networks（弱对抗网络对反问题的数值求解）</h1>
<p>期刊：Inverse Problems</p>
<p>时间：2020</p>
<h2 id="摘要">摘要</h2>
<p>本文发展了一种弱对抗网络方法来数值求解一类反问题，包括电阻抗层析成像和动态电阻抗层析成像问题。<strong>利用给定反问题的PDE的弱形式，其中解和测试函数被参数化为深度神经网络。然后，弱形式和边界条件诱导出一个关于网络参数的鞍点函数的极小极大问题</strong>。随着参数的交替更新，网络逐渐逼近反问题的解。对所提算法的收敛性给出了理论证明。
所提出的方法是完全无网格的，无需任何空间离散，特别适用于高维数和解的低正则性问题。对各种测试反问题的数值实验表明了该方法具有较高的精度和效率。</p>
<h2 id="介绍">介绍</h2>
<p>反问题(Inverse
Problems，IP)普遍存在于大量的科学学科中，包括地球物理[73]、信号处理与成像[9]、计算机视觉[61]、遥感与控制[87]、统计学[53]和机器学习[38]等。设<span class="math inline">\(\Omega\)</span>是<span class="math inline">\(\mathbb{R}^d\)</span>中的开集和有界集，则定义在<span class="math inline">\(\Omega\)</span>上的IP可表示为一般形式： <span class="math display">\[
\begin{aligned}
\mathcal{A}[u, \gamma]=0, &amp; &amp; \text { in } \Omega  \qquad(1a)\\
\mathcal{B}[u, \gamma]=0, &amp; &amp; \text { on } \partial \Omega
\qquad(1b)
\end{aligned}
\]</span> 其中<span class="math inline">\(\mathcal{A}[u,
\gamma]\)</span>指定一个微分方程，<strong><span class="math inline">\(u\)</span>是反介质问题中的解，<span class="math inline">\(\gamma\)</span>是反源问题中的源函数</strong>。方程<span class="math inline">\(\mathcal{A}\)</span>可以是常微分方程(ODE)，也可以是偏微分方程(PDE)，还可以是积分微分方程(IDE)，<span class="math inline">\((u, \gamma)\)</span>在区域<span class="math inline">\(\Omega\)</span>内需要满足(几乎)处处成立。边界值(以及如果适用的初始值)由<span class="math inline">\(\mathcal{B}[u, \gamma]\)</span>在<span class="math inline">\(\partial
\Omega\)</span>上给出。根据具体的应用，<span class="math inline">\(u\)</span>和/或<span class="math inline">\(\gamma\)</span>的部分信息可以在<span class="math inline">\(\Omega\)</span>的内部获得.那么IP
(1)就是找到同时满足(1a)和(1b)的<span class="math inline">\((u,
\gamma)\)</span>。</p>
<p>为了实例化我们的方法，我们主要以电阻抗成像(Electrical Impedance
Tomography，EIT)中经典的电导率逆问题[19、55]为例，介绍了本文的主要思想和推导过程。然而，我们的方法可以很容易地通过修改应用于其他类型的IP。一个动态EIT问题的例子将在第4节中给出。EIT的目标是根据电势<span class="math inline">\(u\)</span>，电流<span class="math inline">\(-\gamma \partial_{\vec{n}}
u\)</span>的测量和区域<span class="math inline">\(\Omega\)</span>的边界<span class="math inline">\(\partial \Omega\)</span>上/附近的<span class="math inline">\(\gamma\)</span> (也即<span class="math inline">\(\partial_{\vec{n}}
u\)</span>)的知识，确定定义在<span class="math inline">\(\Omega\)</span>上的未知介质的电导率分布<span class="math inline">\(\gamma(x)\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
-\nabla \cdot(\gamma \nabla u)-f=0, &amp; \text { in }
\Omega  \qquad(2a)\\
u-u_b=0, \gamma-\gamma_b=0, \partial_{\vec{n}} u-u_n=0, &amp; \text { on
} \partial \Omega \qquad(2b)
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(u_b\)</span>为测量电压，<span class="math inline">\(\gamma_b\)</span>为物体表面附近的电导率，<span class="math inline">\(u_n \triangleq \nabla u \cdot \vec{n}\)</span>
，其中 <span class="math inline">\(\vec{n}\)</span>为<span class="math inline">\(\partial
\Omega\)</span>的外法线.值得注意的是，<strong>我们的方法并不像EIT问题[23、36、59]的经典方法那样估计与电导率函数相关的Dirichlet
- to - Neumann
(DtN)映射。相反，我们的目标是直接利用给定的数据数值求解一般的一类IPs(1)</strong>，以EIT问题(2)为原型例子，而不利用其特殊结构(例如,
DtN映射)。为了使我们的介绍简洁而有重点，我们只考虑(1a)中具有PDEs特征的<span class="math inline">\(\mathcal{A}[u,
\gamma]\)</span>的IP，并假设给定的IP是定义良好的且至少有一个(弱)解。</p>
<p>我们的方法是<strong>训练能够表示给定IP的解<span class="math inline">\((u,
\gamma)\)</span>的深度神经网络</strong>，与经典的数值方法相比有很大的改进，特别是对于高维问题。更具体地说，<strong>我们利用PDE
(1a)的弱形式，将IP转化为<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>的算子范数最小化问题。然后将<span class="math inline">\(u\)</span>、未知系数<span class="math inline">\(\gamma\)</span>和测试函数<span class="math inline">\(\varphi\)</span>分别参数化为深度神经网络<span class="math inline">\(u_\theta, \gamma_\theta\)</span>, 和 <span class="math inline">\(\varphi_\eta\)</span>，网络参数为<span class="math inline">\((\theta, \eta)\)</span>，并形成参数为<span class="math inline">\((\theta,
\eta)\)</span>的鞍函数的极小极大问题。最后，我们应用随机梯度下降法交替更新网络参数，使得(<span class="math inline">\(u_\theta,
\gamma_\theta\)</span>)逐步逼近IP的解</strong>。利用深度神经网络对<span class="math inline">\((u,
\gamma)\)</span>进行参数化，不需要对空间域和时间域进行离散化，因此是完全无网格的。与经典的有限差分方法(FDM)和有限元法(FEM)相比，这是一种很有前途的替代方法，它们首先在[8]中使用了"维数灾难"这一术语。此外，<strong>我们的方法结合了弱解(原始网络)<span class="math inline">\((u, \gamma)\)</span>和测试函数(对抗网络) <span class="math inline">\(\varphi\)</span>的训练，由PDE的弱形式控制，这对解<span class="math inline">\((u,
\gamma)\)</span>的正则性要求较低，当解具有奇异性时，在许多实际应用中可能更有优势</strong>。</p>
<p>本论文的其余部分组织如下。我们首先在第2节中回顾了基于深度学习的正问题和反问题解决方案的近期工作。在第三节中，我们给出了我们方法的详细推导和一系列理论结果来支持所提出方法的有效性。在第4节中，我们讨论了几种可以提高实际性能的实现技术，并进行了一系列的数值实验来证明所提出方法的有效性。第五部分对本文进行了总结，并给出了一些一般性的注记。</p>
<h2 id="相关工作">相关工作</h2>
<p>在过去的几年中，使用基于深度学习的方法来解决正问题和反问题已经成为一种新的趋势。这些方法大致可以分为两类。第一类包括基于监督学习方法来近似求解给定问题的方法。这些方法通过数值模拟和实验需要大量的输入-输出对来训练所需的网络。在这一类中，深度神经网络用于从测量数据中生成近似的中间结果，用于进一步精化[34、66、69、77、81、88]，应用于改进经典数值方法在后处理阶段的求解[6、41、42、45、51、56、67、84]，或者用于近似从反问题的给定参数到其解的映射，但需要空间离散，无法应用于高维问题[2、50、63]。</p>
<p>第二类是基于问题表述直接求解正问题或反问题的无监督学习方法，而不是额外的训练数据，在实际应用中可能比第一类更有优势。例如，在[24]中，前馈神经网络用于参数化系数函数，并通过最小化性能函数进行训练。在[58]中，提出了一种名为SwitchNet的神经网络架构，通过散射体和散射场之间的映射来解决逆散射问题。文献[32]提出了一种针对2D和3D
EIT问题的深度学习方法，通过紧凑的神经网络架构来表示DtN图。前向问题中PDE对应的倒向随机微分方程(BSDE)是通过神经网络部分参数化，使得对域[13、30、43]中目标点的BSDE积分得到PDE的解。在[31]中，一个正问题的解被参数化为一个深度神经网络，它通过最小化与PDE相关的能量泛函和边值条件上的惩罚项组成的损失函数来训练。在[74]中提出了另一种无网格框架，称为物理信息神经网络(PINN)，用于使用基于PDEs的强大公式的深度神经网络来解决正问题和反问题，其中反问题部分考虑常系数函数。具体来说，PINN使用深度神经网络对给定的PDE的未知量进行参数化，这些网络通过最小化在域和边界条件采样点处违反PDE的最小二乘法形成的损失函数进行训练。PINN的一些实证研究也在[26]中进行。在[52]中也考虑了在问题域中给定数据的基于PINN的IPs的解决方案，并且在[5]中提出了使用自适应采样的配置点来精化解决方案。
在[89]中，PDE的弱形式被用作目标函数，其中PDE的解和测试函数都被参数化为深度神经网络，分别试图最小化和最大化目标函数。在[54]中，使用了类似的变分形式，其中测试函数是固定基，而不是要学习的神经网络。在[68]中，我们使用了三个神经网络，其中一个用于低保真度数据，另外两个用于高保真度数据的线性和非线性函数。具有多保真网络结构的PINN也被提出用于随机PDE情形，其中多项式混沌展开式用于表示解，即作为随机基与待学习系数函数的线性组合[18]。
在[12]中，IP的解被深度神经网络参数化，并通过最小化一个代价函数来学习，该代价函数执行IP和额外的正则化条件，其中PDE的解在训练过程中被要求。</p>
<p>最近，基于元学习的前向问题求解方法也被考虑[18、33、64]。在[33]中，我们利用小波变换的压缩形式来学习从微分算子的系数到伪微分算子(e.g.
,格林函数)的映射。在[64]中，我们引入了一个由分支网络和主干网络组成的深度算子网络。该网络将有限个位置上的输入函数(branch-net)和输出函数的位置(dry-net)进行编码，输出函数由两者的内积加上一个偏置给出。学习网络的宽度和深度参数也在[18]中使用贝叶斯优化来考虑。</p>
<p>我们的IP方法沿用了我们先前针对正问题的工作[89]，这与前面提到的现有方法在使用偏微分方程的弱形式上有所不同。<strong>弱形式是求解偏微分方程的一种强有力的方法，因为它要求更少的正则性，并允许解的必要奇异性，这在成像和异常检测等许多实际应用中都是一个重要的特征</strong>。
从理论的角度来看，<strong>我们的方法对解(作为原始网络)和测试函数(作为对抗网络)进行神经网络参数化，并以一种对抗训练的方式执行，即测试函数在不满足PDE的地方对解网络进行批判训练，解网络在这些地方进行自我修正，直到PDE在域中(几乎)处处被满足</strong>。然而，由于反问题往往是不适定的，且一般情况下比正问题更难求解，因此本文主要关注EIT中的反问题(2)。类似问题的一些实验结果也在第4节中给出。</p>
<p>本工作中的对抗训练与生成对抗网络[39]中使用的对抗训练相似，其中<strong>生成器网络旨在将通用的随机样本(如来自给定的多变量Gaussian的)映射到与训练样本具有相同分布的样本</strong>，而<strong>判别器网络则是将生成器网络产生的样本与真实样本区分开来</strong>。生成器和对抗网络作为零和博弈中的两个参与者，分别通过梯度下降和上升对目标函数进行交替更新，以达到均衡。
特别地，GAN的一个显著变体，称为Wasserstein生成式对抗网络[4]，也具有原始网络(生成器)和对抗网络(由生成分布和样本分布的Wasserstein距离决定的最优运输的对偶函数)的min-max结构作为我们的提法。然而，WGAN要求其在max问题中的对偶函数是1-Lipschitz的，这在数值上是很难实现的，并产生了一系列的后续工作来克服问题[40、70]，谱归一化生成对抗网络。相比之下，<strong>我们工作中的弱解与测试函数的结构自然地产生于PDE理论中的弱公式，它在不对对抗网络(测试函数)施加限制性约束的情况下，为PDE的IP求解提供了大量的理论证明和计算益处</strong>。</p>
<p>与许多现有的深度学习方法需要大量的演示数据(例如,系数/边界值和解对)进行训练不同，我们的方法遵循无监督的学习策略，只需要在给定的IP中制定PDE和边界条件。在[83]中，一项无监督学习研究表明，一般的卷积神经网络(CNN)自动偏向于平滑信号，并且可以在没有任何训练数据的情况下产生类似于图像去噪中一些复杂重建的结果。这种现象被称为深度图像先验(Deep
Image Prior，DIP)，在[29、46]中被进一步利用。
DIP与现有工作最显著的区别在于，我们的方法是完全无网格的，不需要任何空间离散，适用于高维问题。另一方面，在DIP及其后续工作中，重建网络被应用于离散化的2D或3D图像。此外，我们的目标是利用深度网络的表征能力来参数化连续空间中IP的解，而DIP的主要兴趣在于其有趣的自动正则化特性。</p>
<h2 id="针对反问题的弱对抗网络">针对反问题的弱对抗网络</h2>
<p>所提出的IPs弱对抗网络方法受到PDEs弱形式的启发。为了得到(1a)中偏微分方程的弱形式，我们将(1a)两边同时乘以一个任意的测试函数<span class="math inline">\(\varphi \in H_0^1(\Omega)\)</span>(在<span class="math inline">\(\Omega\)</span>中具有有界一阶弱导数和紧支撑的函数的Hilbert空间)，并在<span class="math inline">\(\Omega\)</span>上积分： <span class="math display">\[
\langle\mathcal{A}[u, \gamma], \varphi\rangle:=\int_{\Omega}
\mathcal{A}[u, \gamma](x) \varphi(x) \mathrm{d} x=0 \qquad(3)
\]</span></p>
<p><strong>弱形式(3)的主要优点之一是我们可以通过分部积分将<span class="math inline">\(\mathcal{A}[u,
\gamma]\)</span>中的某些梯度算子转移到<span class="math inline">\(\varphi\)</span>中，从而降低对<span class="math inline">\(u\)</span>(和<span class="math inline">\(\gamma\)</span>，若适用)正则性的要求</strong>。例如，在电导率逆问题(2)中，分部积分和<span class="math inline">\(\partial \Omega\)</span>上<span class="math inline">\(\varphi=0\)</span>的事实一起导致 <span class="math display">\[
\langle\mathcal{A}[u, \gamma], \varphi\rangle=\int_{\Omega}(\gamma
\nabla u \cdot \nabla \varphi-f \varphi) \mathrm{d} x=0 \qquad(4)
\]</span></p>
<p>其中<span class="math inline">\(\gamma \nabla
u\)</span>在经典意义下不一定像式(2)那样可微(在这篇文章中,我们用<span class="math inline">\(\nabla\)</span>表示关于<span class="math inline">\(x\)</span>的梯度算子,<span class="math inline">\(\nabla_\theta\)</span>表示关于<span class="math inline">\(\theta\)</span>的梯度等)。若对所有的<span class="math inline">\(\varphi \in H_0^1(\Omega)\)</span>，<span class="math inline">\((u,
\gamma)\)</span>满足边界条件(1b)和(3)，则称<span class="math inline">\((u, \gamma) \in H^1(\Omega) \times
L^2(\Omega)\)</span>为反问题(1)的弱解(或广义解)。这里<span class="math inline">\(L^2(\Omega)\)</span>是<span class="math inline">\(\Omega\)</span>上平方可积函数的Lebesgue空间，<span class="math inline">\(H^1(\Omega) \subset
L^2(\Omega)\)</span>是一阶弱导数有界的函数的Hilbert空间。注意到(1)式的任何经典(强)解也是弱解。
在这项工作中，我们寻求反问题(1)的弱解，以便我们可能能够提供问题的答案，即使它不存在经典意义上的解。</p>
<p>在文献[89]的基础上，我们考虑了(1)中PDE <span class="math inline">\(\mathcal{A}[u,
\gamma]=0\)</span>的弱形式。为了处理反问题中PDE的未知解<span class="math inline">\(u\)</span>和参数<span class="math inline">\(\gamma\)</span>，我们将<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>都参数化为深度神经网络，并将<span class="math inline">\(\mathcal{A}[u, \gamma]: H_0^1(\Omega) \rightarrow
\mathbb{R}\)</span>看作一个线性泛函，使得<span class="math inline">\(\mathcal{A}[u,
\gamma](\varphi):=\langle\mathcal{A}[u, \gamma],
\varphi\rangle\)</span>，如式(3)所定义。我们定义由<span class="math inline">\(H_1\)</span>范数诱导的<span class="math inline">\(\mathcal{A}[u, \gamma]\)</span>范数为 <span class="math display">\[
\|\mathcal{A}[u, \gamma]\|_{o p}:=\sup _{\varphi \in H_0^1, \varphi \neq
0} \frac{\langle\mathcal{A}[u, \gamma],
\varphi\rangle}{\|\varphi\|_{H^1}} \qquad(5)
\]</span></p>
<p>其中，<span class="math inline">\(\varphi\)</span>的<span class="math inline">\(H^1\)</span>范数由<span class="math inline">\(\|\varphi\|_{H^
1(\Omega)}^2=\int_{\Omega}\left(|\varphi(x)|^2+|\nabla
\varphi(x)|^2\right) \mathrm{d} x\)</span>给出。因此，<span class="math inline">\((u, \gamma)\)</span>是(1)的弱解当且仅当<span class="math inline">\(\partial \Omega\)</span>上<span class="math inline">\(\|\mathcal{A}[u, \gamma]\|_{o p}=0\)</span>，<span class="math inline">\(\mathcal{B}[u, \gamma]=0\)</span>。当<span class="math inline">\(\|\mathcal{A}[u, \gamma]\|_{o p} \geq
0\)</span>时，我们知道方程(1)的一个弱解<span class="math inline">\((u,
\gamma)\)</span>，从而解决了方程(5)的如下观测问题： <span class="math display">\[
\underset{u, \gamma}{\operatorname{minimize}}\|\mathcal{A}[u,
\gamma]\|_{o p}^2=\underset{u, \gamma}{\operatorname{minimize}} \sup
_{\varphi \in H_0^1, \varphi \neq 0} \frac{|\langle\mathcal{A}[u,
\gamma], \varphi\rangle|^2}{\|\varphi\|_{H^1}^2} \qquad(6)
\]</span></p>
<p>其中，<span class="math inline">\((u, \gamma) \in H^1(\Omega) \times
L^2(\Omega)\)</span>，且取得最小值0。这一结果在下面的定理中进行了总结，并在附录A.1中给出了证明。</p>
<p><strong>定理1</strong> 假设<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>满足边界条件<span class="math inline">\(\mathcal{B}\left[u^*,
\gamma^*\right]=0\)</span>，则<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>是(1)式的弱解当且仅当<span class="math inline">\(\left\|\mathcal{A}\left[u^*,
\gamma^*\right]\right\|_{o p}=0\)</span></p>
<p><strong>定理1意味着，由于算子范数的非负性，为了找到(1)的弱解，我们可以通过求取最小的算子范数值<span class="math inline">\(\left\|\mathcal{A}\left[u^*,
\gamma^*\right]\right\|_{o p}=0\)</span>来寻找满足<span class="math inline">\(\mathcal{B}\left[u^*,\gamma^*\right]=0\)</span>且同时最小化(6)的最优解<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>。也就是说，<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>是问题(1)的弱解当且仅当<span class="math inline">\(\left\|\mathcal{A}\left[u^*,
\gamma^*\right]\right\|_{o p}\)</span>和<span class="math inline">\(\left\|\mathcal{B}\left[u^*,
\gamma^*\right]\right\|_{L^2(\partial
\Omega)}\)</span>都消失。因此，我们可以从下面的最小化问题中求解<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>，等价于(1)</strong> <span class="math display">\[
\underset{u, \gamma}{\operatorname{minimize}} I(u,
\gamma)=\|\mathcal{A}[u, \gamma]\|_{o p}^2+\beta\|\mathcal{B}[u,
\gamma]\|_{L^2(\partial \Omega)}^2 \qquad(7)
\]</span></p>
<p>并且<span class="math inline">\(\beta
&gt;0\)</span>是平衡目标函数<span class="math inline">\(I(u,\gamma)\)</span>中两项的权重参数。注意到(7)式中目标函数的两项均为非负，且仅在(1)式的一个弱解<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>处同时消失。</p>
<p>对于高维PDEs的经典数值方法，一个很有前途的替代方法是使用深度神经网络，因为它们不需要区域离散，并且是完全无网格的。深度神经网络是多个简单函数(称为层)的组合，因此它们可以逼近相当复杂的函数。考虑一个简单的多层神经网络<span class="math inline">\(u_\theta\)</span>如下： <span class="math display">\[
u_\theta(x)=w_K^{\top} l_{K-1} \circ \cdots \circ l_0(x)+b_K \qquad(8)
\]</span></p>
<p>其中，第<span class="math inline">\(k\)</span>层<span class="math inline">\(l_k: \mathbb{R}^{d_k} \rightarrow
\mathbb{R}^{d_{k+1}}\)</span>由<span class="math inline">\(l_k(z)=\sigma_k\left(W_k
z+b_k\right)\)</span>给出，权重<span class="math inline">\(W_k \in
\mathbb{R}^{d_{k+1} \times d_k}\)</span>，偏置<span class="math inline">\(b_k \in
\mathbb{R}^{d_{k+1}}\)</span>，所有层的网络参数用<span class="math inline">\(\theta\)</span>统一表示如下： <span class="math display">\[
\theta:=\left(w_K, b_K, W_{K-1}, b_{K-1}, \ldots, W_0, b_0\right)
\qquad(9)
\]</span></p>
<p>综上，本文所有向量默认为列向量。在(8)中，<span class="math inline">\(x \in \Omega\)</span>是网络的输入，<span class="math inline">\(d_0=d\)</span>是(1)的问题维数(也称为输入层的大小)，<span class="math inline">\(w_K \in \mathbb{R}^{d_K}\)</span>和<span class="math inline">\(b_K \in \mathbb{R}\)</span>是最后第<span class="math inline">\(K\)</span>层(也称为输出层)中的参数.非线性激活函数<span class="math inline">\(\sigma_k\)</span>的典型选择包括sigmoid函数<span class="math inline">\(\sigma(z)=\left(1+e^{-z}\right)^{-1}\)</span>、双曲正切(tanh)函数<span class="math inline">\(\sigma(z)=\left(e^z-e^{-z}\right)
/\left(e^z+e^{-z}\right)\)</span>和修正线性单元(ReLU)函数<span class="math inline">\(\sigma(z)=\max (0,
z)\)</span>，它们分别被应用。深度神经网络的训练是指利用可用的数据或约束优化<span class="math inline">\(\theta\)</span>的过程，使得函数<span class="math inline">\(u_\theta\)</span>可以逼近(未知)目标函数。关于深度神经网络的更多细节可参见文献[38]。</p>
<p>尽管有如式(8)的简单结构，但深度神经网络能够在紧支撑<span class="math inline">\(\bar{\Omega}\)</span>上均匀地逼近相当复杂的连续函数(以及必要时的导数)。这个重要的结果被称为万能逼近定理[47]
。由万能逼近定理保证的神经网络的表达能力表明(1)的弱解<span class="math inline">\((u,
\gamma)\)</span>)的无网格参数化是有希望的。接下来，我们对<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>都选取足够深度的神经网络结构，如式(8)所示。数值实验中使用的具体结构，即层数<span class="math inline">\(K\)</span>和尺寸<span class="math inline">\(\left\{d_1, \ldots,
d_{K-1}\right\}\)</span>将在第4节中给出。注意到<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>是两个独立的网络，但我们用一个字母<span class="math inline">\(\theta\)</span>来表示它们的网络参数，而不是用<span class="math inline">\(\theta_u\)</span>和<span class="math inline">\(\theta_\gamma\)</span>来简化符号。<strong>也就是说，我们将<span class="math inline">\((u, \gamma)\)</span>参数化为深度神经网络<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>，并试图找到参数<span class="math inline">\(\theta\)</span>使得<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>解决(7)</strong>。为此，<strong>弱式(3)中的测试函数<span class="math inline">\(\varphi\)</span>也被参数化为深度神经网络<span class="math inline">\(\varphi_\eta\)</span>，其形式类似于(8)和(9)，参数用<span class="math inline">\(\eta\)</span>表示</strong>。通过参数化的<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>和<span class="math inline">\(\varphi_\eta\)</span>，我们遵循(3)中的内积记法并定义
<span class="math display">\[
E(\theta, \eta):=\left|\left\langle\mathcal{A}\left[u_\theta,
\gamma_\theta\right], \varphi_\eta\right\rangle\right|^2 \qquad(10)
\]</span></p>
<p>不像(平方)算子范数(5)的原始定义那样用<span class="math inline">\(\left\|\varphi_\eta\right\|_{H_1}^2\)</span>对
<span class="math inline">\(E(\theta,
\eta)\)</span>进行正规化，而是<strong>将(5)中的平方算子范数近似(上升到一个恒定的标度)为如下<span class="math inline">\(\theta\)</span>的极大值函数</strong>： <span class="math display">\[
L_{\mathrm{int}}(\theta):=\max _{|\eta|^2 \leq 2 B} E(\theta, \eta)
\qquad(11)
\]</span></p>
<p>其中<span class="math inline">\(B&gt;0\)</span>是一个限定网络参数<span class="math inline">\(\eta\)</span>大小的上界。这里<span class="math inline">\(|\eta|^2=\sum_k\left(\sum_{i j}\left[W_k\right]_{i
j}^2+\sum_i\left[b_k\right]_i^2\right),[M]_{i j} \in
\mathbb{R}\)</span>表示矩阵<span class="math inline">\(M\)</span>的<span class="math inline">\((i, j)\)</span>项， <span class="math inline">\([v]_i \in \mathbb{R}\)</span>表示向量<span class="math inline">\(v\)</span>的第<span class="math inline">\(i\)</span>个分量。值得注意的是，式(11)中对<span class="math inline">\(\eta\)</span>的<span class="math inline">\(\ell_2\)</span>-范数的界约束与WGAN
[4]中使用的权重裁剪(等价于关于<span class="math inline">\(\ell_{\infty}\)</span>-范数的界)方法类似。然而，它们服务于不同的目的：在(11)中引入约束，使得积分(如(4))是有界的(这个界的实际值可以是任意的)。在这种情况下，我们的数值实现中的蒙特卡洛近似得到的随机梯度具有有界方差，这在下面定理4的证明中是需要的。另一方面，WGAN中的权重裁剪是为了保证神经网络实现的对偶函数是1-Lipschitz函数类<span class="math inline">\(\mathcal{F}:=\{f: \Omega \rightarrow
\mathbb{R}:|f(x)-f(y)| \leq\)</span> <span class="math inline">\(|x-y|,
\forall x, y \in
\Omega\}\)</span>。正如文献[4]所指出的那样，权重裁剪是实现1-Lipschitz约束的一种简单但不合适的方法，因此有一系列的后续工作来解决这个问题，如[40、70]。</p>
<p>进一步，我们定义与边界条件(1b)相关的损失函数为 <span class="math display">\[
L_{\mathrm{bdry}}(\theta):=\left\|\mathcal{B}\left[u_\theta,
\gamma_\theta\right]\right\|_{L^2(\partial \Omega)}^2=\int_{\partial
\Omega}\left|\mathcal{B}\left[u_\theta, \gamma_\theta\right](x)\right|^2
\mathrm{~d} S(x) \qquad(12)
\]</span></p>
<p>例如，如果在(2b)中给出<span class="math inline">\((u,
\gamma)\)</span>的边界条件，且已知边界值<span class="math inline">\(\left(u_b, \gamma_b, u_n\right)\)</span>，则<span class="math inline">\(L_{\text {bdry }}(\theta)=\int_{\partial
\Omega}\left|u_\theta(x)-u_b(x)\right|^2+\left|\gamma_\theta(x)-\gamma_b(x)\right|^2+\left|\partial_{\vec{n}(x)}
u(x)-u_n(x)\right|^2 \mathrm{~d}
S(x)\)</span>。最后，定义总损失函数<span class="math inline">\(L(\theta)\)</span>，并求解如下关于其最优<span class="math inline">\(\theta^*\)</span>的最小化问题： <span class="math display">\[
\underset{\theta}{\operatorname{minimize}} L(\theta), \quad \text {
where } \quad L(\theta):=L_{\mathrm{int}}(\theta)+\beta
L_{\mathrm{bdry}}(\theta) \qquad(13)
\]</span></p>
<p>其中，我们还限制了参数<span class="math inline">\(\theta\)</span>的大小，使得对于同一个<span class="math inline">\(B\)</span>，<span class="math inline">\(|\theta|^2
\leq 2 B\)</span>，以简化记号。注意到这里<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\eta\)</span>都是有限维向量，且<span class="math inline">\(L_{\text {int }}(\theta), L_{\text {bdry
}}(\theta), E(\theta, \eta) \in
\mathbb{R}_{+}\)</span>，因此可以应用数值优化算法来寻找 <span class="math inline">\(L(\theta)\)</span>的最小值。</p>
<p>求解类似于(13)的极小化问题的标准方法是<strong>投影梯度下降法</strong>，该方法执行如下迭代：
<span class="math display">\[
\theta \leftarrow \Pi\left(\theta-\tau \nabla_\theta L(\theta)\right)
\qquad(14)
\]</span></p>
<p>其中<span class="math inline">\(\Pi(\theta)=\min (\sqrt{2
B},|\theta|) \cdot(\theta /|\theta|)\)</span>是<span class="math inline">\(\theta\)</span>到以原点为圆心，半径为<span class="math inline">\(\sqrt{2 B}\)</span>的球的投影，<span class="math inline">\(\tau&gt;0\)</span>是步长.可以看出，(14)式的主要计算是在梯度<span class="math inline">\(\nabla_\theta L(\theta)=\nabla_\theta
L_{\mathrm{int}}(\theta)+\beta \nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>上进行的。<span class="math inline">\(\nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>的计算简单明了，如下所示。然而，损失<span class="math inline">\(L_{\text {int
}}(\theta)\)</span>被定义为一个最大化问题(11)，我们需要先将其梯度写成关于<span class="math inline">\(\theta\)</span>的函数。为此，我们有下面的引理来计算梯度<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)\)</span>，证明见附录A.2。</p>
<p><strong>引理2.</strong> 假设<span class="math inline">\(L_{i n
t}(\theta)\)</span>在(11)式中定义。则任意<span class="math inline">\(\theta\)</span>处的梯度<span class="math inline">\(\nabla_\theta L_{i n t}(\theta)\)</span>由<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)=\partial_\theta E(\theta,
\eta(\theta))\)</span>给出，其中<span class="math inline">\(\eta(\theta)\)</span>为对指定的<span class="math inline">\(\theta\)</span>求<span class="math inline">\(\max
_{|\eta|^2 \leq 2 B} E(\theta, \eta)\)</span>的解。</p>
<p><strong>注释</strong> 引理2表明，对任意给定的<span class="math inline">\(\theta\)</span>，若要得到<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)\)</span>，我们可以先对<span class="math inline">\(\eta\)</span>取<span class="math inline">\(E\)</span>关于<span class="math inline">\(\theta\)</span>的偏导数，然后利用<span class="math inline">\(\theta\)</span>和最大化问题(11)的解<span class="math inline">\(\eta(\theta)\)</span>求偏导数</p>
<p><span class="math inline">\(L_{\mathrm{int}}(\theta)\)</span>和<span class="math inline">\(L_{\mathrm{bdry}}(\theta)\)</span>的精确梯度需要在连续空间<span class="math inline">\(\Omega\)</span>和<span class="math inline">\(\partial
\Omega\)</span>上对深度神经网络参数化的函数进行积分，这在实际中是计算难以解决的。因此，我们采用这些积分的蒙特卡罗分析(MC)近似。为此，我们需要下面关于利用样本逼近积分的结果，其证明见附录A.3.</p>
<p><strong>引理3</strong> 假设<span class="math inline">\(\Omega \subset
\mathbb{R}^d\)</span>是有界的，<span class="math inline">\(\rho\)</span>是定义在<span class="math inline">\(\Omega\)</span>上的概率密度，使得对所有<span class="math inline">\(x \in \Omega\)</span>，<span class="math inline">\(\rho(x)&gt;0\)</span>。给定函数<span class="math inline">\(\psi \in L^2(\Omega)\)</span>，记<span class="math inline">\(\Psi=\int_{\Omega} \psi(x) \mathrm{d}
x\)</span>。设<span class="math inline">\(x^{(1)}, \ldots,
x^{(N)}\)</span>是从<span class="math inline">\(\rho\)</span>中抽取的<span class="math inline">\(N\)</span>个独立样本。考虑<span class="math inline">\(\Psi\)</span>的如下估计量<span class="math inline">\(\hat{\Psi}\)</span>：</p>
<p><span class="math display">\[
\hat{\Psi}=\frac{1}{N} \sum_{i=1}^N
\frac{\psi\left(x^{(i)}\right)}{\rho\left(x^{(i)}\right)} \qquad(15)
\]</span></p>
<p>则<span class="math inline">\(\hat{\Psi}\)</span>的一阶矩和二阶矩由下式给出.</p>
<p><span class="math display">\[
\mathbb{E}[\hat{\Psi}]=\Psi \quad \text { and } \quad
\mathbb{E}\left[\hat{\Psi}^2\right]=\frac{N-1}{N} \Psi^2+\frac{1}{N}
\int_{\Omega} \frac{\psi(x)^2}{\rho(x)} \mathrm{d} x \qquad(16)
\]</span></p>
<p>因此，<span class="math inline">\(\hat{\Psi}\)</span>的方差为<span class="math inline">\(N^{-1} \cdot\left(\int_{\Omega}\left(\psi^2 /
\rho\right) \mathrm{d} x-\left(\int_{\Omega} \psi \mathrm{d}
x\right)^2\right)\)</span>。特别地，当均匀分布<span class="math inline">\(\rho(x)=1 /|\Omega|\)</span>时，<span class="math inline">\(\hat{\Psi}=(|\Omega| / N) \cdot \sum_i
\psi\left(x^{(i)}\right)\)</span>的方差为<span class="math inline">\(N^{-1} \cdot\left(|\Omega| \int_{\Omega} \psi^2
\mathrm{~d} x-\left(\int_{\Omega} \psi \mathrm{d}
x\right)^2\right)\)</span>。</p>
<p><strong>注释</strong> 关于引理3，有几点注记：</p>
<ul>
<li><p>积分<span class="math inline">\(\Psi\)</span>的估计量<span class="math inline">\(\hat{\Psi}\)</span>是无偏的</p></li>
<li><p>上述<span class="math inline">\(\hat{\Psi}\)</span>的方差在样本配点数<span class="math inline">\(N\)</span>中以<span class="math inline">\(O(1 /
N)\)</span>的速率递减。由Hölder不等式和<span class="math inline">\(\rho\)</span>是一个概率密度，我们知道.</p></li>
</ul>
<p><span class="math display">\[
\left|\int_{\Omega} \psi \mathrm{d} x\right| \leq \int_{\Omega}|\psi|
\mathrm{d} x=\int_{\Omega} \frac{|\psi|}{\sqrt{\rho}} \sqrt{\rho}
\mathrm{d} x \leq\left(\int_{\Omega} \frac{|\psi|^2}{\rho} \mathrm{~d}
x\right)^{1 / 2}\left(\int_{\Omega} \rho \mathrm{d} x\right)^{1 /
2}=\left(\int_{\Omega} \frac{|\psi|^2}{\rho} \mathrm{~d} x\right)^{1 /
2}
\]</span></p>
<p>这也验证了<span class="math inline">\(\mathrm{V}(\hat{\Psi}) \geq
0\)</span>。更重要的是，当<span class="math inline">\(\psi\)</span>不变号且<span class="math inline">\(\rho
\propto|\psi|\)</span>时，等式成立。因此，我们可以设置<span class="math inline">\(\rho\)</span>尽可能地接近<span class="math inline">\(|\psi|\)</span>
(直到一个归一化常数)，以减小方差，但同时保证<span class="math inline">\(\rho\)</span>易于从(15)中采样和评估。这与重要性抽样的概念密切相关。</p>
<ul>
<li>引理3中的结果(15)和(16)可以很容易地推广到无界区域<span class="math inline">\(\Omega\)</span>的情形，只要<span class="math inline">\(\psi / \sqrt{\rho} \in L^2(\Omega)\)</span></li>
</ul>
<p><strong>引理3为式(14)提供了一种可行的逼近<span class="math inline">\(L(\theta)\)</span>梯度的方法</strong>。例如，要计算<span class="math inline">\(\nabla_\theta L_{\text {bdry
}}(\theta)\)</span>，可以取式(12)关于<span class="math inline">\(\theta\)</span>的梯度，在边界<span class="math inline">\(\partial \Omega\)</span>上采样<span class="math inline">\(N_b\)</span>个配置点<span class="math inline">\(\left\{x_b^{(i)}: 1 \leq i \leq
N_b\right\}\)</span> ，通过对采样点处的函数值求和近似<span class="math inline">\(\nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>。如果我们取<span class="math inline">\(\mathcal{B}[u, \gamma]=\left(u-u_b,
\gamma-\gamma_b, \partial_{\vec{n}} u-u_n\right)\)</span>，且<span class="math inline">\(x_b^{(i)}\)</span>是一致样本，则估计变为</p>
<p><span class="math display">\[
\begin{gathered}
\nabla_\theta L_{\mathrm{bdry}}(\theta)=2 \int_{\partial
\Omega}\left(\left(u_\theta-u_b\right) \nabla_\theta
u_\theta+\left(\gamma_\theta-\gamma_b\right) \nabla_\theta
\gamma_\theta+\left(\partial_{\vec{n}} u_\theta-u_n\right) \nabla_\theta
\nabla u \cdot \vec{n}\right) \mathrm{d} S(x) \\
\approx \frac{2|\partial \Omega|}{N_b}
\sum_{i=1}^{N_b}\left(\left(u_\theta\left(x_b^{(i)}\right)-u_b\left(x_b^{(i)}\right)\right)
\nabla_\theta
u_\theta\left(x_b^{(i)}\right)+\left(\gamma_\theta\left(x_b^{(i)}\right)-\gamma_b\left(x_b^{(i)}\right)\right)
\nabla_\theta \gamma_\theta\left(x_b^{(i)}\right)\right. \\
\left.+\left(\partial_{\vec{n}}
u_\theta\left(x_b^{(i)}\right)-u_b\left(x_b^{(i)}\right)\right)
\nabla_\theta \nabla u_\theta x_b^{(i)} \cdot \vec{n} x_b^{(i)}\right)
\end{gathered} \qquad(17)
\]</span></p>
<p>类似地，我们也可以计算出<span class="math inline">\(\nabla_\theta
L_{\text {int }}(\theta)\)</span>的随机梯度。若给定<span class="math inline">\(f\)</span>，在区域<span class="math inline">\(\Omega\)</span>上取<span class="math inline">\(\mathcal{A}[u, \gamma]=\nabla \cdot(\gamma \nabla
u)-f\)</span>，且在区域<span class="math inline">\(\Omega\)</span>内均匀采样<span class="math inline">\(N_r\)</span>个配点<span class="math inline">\(\left\{x_r^{(i)}: 1 \leq i \leq
N_r\right\}\)</span> 在区域<span class="math inline">\(\Omega\)</span>内，则<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)\)</span>可由下式估计</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\theta L_{\mathrm{int}}(\theta) &amp; =2 I(\theta)
\int_{\Omega}\left(\nabla_\theta \gamma_\theta\left(\nabla u_\theta
\cdot \nabla
\varphi_{\eta(\theta)}\right)+\gamma_\theta\left(\nabla_\theta \nabla
u_\theta \cdot \nabla \varphi_{\eta(\theta)}\right)\right) \mathrm{d}
S(x) \\
&amp; \approx \frac{2|\Omega| \hat{I}(\theta)}{N_r}
\sum_{i=1}^{N_r}\left(\nabla_\theta
\gamma_\theta\left(x_r^{(i)}\right)\left(\nabla
u_\theta\left(x_r^{(i)}\right) \nabla
\varphi_{\eta(\theta)}\left(x_r^{(i)}\right)\right)+\gamma_\theta\left(x_r^{(i)}\right)\left(\nabla_\theta
\nabla u_\theta\left(x_r^{(i)}\right) \nabla
\varphi_{\eta(\theta)}\left(x_r^{(i)}\right)\right)\right)
\end{aligned}  \qquad(18)
\]</span></p>
<p>其中<span class="math inline">\(I(\theta)\)</span>及其估计量<span class="math inline">\(\hat{I}(\theta)\)</span>由下式给出</p>
<p><span class="math display">\[
I(\theta)=\int_{\Omega} \gamma_\theta\left(\nabla u_\theta \cdot \nabla
\varphi_{\eta(\theta)}\right) \mathrm{d} x, \quad
\hat{I}(\theta)=\frac{|\Omega|}{2} \sum_{i=1}^{N_r}
\gamma_\theta\left(x_r^{(i)}\right)\left(\nabla
u_\theta\left(x_r^{(i)}\right) \cdot \nabla
\varphi_{\eta(\theta)}\left(x_r^{(i)}\right)\right)
\]</span></p>
<p>并且根据引理2，<span class="math inline">\(\eta(\theta)\)</span>是最大化问题(11)的一个解。梯度中的所有积分都可以用类似的方式进行近似。这些近似梯度实际上是随机梯度，它是无偏的，并且由于网络参数的有界性而具有有界的方差。通过这些近似，式(14)退化为随机投影梯度下降法，通过选择合适的步长，可以保证收敛到式(13)的局部稳定点。由于(13)是有约束的，梯度映射<span class="math inline">\(\mathcal{G}(\theta):=\tau^{-1}\left[\theta-\Pi\left(\theta-\tau
\nabla_\theta L(\theta)\right)\right]\)</span>被用作<span class="math inline">\(\theta\)</span>的收敛准则[37,62,75]。值得注意的是，梯度映射的定义考虑了步长<span class="math inline">\(\tau\)</span>的归一化。 此外，在没有投影<span class="math inline">\(\Pi\)</span>的情况下，梯度映射退化为<span class="math inline">\(\mathcal{G}(\theta)=\nabla_\theta
L(\theta)\)</span>，其大小是无约束情况下局部稳定点(即, <span class="math inline">\(\left|\nabla_\theta
L(\theta)\right|=0\)</span>)的评价标准.这一结果在下面的定理中给出，并在附录A.4.中给出证明。</p>
<p><span class="math display">\[
\begin{array}{l}
   \hline \
   \textbf{算法1} \quad \text{Inverse Problem Solver by Weak Adversarial
Network(IWAN)} \\
   \hline
   \qquad \textbf{输入：}反问题(1)的区域\Omega和数据 \\
   \qquad \textbf{初始化：}\left(u_\theta, \gamma_\theta\right),
\varphi_\eta \\
   \qquad \textbf{for} \, j=1, \ldots, J \,\textbf{do} \\
   \qquad \quad \text { Sample } X_r=\left\{x_r^{(i)}: 1 \leq i \leq
N_r\right\} \subset \Omega \text { and } X_b=\left\{x_b^{(i)}: 1 \leq i
\leq N_b\right\} \subset \partial \Omega \text {. }\\
   \qquad \quad \eta \leftarrow \operatorname{SGD}\left(-\nabla_\eta
E(\theta, \eta), X_r, \eta, \tau_\eta, J_\eta\right) \text {. }\\
   \qquad \quad \theta \leftarrow
\operatorname{SGD}\left(\partial_\theta E(\theta, \eta)+\beta
\nabla_\theta L_{\mathrm{bdry}}(\theta),\left(X_r, X_b\right), \theta,
\tau_\theta, 1\right)\\
   \qquad \textbf{end for} \\
   \qquad \textbf{输出：}\left(u_\theta, \gamma_\theta\right) \\
   \hline
\end{array}
\]</span></p>
<p><strong>定理4</strong> 对于任意的<span class="math inline">\(\varepsilon&gt;0\)</span>，令<span class="math inline">\(\left\{\theta_j\right\}\)</span>是由梯度下降算法(14)生成的<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>中的网络参数序列，在每次迭代中用样本复杂度为<span class="math inline">\(N_r,
N_b=O\left(\varepsilon^{-1}\right)\)</span>的样本均值逼近<span class="math inline">\(\nabla_\theta L(\theta)\)</span>中的积分，则<span class="math inline">\(J=O\left(\varepsilon^{-1}\right)\)</span>次迭代后<span class="math inline">\(\min _{1 \leq j \leq J}
\mathbb{E}\left[\left|\mathcal{G}\left(\theta_j\right)\right|^2\right]
\leq \varepsilon\)</span></p>
<p><strong>注释</strong> 定理4建立了(14)到问题的所谓<span class="math inline">\(\varepsilon\)</span>-解的收敛性和迭代复杂性.该结果是基于梯度映射的期望大小，这是非凸约束随机优化中的一个标准收敛准则。然而，这只能保证在期望意义下逼近一个稳定点(不一定是局部的或全局的极小点)。在理论上，我们可以将额外的全局优化技术应用到(7)中，以便找到一个具有较高计算成本的全局最优解(最好只有高概率才有可能)。然而，我们在本工作中不会对这一问题做进一步的探讨。</p>
<p>现在我们总结了我们的算法使用弱对抗网络求解IPs的步骤。为了简化表述，我们引入如下符号来表示寻找损失函数<span class="math inline">\(L(\theta)\)</span>的极小点的随机梯度下降(SGD)过程：</p>
<p><span class="math display">\[
\theta^* \leftarrow \operatorname{SGD}\left(G(\theta), X, \theta_0,
\tau, J\right) \qquad(19)
\]</span></p>
<p>也就是说，意味着输出<span class="math inline">\(\theta^*\)</span>是对<span class="math inline">\(j=0, \ldots, J-1\)</span>执行步长为<span class="math inline">\(\tau\)</span>的(投影) SGD方案，有初始<span class="math inline">\(\theta_0\)</span>之后的结果<span class="math inline">\(\theta_J\)</span>:</p>
<p><span class="math display">\[
\theta_{j+1} \leftarrow \Pi\left(\theta_j-\tau \hat{G}\left(\theta_j ;
X\right)\right) \qquad(20)
\]</span></p>
<p>这里<span class="math inline">\(X=\left\{x^{(i)}: 1 \leq i \leq
N\right\}\)</span>是<span class="math inline">\(N\)</span>个采样配置点的集合，<span class="math inline">\(G(\theta):=\nabla_\theta
L(\theta)\)</span>是损失函数<span class="math inline">\(L(\theta)\)</span>的梯度，<span class="math inline">\(\hat{G}(\theta ; X)\)</span>表示<span class="math inline">\(G(\theta)\)</span>在任意给定的<span class="math inline">\(\theta\)</span>下的随机逼近，其中积分的估计如(15)中利用采样配置点<span class="math inline">\(X\)</span>。在第一步中，我们固定<span class="math inline">\(\theta\)</span>，通过对<span class="math inline">\(J_\eta\)</span>步施加随机梯度上升来求解目标函数<span class="math inline">\(E(\theta,
\eta)\)</span>在(11)中定义的最大化问题，从而得到一个近似的最大化子<span class="math inline">\(\eta\)</span>；在第二步中，我们固定这个<span class="math inline">\(\eta\)</span>，利用梯度<span class="math inline">\(\nabla_\theta L(\theta)=\partial_\theta E(\theta,
\eta)+\beta \nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>通过一个随机梯度下降步来更新<span class="math inline">\(\theta\)</span>
。然后进入步骤1，开始下一次迭代。因此，我们的目标函数为<span class="math inline">\(E(\theta, \eta)+\beta
L_{\mathrm{bdry}}(\theta)\)</span>，我们通过<span class="math inline">\(\min _\theta \max _\eta E(\theta, \eta)+\beta
L_{\mathrm{bdry}}(\theta)\)</span>的min-max优化来寻找最优点<span class="math inline">\(\left(\theta^*,
\eta^*\right)\)</span>。这个过程被称为使用弱对抗网络(IWAN)的反问题求解器，并在算法1中总结。数值实现中的参数值在第4节中给出</p>
<h2 id="数值实验">数值实验</h2>
<h3 id="实施细则">实施细则</h3>
<p>在这一部分中，我们讨论了关于算法1的一些实现细节和修改。首先，为了避免在固定<span class="math inline">\(\theta\)</span>的情况下求解式(11)中的内部最大化问题<span class="math inline">\(\max _\eta E(\theta,
\eta)\)</span>花费过多的时间，我们只使用少量的迭代次数<span class="math inline">\(J_\eta\)</span>来计算 <span class="math inline">\(\eta\)</span>。然后我们切换到更新<span class="math inline">\(\theta\)</span>进行一次迭代。见算法1中的两个SGD步骤。这样可以提高整体效率，避免在<span class="math inline">\(\eta\)</span>的内部最大化问题上花费过多的时间，特别是当<span class="math inline">\(\theta\)</span>还远未达到最优时。事实上，我们可以采用两个单独的测试函数<span class="math inline">\(\varphi_\eta\)</span>和<span class="math inline">\(\bar{\varphi}_\eta\)</span>
(为了记号的简洁性,我们又用同样的<span class="math inline">\(\eta\)</span>)。在每次迭代<span class="math inline">\(j\)</span>中，我们按顺序交替更新<span class="math inline">\(\left(u_\theta, \varphi_\eta, \gamma_\theta,
\bar{\varphi}_\eta\right)\)</span>，每个<span class="math inline">\(\left(u_\theta, \varphi_\eta, \gamma_\theta,
\bar{\varphi}_\eta\right)\)</span>都有一个或几个SGD步(20)。我们将为下面的实验指定这些网络的步数。</p>
<p>在第3节的推导过程中，我们要求有界的网络参数<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\eta\)</span>，其中界<span class="math inline">\(B\)</span>可以任意大，以确保使用样本的积分估计量的有限方差，从而保证SGD收敛。另一种处理有界性约束的方法是在(7)的目标函数中添加<span class="math inline">\(|\theta|^2\)</span>和<span class="math inline">\(|\eta|^2\)</span>作为正则项。我们也可以使用分母为<span class="math inline">\(\|\varphi\|_2^2:=\int_{\Omega}|\varphi|^2
\mathrm{~d}
x\)</span>(用MC近似,类似于式(15))的算子范数(5)，这在我们的实现中也被采用。这种替换不会引起数值实现上的问题，因为测试函数<span class="math inline">\(\varphi_\eta\)</span>是由一个具有固定宽度/深度和有界参数的网络实现的，因此可以保证在<span class="math inline">\(H^1\)</span>中。</p>
<p>弱式(3)要求一个检验函数<span class="math inline">\(\varphi_\eta\)</span>在<span class="math inline">\(\partial
\Omega\)</span>上消失。保证这一点的一个简单技巧是，预先计算一个函数<span class="math inline">\(\varphi_0 \in C(\Omega)\)</span>，使得当<span class="math inline">\(x \in \partial \Omega\)</span>时，<span class="math inline">\(\varphi_0(x)=0\)</span>且当<span class="math inline">\(x \in \Omega\)</span>时，<span class="math inline">\(\varphi_0(x)&gt;0\)</span>(e.g.,一个到<span class="math inline">\(\partial
\Omega\)</span>的距离函数)。然后寻找一个对其任意边界无约束的参数化网络<span class="math inline">\(\varphi_\eta^{\prime}\)</span>，将测试函数<span class="math inline">\(\varphi_\eta\)</span>设置为<span class="math inline">\(\varphi_0 \varphi_\eta^{\prime}\)</span>，且在∂
<span class="math inline">\(\partial \Omega\)</span>上仍取零值。</p>
<p>我们使用TensorFlow [1]
(Python版本3.7)实现了我们的算法，这是一个先进的深度学习包，可以有效地利用GPU进行并行计算。通过TensorFlow内置的自动微分模块计算关于网络参数(<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\eta\)</span>)和输入(<span class="math inline">\(x\)</span>)的梯度。在训练过程中，我们还可以用它的许多变体来代替标准的SGD优化器，如AdaGrad，RMSprop算法，Adam，Nadam等。在我们的实验中，我们使用了TensorFlow包提供的AdaGrad，在我们的大部分测试中，AdaGrad似乎提供了比其他优化器更好的性能。其他所有参数，如网络结构(层数和神经元数)、步长(也称为学习率)、迭代次数等将在第4节中指定</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">图像处理的偏微分方程方法总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-03 11:18:11" itemprop="dateCreated datePublished" datetime="2025-03-03T11:18:11+08:00">2025-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-05 15:39:20" itemprop="dateModified" datetime="2025-03-05T15:39:20+08:00">2025-03-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E4%B9%A6%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">图书总结</span></a>
                </span>
            </span>

          
            <span id="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/" class="post-meta-item leancloud_visitors" data-flag-title="图像处理的偏微分方程方法总结" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<h2 id="变分法和梯度下降流">变分法和梯度下降流</h2>
<h3 id="变分原理">变分原理</h3>
<p>所希望的解往往是由最小化某一能量泛函所确定的，在一维情况下，这一泛函可能有如下形式：</p>
<p><span class="math display">\[
E(u)=\int_{x_0}^{x_1}F(x,u,u_x)\,dx  \qquad (1)
\]</span></p>
<p><span class="math inline">\(E(u)\)</span>的极值对应于变分<span class="math inline">\(\frac{\partial E}{\partial
u}=0\)</span>所对应的函数。为了求出一阶变分<span class="math inline">\(E&#39;\)</span>，考虑对最优解<span class="math inline">\(u(x)\)</span>作一微扰，得<span class="math inline">\(u(x)+v(x)\)</span>，经推导有</p>
<p><span class="math display">\[
E(u+v)=E(u)+\int_{x_0}^{x_1}[v\frac{\partial F}{\partial
u}-v\frac{d}{dx}(\frac{\partial F}{\partial u&#39;})] \, dx \qquad(2)
\]</span></p>
<p>可见，当<span class="math inline">\(E(u)\)</span>达到极值，对<span class="math inline">\(u(x)\)</span>的任一足够小微扰<span class="math inline">\(v(x),E\)</span>的值不变，故有</p>
<p><span class="math display">\[
\frac{\partial F}{\partial u}-\frac{d}{dx}(\frac{\partial F}{\partial
u&#39;})=0
\]</span></p>
<p>此式称为变分问题式(1)的Euler方程。 二维情况：</p>
<p><span class="math display">\[
E(u)=\iint_\Omega F(x,y,u,u_x,u_y)dxdy
\]</span></p>
<p>对应的Euler方程为</p>
<p><span class="math display">\[
\frac{\partial F}{\partial u}-\frac{d}{dx}(\frac{\partial F}{\partial
u_x})-\frac{d}{dy}(\frac{\partial F}{\partial u_y})=0
\]</span></p>
<p>求解能量泛函极值问题归结为求解相应的Euler方程。</p>
<h3 id="梯度下降流">梯度下降流</h3>
<p>假定我们要求的解可随时间变化，即它可以表示为<span class="math inline">\(u(\cdot,t)\)</span>，并且这种随时间的变化总是使<span class="math inline">\(E(u(\cdot,t))\)</span>减小，那么<span class="math inline">\(u(\cdot,t)\)</span>应该怎样变化才能满足这一要求？以一维问题为例，令式（2）中的微扰项<span class="math inline">\(v(\cdot)\)</span>是由<span class="math inline">\(u(\cdot,t)\)</span>从<span class="math inline">\(t\)</span>到<span class="math inline">\(t+\Delta
t\)</span>所产生的改变量，即</p>
<p><span class="math display">\[
v=\frac{\partial u}{\partial t}\Delta t
\]</span></p>
<p>式（2）就可改写为</p>
<p><span class="math display">\[
E(\cdot,t+\Delta t)=E(\cdot ,t)+\Delta t \int^{x_1}_{x_0}\frac{\partial
u}{\partial t}[\frac{\partial F}{\partial u}-\frac{d}{dx}(\frac{\partial
F}{\partial u&#39;})]\, dx
\]</span></p>
<p>于是只要令</p>
<p><span class="math display">\[
\frac{\partial u}{\partial t}=-[\frac{\partial F}{\partial
u}-\frac{d}{dx}(\frac{\partial F}{\partial
u&#39;})]=\frac{d}{dx}(\frac{\partial F}{\partial
u&#39;})-\frac{\partial F}{\partial u} \qquad(3)
\]</span></p>
<p>就可以使<span class="math inline">\(E(u(\cdot,t))\)</span>不断减小，式（3）称为变分问题式（1）所对应的梯度下降流。</p>
<p>这样一来，我们可以从某一适当选定的初始函数<span class="math inline">\(u_0\)</span>开始，根据式（3）作迭代计算，直到<span class="math inline">\(u\)</span>达到稳态解为止。这时 <span class="math display">\[
\frac{\partial u}{\partial t}=0 \Rightarrow \frac{\partial F}{\partial
u}-\frac{\mathrm{d}}{\mathrm{~d} x}\left(\frac{\partial F}{\partial
u^{\prime}}\right)=0
\]</span> 可见梯度下降流式（3）的稳态解也就是Euler方程式的解。</p>
<p>对于二维变分问题，类似推导，可得梯度下降流 <span class="math display">\[
\frac{\partial u}{\partial t}=\frac{\mathrm{d}}{\mathrm{~d}
x}\left(\frac{\partial F}{\partial
u_x}\right)+\frac{\mathrm{d}}{\mathrm{~d} y}\left(\frac{\partial
F}{\partial u_y}\right)-\frac{\partial F}{\partial u}
\]</span></p>
<p>值得特别注意的是：只有当<span class="math inline">\(E(u)\)</span>是凸性的，它有唯一极小值，从而梯度下降流可得到与初条件无关的唯一解。而当<span class="math inline">\(E(u)\)</span>非凸性时，梯度下降流可能由于选用不同的初条件<span class="math inline">\(u_0(x)\)</span>而得到不同的局部极小值而不是全局最小值。</p>
<h2 id="曲线演化问题">曲线演化问题</h2>
<h3 id="曲线几何演化的一般方程式">曲线几何演化的一般方程式</h3>
<p>曲线几何演化的一般方程式 <span class="math display">\[
\frac{\partial C(p, t)}{\partial t}=\boldsymbol{V}=\alpha(p, t)
\boldsymbol{T}+\beta(p, t) \boldsymbol{N}, \quad C(p, 0)=C_0(p) \quad(4)
\]</span> 式中<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>分别为切向速率和法向速率。</p>
<p>曲线演化的一般方程式可以简化为 <span class="math display">\[
\frac{\partial C}{\partial t}=\beta \boldsymbol{N} \qquad(5)
\]</span></p>
<h2 id="水平集方法">水平集方法</h2>
<h3 id="基本概念">基本概念</h3>
<p>一条平面封闭曲线可以采用隐式表达方法，即将它定义为一个二维函数<span class="math inline">\(u(x,y)\)</span>的水平集 <span class="math display">\[
C=\lbrace (x,y),u(x,y)=c \rbrace
\]</span></p>
<p>这样，<span class="math inline">\(C\)</span>有某种变化，则我们可以将它归结为是由函数<span class="math inline">\(u(x,y)\)</span>发生了某种相应的变化所引起的。随时间变化的封闭曲线，可表达为随时间变化的二维函数<span class="math inline">\(u(x,y)\)</span>水平集，即 <span class="math display">\[
C(t):=\lbrace (x,y),u(x,y,t)=c \rbrace
\]</span></p>
<p>当曲线<span class="math inline">\(C(t)\)</span>按（4）式演化时，嵌入函数<span class="math inline">\(u(x,y,t)\)</span>应如何演化呢？对上式中的函数<span class="math inline">\(u\)</span>求全导数<span class="math inline">\(\frac{du}{dt}\)</span>，由复合函数求导的链式规则可得
<span class="math display">\[
\frac{du}{dt}=\frac{\partial u}{\partial t}+\nabla u \cdot
\frac{\partial(x,y)}{\partial t}=0
\]</span></p>
<p>考虑到式（4），<span class="math inline">\(\frac{\partial(x,y)}{\partial t}=\frac{\partial
C}{\partial t}=\boldsymbol{V}\)</span>,于是得 <span class="math display">\[
\frac{\partial u}{\partial t}=-\nabla u \cdot \boldsymbol{V}=-|\nabla u|
\frac{\nabla u}{|\nabla u|} \cdot \boldsymbol{V}=|\nabla u|
\boldsymbol{N} \cdot \boldsymbol{V}=\beta|\nabla u|  \qquad(6)
\]</span> 式中，<span class="math inline">\(\beta=\boldsymbol{v}\cdot
\boldsymbol{N}\)</span>是运动速度的法向分量。式（6）就是曲线演化水平集方法的基本方程式。</p>
<p>式（6）的推导与常数<span class="math inline">\(c\)</span>的取值无关，为了方便，常取<span class="math inline">\(c=0\)</span>，即我们关心的曲线是嵌入函数的零水平集。</p>
<p>至此，我们看到，对于封闭曲线<span class="math inline">\(C\)</span>在给定的初值<span class="math inline">\(C_0\)</span>条件下，按式（5）演化问题，等价于嵌入函数<span class="math inline">\(u(x,y)\)</span>在给定初值<span class="math inline">\(u_0(x,y)\)</span>条件下按（6）的演化，也就是说，只要在任何时刻<span class="math inline">\(t\)</span>取出<span class="math inline">\(u(x,y,t)=0\)</span>的水平集就可以确定当前曲线<span class="math inline">\(C(t)\)</span></p>
<h3 id="水平集方法的优点">水平集方法的优点</h3>
<p>第一，水平集方法的PDE是直接在固定坐标系<span class="math inline">\((x,y)\)</span>中给出的，因而是一种无参数的方法</p>
<p>第二，可采用迎风方案作数值实现，以得到粘滞解</p>
<p>第三，曲线在演化过程中可能发生拓扑变化，例如，一条封闭曲线演变为两条封闭曲线。如果采用标注质点法，则要求随时监测这种可能的变化。但是对于水平集方法而言，<span class="math inline">\(u(x,y,t)\)</span>只有数值上的变化而没有拓扑上的变化。曲线在拓扑上的任何变化，都将自动嵌入到<span class="math inline">\(u(x,y,t)\)</span>的数值变化之中。因此没有跟踪曲线拓扑变化和修改实现方案的必要。</p>
<h2 id="变分水平集方法">变分水平集方法</h2>
<h3 id="基本概念-1">基本概念</h3>
<p>在将曲线演化应用于图像处理问题时，曲线运动方程往往来自于最小化闭合曲线<span class="math inline">\(C\)</span>的某一“能量”泛函。例如，测地线活动轮廓模型就是最小化如下泛函：
<span class="math display">\[
E(C)=\oint_C g(C) \, ds \qquad(7)
\]</span></p>
<p>式中<span class="math inline">\(g(x,y)ds\)</span>是“加权弧长微元”。可以证明，式（7）的梯度下降流为
<span class="math display">\[
\frac{\partial C}{\partial t}=[g(C)\kappa-\nabla g \cdot
\boldsymbol{N}]\boldsymbol{N} \qquad (8)
\]</span> 采用前面讨论的水平集方法，则对应的关于嵌入函数的PDE为 <span class="math display">\[
\begin{aligned}
\frac{\partial u}{\partial t} &amp; =(g \kappa-\nabla g \cdot
\boldsymbol{N})|\nabla u| \\
&amp; =|\nabla u| \operatorname{div}\left(g \frac{\nabla u}{|\nabla
u|}\right)
\end{aligned} \qquad(9)
\]</span></p>
<p>针对这类由曲线的能量泛函最小化所导出的曲线演化问题，有一种新的水平集方法，称之为变分水平集方法。</p>
<p>首先，利用如下定义的特殊函数（Heaviside函数）： <span class="math display">\[
H(z)= \begin{cases}1, &amp; z \geqslant 0 \\ 0, &amp; z&lt;0\end{cases}
\]</span></p>
<p>可将关于沿<span class="math inline">\(C\)</span>的环路积分式（7）在形式上改写为面积分
<span class="math display">\[
\oint_C g(C) \mathrm{d} s=\iint_{\Omega} g(x, y)|\nabla H(u)| \mathrm{d}
x \mathrm{~d} y
\]</span></p>
<p>由于 <span class="math display">\[
\nabla H(u)=\delta(u) \nabla u, \quad \delta(z)=\frac{\mathrm{d}
H(z)}{\mathrm{d} z}
\]</span> 这样一来，式（7）便可改写为嵌入函数<span class="math inline">\(u\)</span>的泛函 <span class="math display">\[
E(u)=\iint_\Omega g(x, y) \delta(u)|\nabla u| \mathrm{d} x \mathrm{~d} y
\qquad(7&#39;)
\]</span> 利用变分法，可以得到上式的梯度下降流 <span class="math display">\[
\frac{\partial u}{\partial t}=\delta(u) \operatorname{div}\left(g
\frac{\nabla u}{|\nabla u|}\right) \qquad(10)
\]</span> 为了使它成为可实际计算的PDE，式中的<span class="math inline">\(\delta\)</span>函数需用正则化的<span class="math inline">\(\delta_{\epsilon}\)</span>作近似，即将（10）改写为
<span class="math display">\[
\frac{\partial u}{\partial t}=\delta_{\varepsilon}(u)
\operatorname{div}\left(g \frac{\nabla u}{|\nabla u|}\right) \qquad(11)
\]</span> 式中 <span class="math display">\[
\delta_{\varepsilon}(z):=\frac{\mathrm{d}}{\mathrm{~d} z} H_\epsilon(z)
\]</span> 这里，<span class="math inline">\(H_{\epsilon}(z)\)</span>称为正则化的Heaviside函数。原则上它可以是任意满足如下条件：
<span class="math display">\[
H_{\varepsilon}(z) \xrightarrow{\varepsilon \rightarrow 0} H(z)
\]</span> 的函数。</p>
<p>从表面上看，PDE式（11）与（9）似乎差别不大，只是用<span class="math inline">\(\delta_{\epsilon}(u)\)</span>取代了<span class="math inline">\(|\nabla
u|\)</span>。但是，两者在数学上有本质的差别。式（9）属于双曲型，式（11）属于抛物型，稳定性较前者高。因而在数值实现时，可用较大的时间步长，并且常常无需对嵌入函数进行重新初始化。</p>
<p>但是，这并不意味变分水平集方法就可取代水平集。因为能够采用变分水平集方法来求解曲线演化问题的前提是：该问题是来自最小化曲线<span class="math inline">\(C\)</span>的“能量”泛函<span class="math inline">\(E(u)\)</span>。这时通过引入嵌入函数<span class="math inline">\(u\)</span>和利用Heaviside函数，将<span class="math inline">\(E(C)\)</span>改造成<span class="math inline">\(E(u)\)</span>，通过变分法得到关于<span class="math inline">\(u\)</span>的PDE。而水平集方法是：先利用变分法最小化<span class="math inline">\(C\)</span>的“能量”泛函，得到关于<span class="math inline">\(C\)</span>的运动方程后，再引入嵌入函数，得到关于<span class="math inline">\(u\)</span>的PDE。问题在于，并不是所有的曲线和曲面的演化问题都是由“能量”泛函最小化而导出的。水平集方法是较变分水平集方法适用面更广的方法。</p>
<h2 id="曲线演化的线性热流">曲线演化的线性热流</h2>
<h3 id="线性几何热流">线性几何热流</h3>
<p>考虑一条简单封闭平面曲线</p>
<p><span class="math display">\[
C_0(p)=(x_0(p),y_0(p))
\]</span></p>
<p>为初始条件，按照热方程演化</p>
<p><span class="math display">\[
\frac{\partial C}{\partial t}=\frac{\partial^2 C}{\partial p^2}
\qquad(12)
\]</span></p>
<p>得到曲线族</p>
<p><span class="math display">\[
C(p,t)=(x(p,t),y(p,t)), \qquad C(p,0)=C_0(p)
\]</span></p>
<p>用Fourier方法求解得到</p>
<p><span class="math display">\[
x(p,t)=x_0(p)*g(p,t),\qquad y(p,t)=y_0(p)*g(p,t)
\\g(p,t)=\frac{1}{\sqrt{4\pi t}}\exp[\frac{-p^2}{4t}]
\]</span></p>
<p>由此可见，曲线按式(12)做线性热运动，等价于对曲线上的每一点坐标<span class="math inline">\((x,y)\)</span>同时作Gaussian滤波。并且Gaussian滤波器的标准偏离<span class="math inline">\(\sigma\)</span>与演化时间<span class="math inline">\(t\)</span>有以下对应关系：</p>
<p><span class="math display">\[
\sigma=\sqrt{2}t
\]</span></p>
<h2 id="非线性几何不变流">非线性几何不变流</h2>
<h3 id="euclidean不变流">Euclidean不变流</h3>
<p>线性热流式（12）提供了一个很好的平面曲线的多尺度表达，但它存在一个严重的不足之处，它不能保证一个简单的（不自相交的）闭合曲线在演化过程中始终保持为一条简单封闭曲线。</p>
<p>如果用曲线的Euclidean弧长<span class="math inline">\(s\)</span>取代参数<span class="math inline">\(p\)</span>，则式（12）变为</p>
<p><span class="math display">\[
\frac{\partial C}{\partial t}=\frac{\partial^2 C}{\partial s^2}
\qquad(13)
\]</span></p>
<p>考虑到<span class="math inline">\(C_s=T,C_{ss}=T_s=\kappa
N\)</span>，故式（13）可改写为</p>
<p><span class="math display">\[
\frac{\partial C}{\partial t}=\kappa N \qquad(14)
\]</span> 可见Euclidean几何不变流也就是（平均）曲率运动（MCM）方程。</p>
<p>如果令<span class="math inline">\(E(C)=\oint_C
g(C)\,ds\)</span>中的函数<span class="math inline">\(g(x,y)=1\)</span>，即能量泛函简化为 <span class="math display">\[
E(C)=\oint_C ds \qquad(15)
\]</span> 那么，最小化这一泛函的意义就是使闭合曲线<span class="math inline">\(C\)</span>的全弧长缩短。这时对应的梯度下降流式（8）中，令<span class="math inline">\(g=1,\nabla
g=0\)</span>，它将简化为式（14）。据此，也可将式（14）称为弧长缩短流。</p>
<p>可以从三个不同的出发点得到同一个曲线运动方程：
（1）集合论的形态学算子——中值滤波，在原盘结构元素半径<span class="math inline">\(r \to 0\)</span>的极限
（2）Euclidean不变几何流——以Euclidean弧长为参数的热流
（3）弧长缩短流——最小化式（15）的梯度下降流。</p>
<h3 id="仿射不变几何流">仿射不变几何流</h3>
<p>另一个有重要意义的曲线演化PDE是 <span class="math display">\[
\frac{\partial C}{\partial t}=\frac{\partial^2 C}{\partial v^2}
\qquad(16)
\]</span> 式中<span class="math inline">\(v\)</span>表示仿射弧长。有
<span class="math display">\[
C_{v v}=\kappa^{1 / 3} \boldsymbol{N}+f\left(\kappa, \kappa_v\right)
\boldsymbol{T} \\
f\left(\kappa, \kappa_v\right)=\frac{\mathrm{d}^2 s}{\mathrm{~d}
v^2}=\left(\kappa^{-1 / 3}\right)_v
\]</span>
考虑到运动速度的切线分量不影响曲线的几何形变，所以式（16）在几何上等价于
<span class="math display">\[
\frac{\partial C}{\partial t}=\kappa^{1 / 3} N \qquad(17)
\]</span>
此式称为仿射不变几何流。它将任何简单的非凸闭合形变为全凸的闭合曲线，在演化过程中，曲线逐渐平滑，不产生奇异性，也不发生自相交，因而它也生成一个尺度空间，称为仿射形态学尺度空间（AMSS）。</p>
<p>对（17）采用水平集方法，可以得到关于嵌入函数<span class="math inline">\(u\)</span>的PDE为 <span class="math display">\[
\frac{\partial u}{\partial t}=\kappa^{1 / 3}|\nabla u|=\left(u_{x x}
u_y^2-2 u_x u_y u_{x y}+u_{y y} u_x^2\right)^{1 / 3}
\]</span>
由于在此PDE中不含有分式，因而不会像MCM方程那样出现“被零除”的问题。</p>
<h2 id="测地线活动轮廓模型">测地线活动轮廓模型</h2>
<h3 id="活动轮廓模型的基本概念">活动轮廓模型的基本概念</h3>
<p>图像分割中的活动轮廓模型（active
contour）或“蛇”（snake）模型的基本思想是将图像分割问题归结为最小化一个封闭曲线<span class="math inline">\(C(p)\)</span>的“能量”泛函： <span class="math display">\[
E[C(p)]=\alpha \int_0^1\left|C_p(p)\right| \mathrm{d} p+\beta
\int_0^1\left|C_{p p}(p)\right|^2 \mathrm{~d} p-\lambda \int_0^1|\nabla
I[C(p)]| \mathrm{d} p \qquad(18)
\]</span> 式中第一项的积分是曲线的Euclidean弧长，第二项 <span class="math display">\[
\int_0^1\left|C_{p p}(p)\right|^2 \mathrm{~d}
p=\int_0^1|\kappa|^2\left(\frac{\mathrm{~d} p}{\mathrm{~d} s}\right)^2
\mathrm{~d} p
\]</span>
表示曲线“振荡”的能量。因而最小化这两项就是要求闭合曲线尽可能短并且尽可能光顺。但根据关于曲率运动演化性质的讨论可知，在短程化弧长的过程中，也将使曲线逐渐光顺，因而第二项可不必单独提出。如果图像中的对象与背景的分解处存在灰度值的较大差异，那么对象的轮廓就将形成明显的边缘，即图像的梯度模值<span class="math inline">\(|\nabla
I|\)</span>，在对象的边界将达到局部极大值。注意到第三项的符号为负，最小化<span class="math inline">\(E(C)\)</span>对应于最大化第三项，这就要求曲线<span class="math inline">\(C\)</span>尽可能落在<span class="math inline">\(|\nabla
I|\)</span>达到极大值的位置上。由此可见，“蛇”模型的基本出发点也是“基于边缘的”。由于这一模型的前提是<span class="math inline">\(C(p)\)</span>为封闭曲线，因而利用“蛇”模型分割时。将不会产生边缘断裂问题。</p>
<p>为了避免出现第三项的负号，可引入一个新的函数<span class="math inline">\(g(r),r \in
\R^+\)</span>。原则上，它可以时任何具有单调递减的非负函数，以保证<span class="math inline">\(|\nabla I|\)</span>的局部极大值对应于<span class="math inline">\(g(|\nabla
I|)\)</span>的局部极小值，从而使最大化<span class="math inline">\(\int^1_0 |\nabla I[C(p)]| \,
dp\)</span>等价于最小化<span class="math inline">\(\int g(\nabla
I[C(p)])\,
dp\)</span>。略去式（18）中的第二项，并按上述方法修改第三项，此模型便可改写成
<span class="math display">\[
E[C(p)]=\alpha_1 \int_0^1\left|C_p(p)\right| \mathrm{d} p+\alpha_2
\int_0^1 g(\nabla I[C(p)]) \mathrm{d} p \qquad(19)
\]</span></p>
<p>但是，式（19）仍然存在一个严重缺陷，即它不依赖于曲线<span class="math inline">\(C\)</span>的几何形状和位置，而且还依赖于曲线的参数<span class="math inline">\(p\)</span>。为了克服这一缺陷，提出了不含自由参数测地线活动轮廓（geodesic
active
contour，GAC）模型。GAC模型的提出是PDE方法在图像分割应用中的重大突破。</p>
<h3 id="gac模型的建立">GAC模型的建立</h3>
<p>文献[35]提出用最小化以下“能量”泛函来确定活动轮廓： <span class="math display">\[
L_R(C)=\int_0^{L(C)} g(|\nabla I[C(s)]|) \mathrm{d} s \qquad(20)
\]</span></p>
<p>式中<span class="math inline">\(L(C)\)</span>表示闭合曲线<span class="math inline">\(C\)</span>的弧长，而<span class="math inline">\(L_R
(C)\)</span>则是“加权弧长”。由于以上泛函是建立在曲线固有参数——弧长之上的，消除了经典蛇模型依赖自由参数的缺陷。</p>
<p>可以证明，最小化式（20）所对应的梯度下降流为 <span class="math display">\[
\frac{\partial C}{\partial t}=g(C) \kappa N-(\nabla g \cdot N) N
\qquad(21)
\]</span></p>
<h3 id="gac模型的行为分析">GAC模型的行为分析</h3>
<p>对（21）的行为作一定性分析。我们看到式（21）右边的前一项是平均曲率运动（MCM）乘以非负标量因子<span class="math inline">\(g(x,y)\)</span>，因而这一项与MCM的行为是一致的，即在曲率为正的局部，曲线向内部收缩，而在曲率为负的局部，曲线将向外扩张，曲线的总长度将逐渐缩短，并逐渐平滑（曲率过零点逐渐减少）。不过现在这一运动收到函数<span class="math inline">\(g(x,y)\)</span>的控制，即在平坦区，有 <span class="math display">\[
|\nabla I| \approx 0 \Rightarrow g \approx 1
\]</span> 曲线将完全按照曲率运动方程演化，但在图像边缘附近，则因为 <span class="math display">\[
|\nabla I| \gg K \Rightarrow g \approx 0
\]</span> 使第一项失去作用。</p>
<p>式（21）后一项的行为值得更仔细的考察。在图像平坦区，由于<span class="math inline">\(g \approx 1 \Rightarrow \nabla g \approx
0\)</span>，于是这一项基本失去作用。对于在图像边缘附近的情况，用图4.14来进行讨论。图4.14中的闭合曲线表示一个“对象”的边缘，假定其内部灰度值较外部低。由于梯度模值在边缘达到局部极大值，从而边缘函数<span class="math inline">\(g\)</span>达到局部极小值。在图4.14中分别用<span class="math inline">\(|\nabla I|\)</span>和<span class="math inline">\(g(|\nabla I|)\)</span>曲线表示。由于<span class="math inline">\(\nabla g\)</span>的方向总是指向<span class="math inline">\(g\)</span>增大的方向，故不论在物体的内部还是外部，<span class="math inline">\(\nabla
g\)</span>总是指向离开边缘的方向。现在假定曲线<span class="math inline">\(C(t)\)</span>已经运动到物体的边缘附近，按规定<span class="math inline">\(C(t)\)</span>的法方向<span class="math inline">\(N\)</span>总是指向曲线的内部。于是，如果当前曲线的位置是处在边界的外部，那么<span class="math inline">\(N\)</span>将与<span class="math inline">\(\nabla
g\)</span>方向相反，即<span class="math inline">\((\nabla g \cdot
N)\)</span>为负值，因而<span class="math inline">\(-(\nabla g \cdot
N)N\)</span>与<span class="math inline">\(N\)</span>相一致，可见这时第二项的作用是使<span class="math inline">\(C(t)\)</span>从边界外部。向更靠近边界的方向运动；反之，如果当前曲线的位置是处在边界的内部，那么，由于<span class="math inline">\(\nabla g\)</span>与<span class="math inline">\(N\)</span>相一致，即<span class="math inline">\(\nabla g \cdot N\)</span>取正值，故<span class="math inline">\(-(\nabla g \cdot N)N\)</span>将与<span class="math inline">\(N\)</span>方向相反，也就是说第二项的作用是使<span class="math inline">\(C(t)\)</span>从边界内部，向靠近边界的方向运动。由此可见，式（21）第二项的行为是将曲线<span class="math inline">\(C(t)\)</span>推向<span class="math inline">\(|\nabla I|\)</span>的局部极大值（也就是<span class="math inline">\(g(|\nabla
I|)\)</span>的局部极小值），并稳定平衡在<span class="math inline">\(|\nabla I|\)</span>的“屋脊”上（或者说，<span class="math inline">\(g|\nabla
I|\)</span>的“谷底”中）。但这种作用只有当<span class="math inline">\(C(t)\)</span>已经相当靠近边界，以致函数<span class="math inline">\(g(|\nabla I|)\)</span>有明显的梯度时。才能产生。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1.png" alt="image"></p>
<p>总结以上分析可知，曲线按照GAC模型（21）演化时，将受两种“力”的支配：其一是来自于曲线自身的几何形变——曲率运动，故称为内力。不过这种“力”的强弱受到由图像<span class="math inline">\(I(x,y)\)</span>的梯度所提供的标量场<span class="math inline">\(g(x,y)\)</span>的控制。在图像边缘附近，这种“力”将变得很小以至“停止”。所以也常将边缘函数<span class="math inline">\(g(|\nabla
I|)\)</span>称之为边缘停止函数。第二种“力”来自于<span class="math inline">\(g(x,y)\)</span>的梯度<span class="math inline">\(\nabla g\)</span>，由于<span class="math inline">\(g(x,y)=g(|\nabla I(x,y)|)\)</span>，故<span class="math inline">\(\nabla y\)</span>是由图像<span class="math inline">\(I(x,y)\)</span>产生的，所以第二项的力称为外力。它能使<span class="math inline">\(C\)</span>向着图像中对象的边缘靠近，并稳定在边缘上。</p>
<p>GAC模型存在一个严重局限性，即当图像中对象由较深的凹陷边界时，GAC模型可能使<span class="math inline">\(C(t)\)</span>停止在某一<span class="math inline">\(E(C)\)</span>局部极小值状态，它并不与对象的边界相一致。图4.15是一个简单的实例，图（a）表示初始曲线，图（b）表示演化达到稳态的结果。我们看到，在对象凹陷部分，曲率<span class="math inline">\(\kappa &lt;0\)</span>。若<span class="math inline">\(C(t)\)</span>靠近这一部分，<span class="math inline">\(C(t)\)</span>的这一局部的曲率也必将是负的。式（21）前一项的“力”将是使曲线向外<span class="math inline">\((-N)\)</span>运动；另一方面，这时<span class="math inline">\(C(t)\)</span>的实际位置距离物体的边界还较远，故式（21）的后一项几乎为零。于是<span class="math inline">\(C(t)\)</span>的这部分就将停止演化，形成一段接近于直线的线段。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/2.png" alt="image"></p>
<h2 id="无边缘活动轮廓模型">无边缘活动轮廓模型</h2>
<h3 id="模型的建立">模型的建立</h3>
<p>在图像中，对象与背景的区别也可能表现为平均灰度值的明显不同。图4.23是这类图像的一个例子。由于这类图像既没有明显的边缘，也缺乏明显的纹理特征，以上讨论的GAC模型将难以实现成功的分割。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/3.png" alt="image"></p>
<p>对图4.23所示的这类图形，如果我们能找到闭合曲线<span class="math inline">\(C\)</span>，它将全部图像划分为内部区和外部区两个分部<span class="math inline">\(\Omega_1\)</span>和<span class="math inline">\(\Omega_2\)</span>，使在<span class="math inline">\(\Omega_1\)</span>内的图像部分与在<span class="math inline">\(\Omega_2\)</span>的图像的平均灰度恰好反映出对象与背景之间的灰度平均值的差别，那么这一闭合曲线就可看成是对象的轮廓。基于这一思路，T.Chan和L.Vese提出了如下“能量”泛函：
<span class="math display">\[
E\left(c_1, c_2, C\right)=\mu \oint_C \mathrm{~d} s+\lambda_1
\iint_{\Omega_1}\left(I-c_1\right)^2 \mathrm{~d} x \mathrm{~d}
y+\lambda_2 \iint_{\Omega_2}\left(I-c_2\right)^2 \mathrm{~d} x
\mathrm{~d} y \qquad(22)
\]</span></p>
<p>它有三个宗量：标量<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>，以及曲线<span class="math inline">\(C\)</span>。其中第一项是<span class="math inline">\(C\)</span>的全弧长，第二和第三项分别是内部区和外部区的灰度值与标量<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>的平方误差，也就是实际图像与假定的“分片常数”图像之间的偏离。图4.24给出了同一图像，当<span class="math inline">\(C\)</span>处在四个不同状态时，式（22）中第二项(<span class="math inline">\(F_1\)</span>)和第三项(<span class="math inline">\(F_2\)</span>)的变化情况。只有当<span class="math inline">\(C\)</span>达到“正确”位置（图4.24(d)）时，这两项才能同时达到最小值。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/4.png" alt="image"></p>
<p>式（22）称为无边缘活动轮廓模型。也可按提出者姓名称之为C-V模型。实际上，其基本思想与传统的基于区域的图像分割方法是一致的。故也可称为测地线活动区域（geodesic
active region,GAR）模型。</p>
<p>采用变分水平集方法，先在式（22）引入Heaviside函数，将它修改为关于嵌入函数<span class="math inline">\(u\)</span>的泛函，即 <span class="math display">\[
\begin{aligned}
E\left(c_1, c_2, u\right)= &amp; \mu \iint_{\Omega} \delta(u)|\nabla u|
\mathrm{d} x \mathrm{~d} y \\
&amp; +\lambda_1 \iint_{\Omega}\left(I-c_1\right)^2 H(u) \mathrm{d} x
\mathrm{~d} y+\lambda_2 \iint_{\Omega}\left(I-c_2\right)^2[1-H(u)]
\mathrm{d} x \mathrm{~d} y
\end{aligned} \qquad(23)
\]</span></p>
<p>这样，在函数<span class="math inline">\(u\)</span>固定的条件下，相对<span class="math inline">\(c_1，c_2\)</span>最小化式（23），可得 <span class="math display">\[
c_i=\frac{\iint_{\Omega_i} I \mathrm{~d} x \mathrm{~d}
y}{\iint_{\Omega_i} \mathrm{~d} x \mathrm{~d} y}, i=1,2 \qquad(24)
\]</span></p>
<p>即<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>分别是输入图像<span class="math inline">\(I(x,y)\)</span>在<span class="math inline">\(\Omega_1\)</span>（当前曲线的内部）和<span class="math inline">\(\Omega_2\)</span>（当前曲线的外部）的平均值。</p>
<p>在<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>固定的条件下，相对于<span class="math inline">\(u\)</span>最小化式（23），则可得 <span class="math display">\[
\frac{\partial u}{\partial t}=\delta_{\varepsilon}\left[\mu
\operatorname{div}\left(\frac{\nabla u}{|\nabla
u|}\right)-\lambda_1\left(I-c_1\right)^2+\lambda_2\left(I-c_2\right)^2\right]  \qquad(25)
\]</span></p>
<p>于是根据C-V模型，通过方程式（24），（25）的联立，求稳态解，便得到分割结果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/03/Active-Contours-Without-Edges/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/03/Active-Contours-Without-Edges/" class="post-title-link" itemprop="url">Active_Contours_Without_Edges</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-03 08:52:42" itemprop="dateCreated datePublished" datetime="2025-03-03T08:52:42+08:00">2025-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-10 17:36:27" itemprop="dateModified" datetime="2025-03-10T17:36:27+08:00">2025-03-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文泛读</span></a>
                </span>
            </span>

          
            <span id="/2025/03/03/Active-Contours-Without-Edges/" class="post-meta-item leancloud_visitors" data-flag-title="Active_Contours_Without_Edges" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/03/03/Active-Contours-Without-Edges/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/03/Active-Contours-Without-Edges/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="active-contours-without-edges">Active Contours Without
Edges</h1>
<p>期刊：IEEE Transactions on image processin</p>
<p>时间：2001</p>
<h2 id="摘要">摘要</h2>
<p>在本文中，我们提出了一种新的活动轮廓模型，用于检测给定图像中的对象，基于曲线演化技术，用于分割和水平集的Mumford-Shah函数。我们的模型可以检测其边界不一定由梯度定义的对象。我们最小化了能量，这可以看作是最小分区问题的一个特殊情况。在水平集公式中，问题变成了一个
“平均曲率流”
--类似演化活动等值线，它将在所需的边界上停止。然而，停止项并不依赖于图像的梯度，就像经典的主动轮廓模型那样，而是与图像的特定分割有关。我们将给出一个使用有限差分的数值算法。最后，我们将介绍各种实验结果，特别是一些基于梯度的经典蛇方法不适用的示例。此外，初始曲线可以位于图像中的任何位置，并且会自动检测内部轮廓。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/03/03/Active-Contours-Without-Edges/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/02/The-Perception-Distortion-Tradeoff/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/02/The-Perception-Distortion-Tradeoff/" class="post-title-link" itemprop="url">The_Perception-Distortion_Tradeoff</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-02 10:30:01" itemprop="dateCreated datePublished" datetime="2025-03-02T10:30:01+08:00">2025-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-03-03 12:04:52" itemprop="dateModified" datetime="2025-03-03T12:04:52+08:00">2025-03-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文泛读</span></a>
                </span>
            </span>

          
            <span id="/2025/03/02/The-Perception-Distortion-Tradeoff/" class="post-meta-item leancloud_visitors" data-flag-title="The_Perception-Distortion_Tradeoff" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/03/02/The-Perception-Distortion-Tradeoff/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/03/02/The-Perception-Distortion-Tradeoff/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.7k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="the-perception-distortion-tradeoff感知-失真权衡">The
Perception-Distortion Tradeoff（感知-失真权衡）</h1>
<p>会议： CVPR</p>
<p>时间： 2018</p>
<h2 id="摘要">摘要</h2>
<p>图像复原算法通常是通过某种失真测度(例如：PSNR、SSIM、IFC、VIF)或通过量化感知质量的人类主观评分来评估的。在本文中，我们从数学上证明了失真和感知质量是相互矛盾的。具体来说，<strong>我们研究了从真实图像中正确判别图像复原算法输出的最佳概率</strong>。<strong>我们证明，随着平均扭曲的减小，这个概率必定增加(说明感知质量较差)</strong>。与通常的信念相反，这个结果对任何失真测度都是正确的，而不仅仅是PSNR或SSIM标准的问题。然而，正如我们在实验中所显示的那样，对于某些措施来说，(例如：VGG特征之间的距离)并不那么严重。我们还表明，生成对抗网络(Generative-adversarial-nets，GANs)为接近感知-失真界提供了一种原则性的方法。
这构成了他们在低级视觉任务中观察到的成功的理论支持。基于我们的分析，我们提出了一种新的评估图像复原方法的方法，并使用它对最近的超分辨率算法进行了广泛的比较。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/03/02/The-Perception-Distortion-Tradeoff/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/" class="post-title-link" itemprop="url">Unsupervised Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal Transport</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-24 11:26:51" itemprop="dateCreated datePublished" datetime="2025-02-24T11:26:51+08:00">2025-02-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-27 15:07:58" itemprop="dateModified" datetime="2025-02-27T15:07:58+08:00">2025-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文泛读</span></a>
                </span>
            </span>

          
            <span id="/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/" class="post-meta-item leancloud_visitors" data-flag-title="Unsupervised Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal Transport" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.9k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="unsupervised-noise-adaptive-speech-enhancement-by-discriminator-constrained-optimal-transport判别器约束最优传输的无监督噪声自适应语音增强">Unsupervised
Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal
Transport（判别器约束最优传输的无监督噪声自适应语音增强）</h1>
<p>会议：NeurIPS</p>
<p>时间：2021</p>
<h2 id="摘要">摘要</h2>
<p>本文提出了一种新的判别器约束的最优传输网络(DOTN)，该网络执行无监督的域自适应语音增强(SE)，这是语音处理中必不可少的回归任务。<strong>DOTN旨在利用从源域获得的知识，在目标域中估计带噪语音的干净参考</strong>。训练和测试数据之间的领域转换已被报道是不同领域学习问题的障碍。尽管有丰富的文献研究无监督域适应分类，但所提出的方法，特别是在回归中，仍然是稀缺的，并且往往依赖于关于输入数据的额外信息。<strong>提出的DOTN方法将数学分析中的最优传输(OT)理论与生成对抗框架进行策略性融合，以帮助评估目标域中的连续标签</strong>。在两个SE任务上的实验结果表明，通过扩展经典的OT公式，我们提出的DOTN以一种纯无监督的方式优于先前的对抗域适应框架。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/" class="post-title-link" itemprop="url">两个曲线之间的距离</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-13 20:13:32" itemprop="dateCreated datePublished" datetime="2025-02-13T20:13:32+08:00">2025-02-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-14 18:48:12" itemprop="dateModified" datetime="2025-02-14T18:48:12+08:00">2025-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          
            <span id="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/" class="post-meta-item leancloud_visitors" data-flag-title="两个曲线之间的距离" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.3k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="两个曲线之间的距离">两个曲线之间的距离</h2>
<p>搜Fréchet怎么打的时候看到了Fréchet距离，感觉这两天接触的距离比较多，做下整理（说不定以后换个距离就是个创新点）</p>
<p>本篇主要介绍两个距离：<strong>Fréchet距离</strong>和<strong>Hausdorff距离</strong>。以后看到了比较两个曲线的方法再做补充（之前介绍过的两个分布之间的距离也会写篇博客整理出来）</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/" class="post-title-link" itemprop="url">OPTIMAL TRANSPORTATION FOR ELECTRICAL IMPEDANCE  TOMOGRAPHY</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-13 19:26:47" itemprop="dateCreated datePublished" datetime="2025-02-13T19:26:47+08:00">2025-02-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-23 16:15:45" itemprop="dateModified" datetime="2025-02-23T16:15:45+08:00">2025-02-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文精读</span></a>
                </span>
            </span>

          
            <span id="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/" class="post-meta-item leancloud_visitors" data-flag-title="OPTIMAL TRANSPORTATION FOR ELECTRICAL IMPEDANCE  TOMOGRAPHY" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>24k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="optimal-transportation-for-electrical-impedance-tomography电阻抗断层成像的最优传输">OPTIMAL
TRANSPORTATION FOR ELECTRICAL IMPEDANCE
TOMOGRAPHY（电阻抗断层成像的最优传输）</h1>
<p>期刊：MATHEMATICS OF COMPUTATION</p>
<p>时间：September 2024</p>
<h2 id="摘要">摘要</h2>
<p>这项工作建立了一个用基于测地线的二次Wasserstein距离(<span class="math inline">\(W_2\)</span>)求解逆边界问题的框架。Fréchet梯度的一般形式由最优运输(
OT )理论系统推导得到。此外，基于OT在<span class="math inline">\(\mathbb{S}^1\)</span>上的新公式开发了一种快速算法来求解相应的最优运输问题。该算法的计算复杂度由传统方法的<span class="math inline">\(O(N^3)\)</span>降低到<span class="math inline">\(O(N)\)</span>。结合伴随状态方法，该框架为解决具有挑战性的电阻抗层析成像问题提供了一种新的计算方法。数值例子说明了我们方法的有效性。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/11/NAS-PINN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/11/NAS-PINN/" class="post-title-link" itemprop="url">NAS-PINN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-02-11 09:43:32 / 修改时间：11:45:56" itemprop="dateCreated datePublished" datetime="2025-02-11T09:43:32+08:00">2025-02-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E6%B3%9B%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文泛读</span></a>
                </span>
            </span>

          
            <span id="/2025/02/11/NAS-PINN/" class="post-meta-item leancloud_visitors" data-flag-title="NAS-PINN" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/02/11/NAS-PINN/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/02/11/NAS-PINN/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.9k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="nas-pinn-neural-architecture-search-guided-physics-informed-neural-network-for-solving-pdesnas-pinn神经网络结构搜索引导的物理信息神经网络用于求解偏微分方程">NAS-PINN:
Neural architecture search-guided physics-informed neural network for
solving
PDEs(NAS-PINN：神经网络结构搜索引导的物理信息神经网络，用于求解偏微分方程)</h1>
<h2 id="摘要">摘要</h2>
<p>物理信息神经网络( PINN
)自提出以来一直是求解偏微分方程的主流框架。通过损失函数将物理信息融入到神经网络中，它可以以无监督的方式预测PDEs的解。然而，神经网络结构的设计基本依赖于先验知识和经验，这造成了很大的麻烦和较高的计算开销。因此，<strong>我们提出了一种神经结构搜索引导的方法，即NAS
- PINN，用于自动搜索求解某些PDEs的最佳神经结构</strong>。
通过将搜索空间松弛为连续空间，并利用掩码实现不同形状张量的添加，NAS -
PINN可以通过双层优化进行训练，其中内层循环优化神经网络的权重和偏置，外层循环优化网络结构参数。我们通过包括Poisson，Burgers和Advection方程在内的几个数值实验来验证NAS
-
PINN的能力。总结了求解不同PDE的有效神经网络结构的特点，可用于指导PINN中神经网络的设计。研究发现，更多的隐藏层并不一定意味着更好的性能，有时可能是有害的。
特别是对于Poisson和Advection，在PINNs中更适合采用神经元数目较多的浅层神经网络。研究还表明，对于复杂问题，具有残差连接的神经网络可以提高PINNs的性能。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/11/NAS-PINN/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="宋嘉晨">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/" class="post-title-link" itemprop="url">Lagrange对偶（Lagrange duality）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-10 19:16:52" itemprop="dateCreated datePublished" datetime="2025-02-10T19:16:52+08:00">2025-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-11 18:59:32" itemprop="dateModified" datetime="2025-02-11T18:59:32+08:00">2025-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
            </span>

          
            <span id="/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/" class="post-meta-item leancloud_visitors" data-flag-title="Lagrange对偶（Lagrange duality）" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.7k</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题背景">问题背景</h2>
<p>在一个优化问题中，原始问题通常会带有很多约束条件，这样直接求解原始问题往往是很困难的，于是考虑将原始问题转化为它的对偶问题，通过求解它的对偶问题来得到原始问题的解。<strong>对偶性</strong>（Duality）是凸优化问题的核心内容。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="宋嘉晨"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">宋嘉晨</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">宋嘉晨</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">116k</span>
</div>

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共116k字</span>
  <span class="post-meta-divider">|</span>
  本站总访问量<span id="busuanzi_value_site_pv"></span>次
  <span class="post-meta-divider">|</span>
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
  <span class="post-meta-divider">|</span>
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共116k字</span>
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'x2HNNt4kVm1AzbK4G5OMYocX-gzGzoHsz',
      appKey     : 'UZukMakW4tBBED8F52h1Hgn4',
      placeholder: "输入你的评论\n不输入昵称则为匿名",
      avatar     : '',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/clicklove.js"></script>
