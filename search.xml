<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>NAS-PINN</title>
    <url>/2025/02/11/NAS-PINN/</url>
    <content><![CDATA[<h1 id="nas-pinn-neural-architecture-search-guided-physics-informed-neural-network-for-solving-pdesnas-pinn神经网络结构搜索引导的物理信息神经网络用于求解偏微分方程">NAS-PINN:
Neural architecture search-guided physics-informed neural network for
solving
PDEs(NAS-PINN：神经网络结构搜索引导的物理信息神经网络，用于求解偏微分方程)</h1>
<h2 id="摘要">摘要</h2>
<p>物理信息神经网络( PINN
)自提出以来一直是求解偏微分方程的主流框架。通过损失函数将物理信息融入到神经网络中，它可以以无监督的方式预测PDEs的解。然而，神经网络结构的设计基本依赖于先验知识和经验，这造成了很大的麻烦和较高的计算开销。因此，<strong>我们提出了一种神经结构搜索引导的方法，即NAS
- PINN，用于自动搜索求解某些PDEs的最佳神经结构</strong>。
通过将搜索空间松弛为连续空间，并利用掩码实现不同形状张量的添加，NAS -
PINN可以通过双层优化进行训练，其中内层循环优化神经网络的权重和偏置，外层循环优化网络结构参数。我们通过包括Poisson，Burgers和Advection方程在内的几个数值实验来验证NAS
-
PINN的能力。总结了求解不同PDE的有效神经网络结构的特点，可用于指导PINN中神经网络的设计。研究发现，更多的隐藏层并不一定意味着更好的性能，有时可能是有害的。
特别是对于Poisson和Advection，在PINNs中更适合采用神经元数目较多的浅层神经网络。研究还表明，对于复杂问题，具有残差连接的神经网络可以提高PINNs的性能。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>神经网络结构搜索( Neural Architecture Search，NAS
)是一种在特定搜索空间中搜索最优神经网络结构的算法。传统的NAS算法通过神经网络模块的排列组合来构建架构，并对这些架构进行训练和测试以确定其性能，然后根据性能排序选择最佳的神经网络架构。这样的离散过程面临着计算效率低和计算成本高的问题。因此，减少计算开销、提高搜索效率一直是NAS的主要研究热点之一。
<strong>本文将NAS融入PINN的框架中，提出了一种神经架构搜索引导的物理信息神经网络(
NAS-PINN)</strong>。我们实现了用少量数据自动搜索求解给定PDE的最佳神经网络结构。掩码用于张量的添加，以帮助搜索每层中不同数量的神经元。通过对一系列PDE的数值实验，验证了所提方法的有效性.通过对数值结果的分析，总结了高效神经网络结构的特点，为PINNs的进一步研究提供了指导。</p>
<h2 id="方法">方法</h2>
<h3 id="pinn的框架">PINN的框架</h3>
<p><img src="/2025/02/11/NAS-PINN/1.jpg"></p>
<h3 id="可微nas">可微NAS</h3>
<p>在传统的NAS算法中，神经网络的层数通常是固定的，并为每一层提供特定的操作选择。这样的配置使得搜索空间不连续，无法通过基于梯度的方法进行优化，极大地限制了算法的收敛速度和效率。
Liu等人[ 29 ]提出了DARTS，并引入了可微NAS的概念。设<span class="math inline">\(O\)</span>是一个由候选操作组成的集合，其中任何一个操作都表示关于输入x的某个函数<span class="math inline">\(o(x)\)</span>.通过对候选操作施加松弛，可以使搜索空间连续：
<span class="math display">\[
\bar{o}^{(i,j)}(x)=\sum_{o\in
O}\frac{exp(\alpha^{(i,j)}_o)}{\textstyle\sum_{o&#39;\in
O}exp(\alpha^{(i,j)_{o&#39;}})}o(x) \qquad (6)
\]</span> 其中<span class="math inline">\(\bar
o^{(i,j)}(x)\)</span>为松弛后第<span class="math inline">\(i\)</span>层与第<span class="math inline">\(j\)</span>层之间的混合运算，<span class="math inline">\(\alpha^{(i,j)}_o\)</span>为运算<span class="math inline">\(o\)</span>的权.现在测试和比较所有可能的操作组合的离散过程可以简化为通过基于梯度的优化方法学习一组合适的权重<span class="math inline">\(\alpha^{(i,j)}_o\)</span>。当算法收敛时，通过选择权重最高的候选操作，可以将松弛的搜索空间提取到离散的神经架构中。</p>
<h3 id="掩码">掩码</h3>
<p>虽然式(6)将搜索空间缩小为一个连续的空间，张量运算只允许相同形状的张量相加，使得搜索神经元个数不切实际，如图2(a)所示。受卷积神经网络中零填充的启发，我们可以将神经元填充到最大数量k，如图2
(b)所示。在图2(c)中，通过将填充的神经元乘以一个零张量掩码，我们将额外的神经元去激活，以模拟不同数量的神经元。最后，通过共享权重，可以将可选的隐藏层减少为一个，输出y可以表示为：
<span class="math display">\[
\mathbf{y} = \sigma(\mathbf{w}\cdot\mathbf{x}+\mathbf{b})\cdot
\begin{pmatrix}
[g_1,g_2,g_3]\times     
\begin{bmatrix}
   \mathbf{mask}_1 \\
   \mathbf{mask}_2 \\
   \mathbf{mask}_3
\end{bmatrix}^T
\end{pmatrix}
\]</span> 其中<span class="math inline">\(\sigma(\cdot)\)</span>为激活函数，<span class="math inline">\(\mathbf{w}\)</span>和<span class="math inline">\(\mathbf{b}\)</span>分别为单隐层的权值和偏置，<span class="math inline">\(g_i\)</span>为标量，为每个神经元个数的权值，<span class="math inline">\(\mathbf{mask}_i\)</span>为每个神经元个数的掩码，形状为<span class="math inline">\(1\times k\)</span>。假设神经元个数为<span class="math inline">\(j\)</span>，则<span class="math inline">\(\mathbf{mask}\)</span>的前<span class="math inline">\(j\)</span>个元素为1，其余<span class="math inline">\((k-j)\)</span>个元素为0。 <img src="/2025/02/11/NAS-PINN/2.jpg" alt="fig.2">
为了确定层数，我们引入身份变换作为操作，即跳过该层，且输出<span class="math inline">\(\mathbf{y}\)</span>变为: <span class="math display">\[
\mathbf{y} = \alpha_1 \cdot
\mathbf{x}+\alpha_2\cdot\sigma(\mathbf{w}\cdot\mathbf{x}+\mathbf{b})\cdot
\begin{pmatrix}
[g_1,g_2,g_3]\times     
\begin{bmatrix}
   \mathbf{mask}_1 \\
   \mathbf{mask}_2 \\
   \mathbf{mask}_3
\end{bmatrix}^T
\end{pmatrix} \qquad (8)
\]</span> 其中，<span class="math inline">\(\alpha_1\)</span>为身份转换权重，表示跳过该层，<span class="math inline">\(\alpha_2\)</span>为保留该层的权重。
式(8)给出了每层输入和输出之间的映射关系，通过反复应用，可以建立一个DNN模型，其中最合适的层可以根据权重<span class="math inline">\(\alpha\)</span>来选择，最合适的每层神经元可以根据权重<span class="math inline">\(g\)</span>来决定。这里，我们将<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(g\)</span>统称为<span class="math inline">\(\boldsymbol{\alpha}\)</span>，<span class="math inline">\(\bf{w}\)</span>和<span class="math inline">\(\bf{b}\)</span>统称为<span class="math inline">\(\boldsymbol{\theta}\)</span>。</p>
<h3 id="mas-pinn">MAS-PINN</h3>
<p>现在我们可以得到NAS -
PINN的整体框架，如图3所示，它可以被认为是一个<strong>双层优化问题</strong>。在内循环中，对DNN的权值和偏置<span class="math inline">\(\boldsymbol{\theta}\)</span>进行优化，而在外循环中，优化目标是寻找最佳的<span class="math inline">\(\boldsymbol{\alpha}\)</span>。其过程可以表示为：
<span class="math display">\[
\underset{\boldsymbol{\alpha}}{min}MSE(\boldsymbol{\theta}^*,\boldsymbol{\alpha})
\\ s.t. \boldsymbol{\theta}^*=
\underset{\boldsymbol{\theta}}{argmin}Loss(\boldsymbol{\theta},\boldsymbol{\alpha})
\]</span> 内环的损失函数可以设计为式( 2 ) ~ ( 5
)（PINN损失函数：数据匹配损失，PDE残差损失，边界条件损失）和外环的损失函数可以写成：
<span class="math display">\[
MSE=\frac{1}{n}\sum^n_{i=1}(\hat u-u)^2  \qquad(10)
\]</span>
其中u是已知的解析解或数值解，n是数据点的个数，对于外循环，所需的n可以很小。这样的双层优化问题可以通过交替优化来解决，相应的过程在算法1中展示。
当训练结束时，可以根据<span class="math inline">\(\boldsymbol{\alpha}\)</span>推导出离散的神经网络模型。基本上，我们可以首先通过比较<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>来决定是否跳过某一层。如果保留该层，我们可以根据<span class="math inline">\(g\)</span>来决定神经元的数量。如果跳过某一层，则无需考察其权重<span class="math inline">\(g\)</span>。 在一些<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>相对接近的情况下，我们假设跳过层和保留层同样重要，并给出了一个混合模型。在混合模型中，层是身份变换和神经网络操作的组合。神经元的数量决定于一个离散的神经元，因此这些层可以表示为：
<span class="math display">\[
\mathbf{y} = \alpha_1 \cdot
\mathbf{x}+\alpha_2\cdot\sigma(\mathbf{w}\cdot\mathbf{x}+\mathbf{b})\cdot
(g_{max}\times \boldsymbol{mask}_{max})^T
\]</span> 式中,<span class="math inline">\(g_{max}\)</span>为所有权值<span class="math inline">\(g\)</span>中的最大值，<span class="math inline">\(\boldsymbol{mask}_{max}\)</span>为对应于<span class="math inline">\(g_{max}\)</span>的零张量掩码。 <img src="/2025/02/11/NAS-PINN/3.jpg" alt="fig.3"> <img src="/2025/02/11/NAS-PINN/4.jpg" alt="algorithm 1"></p>
<h2 id="实验">实验</h2>
<h3 id="possion-equation">Possion equation</h3>
<p>泊松方程是一类描述电磁场和热场的基本偏微分方程，在电磁学和机械工程中有着广泛的应用。这里，我们考虑一个带有Dirichlet边界条件的二维Poisson方程：
<img src="/2025/02/11/NAS-PINN/5.jpg" alt="possion equation"> 该方程有解析解：
<img src="/2025/02/11/NAS-PINN/6.jpg" alt="analytical solution">
我们首先考虑正方形计算域中的泊松方程，以验证所提出的NAS -
PINN的有效性。我们构造了一个相对较小的搜索空间，它是一个最多包含5个隐藏层的神经网络，每层包含30、50或70个神经元。对离散搜索空间中的每一种可能的神经架构分别进行训练和测试，作为网格搜索的近似实例。然后，我们使用NAS
-
PINN来搜索一个神经结构，并研究它是否是最好的。所有离散搜索空间中的363个架构由Adam训练，其中500个配置点在域内随机采样，100个边界点均匀分布在边界上。
在架构搜索阶段，1000个配置点和200个边界点采用与之前相同的策略进行采样，以搜索最佳的神经架构，并将Adam应用于架构搜索阶段。然后以与363架构相同的方式从头开始训练得到的神经架构。</p>
<p>为了进行更全面的比较，还对传统的Auto
ML方法SMAC进行了测试。SMAC是一个通用的用于超参数优化的贝叶斯优化包，对于所讨论的问题，需要优化的超参数是隐藏层的数量和神经元的数量。对于SMAC，一个相当小的研究空间包括15种不同的神经结构，其中每个隐藏层的神经元数量只能是相同的。SMAC使用与架构搜索阶段相同的1000个配置点和200个边界点。
最后，均匀采样1000000个点，测试所有收敛的神经架构。不同架构的预测解和误差分布如图4所示，L2误差如表1所示。所有实验均重复5次，L2误差由5次重复的平均值得到。
这些结构以序列的形式描述在表1中。序列的第一个和最后一个元素代表输入和输出通道，而其他元素代表每一层的神经元数目。例如，98号架构的输入大小为n×2，其中n为批次大小，2代表坐标x和y，第一层隐含层有70个神经元。通过NAS
- PINN得到的架构为No.358。</p>
<p>从表1和图4中，我们可以清楚地看到NAS-PINN的神经架构具有最小的L2误差和最小的最大误差值，并且其误差分布相比于其他架构也有所改善。因此，所提出的NASPINN确实可以在给定的搜索空间中找到最佳的神经网络结构。此外，虽然No.98也表现出相对较好的性能(在363种可能的体系结构中,它是第二好的体系结构)，但它比NAS-PINN的架构拥有更多的参数，这表明更多的参数并不一定意味着更好的性能，一个适当设计的神经架构显得尤为重要。
此外，在PINNs中，更深的神经网络总是更好的这一常识似乎并不是在所有情况下都是正确的。至少对于给定的泊松方程，浅层但宽的神经网络(隐含层较少,但每层神经元较多的神经网络)优于深层的神经网络。</p>
<p>与SMAC相比，NAS - PINN可以在更大、更灵活的搜索空间中进行搜索，从而NAS
-
PINN更有可能找到真正最佳的神经架构。值得注意的是，虽然SMAC在只有15个神经结构的较小搜索空间中进行搜索，但它需要SMAC
2。08 h找到357号架构，而NAS -
PINN使用1.57h，从363个不同的架构中找到358号架构。所有的数值实验均在Intel(R)Core
i9-9900 K @ 3.60 Ghz /NVIDIA GeForce Rtx 3090 上进行 <img src="/2025/02/11/NAS-PINN/7.jpg" alt="fig.4"> <img src="/2025/02/11/NAS-PINN/8.jpg" alt="table.1"></p>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>神经网络架构</tag>
        <tag>PINN</tag>
      </tags>
  </entry>
  <entry>
    <title>OPTIMAL TRANSPORTATION FOR ELECTRICAL IMPEDANCE  TOMOGRAPHY</title>
    <url>/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/</url>
    <content><![CDATA[<h1 id="optimal-transportation-for-electrical-impedance-tomography电阻抗断层成像的最优传输">OPTIMAL
TRANSPORTATION FOR ELECTRICAL IMPEDANCE
TOMOGRAPHY（电阻抗断层成像的最优传输）</h1>
<p>期刊：MATHEMATICS OF COMPUTATION</p>
<p>时间：September 2024</p>
<h2 id="摘要">摘要</h2>
<p>这项工作建立了一个用基于测地线的二次Wasserstein距离(<span class="math inline">\(W_2\)</span>)求解逆边界问题的框架。Fréchet梯度的一般形式由最优运输(
OT )理论系统推导得到。此外，基于OT在<span class="math inline">\(\mathbb{S}^1\)</span>上的新公式开发了一种快速算法来求解相应的最优运输问题。该算法的计算复杂度由传统方法的<span class="math inline">\(O(N^3)\)</span>降低到<span class="math inline">\(O(N)\)</span>。结合伴随状态方法，该框架为解决具有挑战性的电阻抗层析成像问题提供了一种新的计算方法。数值例子说明了我们方法的有效性。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>OT给出了一个比较两个概率测度的框架，通过寻求将一个测度重新排列到另一个测度的最小成本。它在机器学习、经济学、光学设计、成像科学、图形学[25、27、50]等不同领域都有广泛的应用。</p>
<p>在过去的几年中，最优传输已经被应用于求解反问题[1、14、26、36、53]。基于OT的一个通用框架是使用Wasserstein距离来衡量数据匹配问题中数据集的差异性。这是一种有吸引力的方法，因为Wasserstein距离，特别是二次Wasserstein距离<span class="math inline">\((W_2)\)</span>，具有捕获幅度和空间信息的能力。与传统的<span class="math inline">\(L^2\)</span>度量相比，<span class="math inline">\(W_2\)</span>具有更好的凸性，对噪声更加鲁棒[21]。</p>
<p>本文旨在<strong>发展一种基于二次Wasserstein距离的方法来求解严重不适定的反问题</strong>（EIT问题）。</p>
<p>对于许多反问题，观测数据是在边界上测量的，通常是欧氏空间中的低维流形。在度量中融入流形的几何信息是很自然的。在这项工作中，我们<strong>不使用传统的欧氏距离作为OT的代价函数，而是考虑流形上的运输问题，并采用相应的测地线距离作为其代价函数</strong>。这不仅提高了计算效率，而且更好地捕获了数据的几何特征。特别地，<strong>采用基于测地线的<span class="math inline">\(W_2\)</span>距离作为误匹配函数来解决数据在圆上测量的二维EIT问题。基于我们提出的<span class="math inline">\(\mathbb
S^1\)</span>上的OT新公式，设计了一种有效的算法来计算二次Wasserstein距离</strong>。本文方法的复杂度降低为<span class="math inline">\(O(N)\)</span>，而单纯形算法和Sinkhorn算法的复杂度分别为<span class="math inline">\(O(N^3)\)</span>和<span class="math inline">\(O(N^2)\)</span>。求解该优化问题的一个关键步骤是<strong>开发一种新的计算<span class="math inline">\(W_2\)</span>的Fréchet梯度的框架</strong>，该框架通过观察Kantorovich势和最优映射之间的显式联系来实现。该框架与现有的方法[14、53]形成了强烈的对比，[14、53]的梯度是通过对完全非线性Monge-Ampère方程的扰动得到的。最后，采用梯度下降算法求解EIT的优化问题。</p>
<h2 id="最优传输理论">最优传输理论</h2>
<p>这一节主要内容：开发了一种新的方法来推导<span class="math inline">\(W\)</span>的Fréchet梯度</p>
<h3 id="原问题和对偶问题">原问题和对偶问题:</h3>
<p>Monge的大批量运输问题就是最小化泛函 <span class="math display">\[
\int_X c(x,T(x)) \,d\mu(x) \qquad (2.1)
\]</span> 问题(2.1)的对偶问题是最大化 <span class="math display">\[
J(\varphi,\psi):= \int_X \varphi(x)\, d\mu(x) + \int_Y \psi(y) \,
d\nu(y) \qquad (2.3)
\]</span> 连续函数集<span class="math inline">\((\varphi,\psi)\in C(X)
\times C(Y)\)</span>满足<span class="math inline">\(\text{Lip}_c\)</span> <span class="math display">\[
\varphi(x) + \psi(y) \le c(x,y) , \forall (x,y) \in X \times Y  \qquad
(2.4)
\]</span>
标准对偶结果[48]表明(2.1)式的下确界等于(2.3)式的上确界。对偶形式是凸约束下的线性优化问题，这对于设计数值算法是可取的。</p>
<p><strong>注2.1</strong>：  事实上，<strong>对偶问题(2.3)只依赖于单一变量<span class="math inline">\(\varphi\)</span></strong>。对于<span class="math inline">\(\varphi \in C(X)\)</span>，其c-变换<span class="math inline">\(\varphi^c\)</span>定义为. <span class="math display">\[
\varphi^c(y):= \underset{x \in X}{\inf}\lbrace c(x,y)-\varphi(x) \rbrace
, \forall y \in Y \qquad (2.5)
\]</span> 对任意<span class="math inline">\((\varphi,\psi) \in
\text{Lip}_c\)</span>，我们有<span class="math inline">\(\varphi^c \ge
\psi\)</span>，进一步有<span class="math inline">\(\varphi^{cc}:=(\varphi^c)^c \ge
\varphi\)</span>。由此可知，<span class="math inline">\(J(\varphi^{cc},\varphi^c) \ge J(\varphi,\varphi^c)
\ge J(\varphi,\psi)\)</span>。<span class="math inline">\(J(\varphi,\psi)\)</span>的上确界是在一个较小的集合中得到的：
<span class="math display">\[
\Phi_c:=\lbrace (\varphi^{cc},\varphi^c),\varphi \in C(X) \rbrace \qquad
(2.6)
\]</span></p>
<p>集合<span class="math inline">\(\Phi_c\)</span>是良定义的，因为有<span class="math inline">\((\varphi^{cc})^c =
\varphi^c\)</span>。<strong>对<span class="math inline">\((\varphi,\psi)
\in \Phi_c\)</span>最大化(2.3)</strong>，我们<strong>称这个最优的<span class="math inline">\(\varphi\)</span>为Kantorovich势</strong>。</p>
<h3 id="wasserstein距离">Wasserstein距离</h3>
<p>当<span class="math inline">\(X=Y:=M\)</span>表示具有度量<span class="math inline">\(d\)</span>的同一度量空间时，最优运输问题自然地定义了概率测度空间上的距离，通常称为Wasserstein距离.
<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>之间的p-Wasserstein距离为： <span class="math display">\[
W_p(\mu,\nu)=(\underset{T \in \Pi(\mu,\nu)}{\inf} \int_M
d(x,T(x))^p\,d\mu(x))^\frac{1}{p}
\]</span>
它将μ和ν之间的距离度量为将一个分布重新排列到另一个分布的最优成本。特别地，我们主要关注二次Wasserstein距离(<span class="math inline">\(W_2\)</span>)，我们只考虑<span class="math inline">\((M,d)\)</span>的最常见情形 1. <span class="math inline">\(M=\mathbb{R}^d  \qquad d(x,y)=|x-y|\)</span> 2.
<span class="math inline">\(M\)</span>是紧致黎曼流形，<span class="math inline">\(d\)</span>是<span class="math inline">\(M\)</span>上的测地距离</p>
<p>为了说明最小化式(2.1)的最优运输映射<span class="math inline">\(T \in
\Pi(\mu,\nu)\)</span>和最大化式(2.3)的最优对偶对<span class="math inline">\((\varphi,\psi)\in
\Phi_c\)</span>之间的关系，我们考虑特殊情况<span class="math inline">\(M
=\mathbb R^d\)</span>和<span class="math inline">\(c(x,y) = \frac{1}{2}|
x-y |^2\)</span>。通过(2.2), <span class="math display">\[
\int_{\mathbb{R}^d} c(x,T(x))\, d\mu(x)=\int_{\mathbb{R}^d} \varphi(x)\,
d\mu(x) + \int_{\mathbb{R}^d} \psi(y)\, d\nu(y) \\ = \int_{\mathbb{R}^d}
(\varphi(x)+\psi(T(x)))\, d\mu(x)
\]</span></p>
<p>结合不等式(2.4),得到</p>
<p><span class="math display">\[
\varphi (x) + \psi (T(x)) = c(x,T(x)),\quad d\mu \, \text{almost
everywhere} \qquad (2.9)
\]</span> (这里是x和T(x)的关系，不等式（2.4）是任意x,y)</p>
<p>另一方面，在重新排列项后，我们从(2.4)中得到</p>
<p><span class="math display">\[
x \cdot y \le (\frac{1}{2}|x|^2-\varphi (x))+(\frac{1}{2}|y|^2-\psi
(y)), \qquad \forall x,y\in \mathbb{R}^d
\]</span></p>
<p>因此，注2.1中的C-变换,通过引入<span class="math inline">\(\varphi^*
(x):=\frac{1}{2}|x|^2-\varphi(x)\)</span> 和 <span class="math inline">\(\psi^*(x):=\frac{1}{2}|y|^2-\psi(x)\)</span>可以转化为Legendre变换
<strong>(?)</strong>。由于$ (,) _c<span class="math inline">\(，\)</span>^<em><span class="math inline">\(与\)</span>^</em>$之间的关系由下面的Legendre变换刻画</p>
<p><span class="math display">\[
\begin{cases}
   \psi^*(y):=\underset{x \in \mathbb {R}^d}{\sup} \lbrace x \cdot y -
\varphi^*(x) \rbrace \\
   \varphi^*(x):=\underset{y \in \mathbb {R}^d}{\sup} \lbrace x \cdot y
- \psi^*(y) \rbrace
\end{cases} \qquad (2.10)
\]</span></p>
<blockquote>
<p>证明： <span class="math display">\[
\begin{split}   
\psi^*(y) &amp;=\frac{1}{2}|y|^2-\psi(y)\\
&amp;=\frac{1}{2}|y|^2-\varphi^c(y) \\
&amp;= \frac{1}{2}|y|^2-\underset{x\in X}{\inf} \lbrace c(x,y) -
\varphi(x) \rbrace\\
&amp;= \underset{x\in X}{\sup}\lbrace
\frac{1}{2}|y|^2-\frac{1}{2}|x-y|^2+\varphi(x) \rbrace \\
&amp;=\underset{x\in X}{\sup}\lbrace x \cdot
y-\frac{1}{2}|x|^2+\varphi(x) \rbrace \\
&amp;=\underset{x\in X}{\sup}\lbrace x \cdot y-\varphi^*(x) \rbrace
\end{split}
\]</span></p>
</blockquote>
<p><span class="math inline">\(\varphi^*\)</span>和$^*
$都是凸函数，因为它们是一族线性函数的上确界。因此 $<sup>* <span class="math inline">\(几乎处处可微。对于使\)</span></sup>* <span class="math inline">\(可微的\)</span>x<sup>d<span class="math inline">\(，我们寻求\)</span>y</sup>d<span class="math inline">\(，使得\)</span>^* ( x ) + ^ * ( y ) = x
y$。由(2.10)，该式成立当且仅当:</p>
<p><span class="math display">\[
\underset{z \in \mathbb{R}^d}{\sup} \lbrace \varphi^*(x)-\varphi^*(z)-y
\cdot (x-z) \rbrace = 0
\]</span></p>
<blockquote>
<p>证明： <span class="math display">\[
\varphi^*(x)+\psi^*(y)=x \cdot y \iff \varphi^*(x)+\psi^*(y)-x \cdot y
=0 \\
\iff \varphi^*(x) + \underset{z \in \mathbb {R}^d}{\sup} \lbrace z \cdot
y - \varphi^*(z) \rbrace - x \cdot y=0\\ \iff \underset{z \in
\mathbb{R}^d}{\sup} \lbrace \varphi^*(x)-\varphi^*(z)-y \cdot (x-z)
\rbrace = 0
\]</span></p>
</blockquote>
<p>那就是说，<span class="math inline">\(\varphi^*(z)\ge \varphi^*(x)+y
\cdot(z-x)\)</span>，<span class="math inline">\(\forall z \in
\mathbb{R}^d\)</span>。<span class="math inline">\(y\)</span>的这一特征与<span class="math inline">\(\varphi^*
(x)\)</span>的次梯度的定义是一致的。由于<span class="math inline">\(\varphi^* (x)\)</span>在<span class="math inline">\(x\)</span>处可微，所以<span class="math inline">\(y =\nabla\varphi^*
(x)\)</span>成立[43]。这里我们用原始的<span class="math inline">\((\varphi,\psi)\in
\Phi_c\)</span>来表示这种等价关系：</p>
<p><span class="math display">\[
\varphi(x)+\psi(y)=c(x,y) \quad \text{iff} \quad y=x-\nabla \varphi(x)
\quad \text{dx almost everywhere} \quad (2.11)
\]</span></p>
<p>式中<span class="math inline">\(dx\)</span>为<span class="math inline">\(\mathbb{R}^d\)</span>的Lebesgue测度元素。结合(2.9)和(2.11)，如果<span class="math inline">\(\mu\)</span>关于Lebesgue测度绝对连续，则方程<span class="math inline">\(T(x) = x-\nabla \varphi ( x
)\)</span>几乎处处成立。因此，<strong>最优映射<span class="math inline">\(T\)</span>在二次成本函数下具有关于Kantorovich势<span class="math inline">\(\varphi\)</span>的显式表达式</strong>。</p>
<blockquote>
<p>这里的意思是如果(x,y)满足<span class="math inline">\(\varphi(x)+\psi(y)=c(x,y)\)</span>，那么(x,y)应该满足<span class="math inline">\(y=T(x)(式2.9)\)</span>。上边的推导又说<span class="math inline">\(y=x-\nabla \varphi(x)\)</span>，那么<span class="math inline">\(T(x)=x-\nabla \varphi\)</span></p>
</blockquote>
<p>上述推导提供了最优映射与Kantorovich势之间的重要联系。更一般地，定理2.1总结了最优运输映射的存在性和特征：</p>
<p><strong>定理2.1</strong>:  令<span class="math inline">\(( M ,
d)\)</span>为式( 2.8 )定义的度量空间，<span class="math inline">\(c( x ,
y) = \frac{1}{2} d^2( x , y)\)</span>为二次成本。假设概率测度<span class="math inline">\(\mu\)</span>关于<span class="math inline">\(M\)</span>的体积测度绝对连续，则Monge问题( 2.1
)存在唯一解<span class="math inline">\(T\)</span>，其特征为<span class="math inline">\(T ( x ) = \exp_x ( -\nabla \varphi( x )
)\)</span>，其中<span class="math inline">\(\varphi：M→\mathbb{R}\)</span>为对偶问题(2.3)的Kantorovich势。此外，如果<span class="math inline">\(\text{supp}(\mu)\)</span>是某个连通开集的闭包，则<span class="math inline">\(\varphi\)</span>在可加常数下是唯一的。</p>
<p>这里<span class="math inline">\(\exp\)</span>表示切丛TM上的指数映射
<strong>(?)</strong>。记号<span class="math inline">\(\exp_p
X_p\)</span>是以<span class="math inline">\(p\in
M\)</span>为起点，沿<span class="math inline">\(X_p∈TM\)</span>的方向，长度为<span class="math inline">\(| X_p
|\)</span>的测地线段的端点。特别地，在欧氏空<span class="math inline">\(\exp_p X_p = x + X_p\)</span>中，等价于结果<span class="math inline">\(T ( x ) = x-\nabla \varphi ( x
)\)</span>。<strong>定理2.1假设<span class="math inline">\(\mu\)</span>不给<span class="math inline">\(M\)</span>的体积测度的可忽略集赋予质量，从而保证了二次代价下最优映射的存在性和唯一性。</strong></p>
<p>事实上，Monge的最优运输<span class="math inline">\(T\)</span>可能并不总是存在的。我们的工作主要集中在Monge问题上，因为我们感兴趣的是具有适当密度函数的测度。这里我们假设测度<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>具有密度函数<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>：<span class="math inline">\(d\mu = f (
x ) dx，d\nu = g ( y ) dy\)</span>，其中<span class="math inline">\(dx\)</span>和<span class="math inline">\(dy\)</span>是<span class="math inline">\(M\)</span>的体积元.在整个过程中，用<span class="math inline">\(W_p( f , g)\)</span>代替<span class="math inline">\(W_p( \mu , \nu)\)</span>来表示<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>之间的Wasserstein距离。</p>
<p>注2.2 ( C -周期单调性)  在<span class="math inline">\(\mathbb{R}^d\)</span>中，<span class="math inline">\(T ( x ) = x-\nabla \varphi =\nabla
\varphi^*\)</span>，其中<span class="math inline">\(\varphi\)</span>是我们前面讨论过的凸函数。这个结果常被称为Brenier定理[11]
。此外，由于<span class="math inline">\(T\)</span>是某个凸函数的梯度，所以<span class="math inline">\(T\)</span>是循环单调的[43]。即对集合<span class="math inline">\(\lbrace 1，· · ·，N
\rbrace\)</span>上的任意置换<span class="math inline">\(\sigma\)</span>，对任意的<span class="math inline">\(\lbrace x_i \rbrace ^N_i = \subset \text{supp}
(\mu)\)</span>，有<span class="math inline">\(\sum^N_{i=1}x_i \cdot
T(x_i) \ge \sum^N_{i=1}x_i \cdot T(x_{\sigma(i)})\)</span>。不等式推出
<span class="math display">\[
\sum^N_{i=1}c(x_i,T(x_i)) \le \sum_{i=1}^N c(x_i,T(x_{\sigma(i)}))
\]</span> 式中<span class="math inline">\(c\)</span>为二次成本。事实上，式(2.12)可以推广到任意连续成本函数<span class="math inline">\(c\)</span>的Polish空间上的最优运输问题。OT的这一性质被称为"
c
-周期单调性"，为刻画最优运输方案提供了可供选择的论据。具体参见[3、24、48]。</p>
<p>注2.3(Monge–Ampèere方程)   如注释2.2所述最优运输映射<span class="math inline">\(T ( x ) =\nabla \varphi^*\)</span>。考虑到( 2.2
)的保测性质，我们利用变量替换技巧得到如下Monge - Ampère方程. <span class="math display">\[
\det(D^2\varphi^*(x))=\frac{f(x)}{g(\nabla \varphi^*(x))}
\]</span> 此外，由Caffarelli正则性定理[12、49]可知，若<span class="math inline">\(f，g\in
C^{0,\alpha}\)</span>在它们的支撑上，上下界被正常数所约束，且<span class="math inline">\(\text{supp} \, g\)</span>是凸的，则<span class="math inline">\(\varphi^ *\in C^{2,α}\)</span>，即<span class="math inline">\(\varphi \in C^{2,\alpha}\)</span>和<span class="math inline">\(T \in C^{1,\alpha}\)</span>。</p>
<p>使用<span class="math inline">\(W_2\)</span>作为失配函数也需要我们获取其梯度信息。由定理2.2可知，Wasserstein距离的Fréchet梯度与对应的Kantorovich势有关</p>
<p><strong>定理2.2</strong> ( Wasserstein距离的Fréchet梯度)   泛函<span class="math inline">\(f\to W_p^p( f , g)\)</span>是凸函数，其在<span class="math inline">\(f\)</span>处的次微分与式( 2.3
)的Kantorovich势集合重合。如果存在唯一的Kantorovich势<span class="math inline">\(\varphi\)</span>直到加性常数(additive
constants)<strong>(?)</strong>，则Fréchet导数<span class="math inline">\(\frac{\delta W^p_p(f,g)}{\delta
f}=\varphi\)</span>
事实上，定理2.2对于具有一般连续消耗函数的最小运输成本是有效的，除了<span class="math inline">\(c( x , y) = d^p( x , y)\)</span>。<strong>当<span class="math inline">\(p = 2\)</span>时，由定理2. 1和定理2.2可得，若<span class="math inline">\(\text{supp}\,
f\)</span>是某个连通开集的闭包，则<span class="math inline">\(\frac{\delta W^2_2(f,g)}{\delta
f}=\varphi\)</span></strong></p>
<h2 id="mathbbs1上的最优运输"><span class="math inline">\(\mathbb{S}^1\)</span>(?)上的最优运输</h2>
<p>令<span class="math inline">\(M =
\mathbb{R}\)</span>，假设概率测度支撑在区间[0,1]。众所周知[48]，对于<span class="math inline">\(f，g \in L^1，W_2\)</span>及其最优运输映射为 <span class="math display">\[
W^2_2(f,g)=\int^1_0|F^{-1}(t)-G^{-1}(t)|^2 \, dt,\quad T(t)=G^{-1}(F(t))
\quad (3.1)
\]</span> 其中，<span class="math inline">\(F\)</span>和<span class="math inline">\(G\)</span>分别为<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>的累积分布函数： <span class="math display">\[
F(t)=\int ^t_0 f(\tau) \, d\tau, \qquad G(t)=\int ^t_0 g(\tau)\, d\tau
\]</span> 式(3.1)中分布函数的伪逆定义为 <span class="math display">\[
F^{-1}(y)=\inf \lbrace t:y&lt;F(t) \rbrace, \quad G^{-1}(y)= \inf
\lbrace t:y&lt;G(t) \rbrace \qquad (3.3)
\]</span> 特别地，如果<span class="math inline">\(f,g\)</span>有正的下界，即<span class="math inline">\(f,g \in D\)</span>，其中 <span class="math display">\[
D:= \lbrace f \in L^1[0,1]: \int^1_0 f(\tau) \, d\tau=1,f \ge \eta \quad
\text{on}\, \mathbb{S}^1 \text{for some} \, \eta&gt;0 \rbrace
\]</span> 那么分布函数<span class="math inline">\(F\)</span>和<span class="math inline">\(G\)</span>是严格递增的，因此它们的伪逆(3.3)成为经典的逆函数.由注2.3中提到的Caffarelli定理，严格正性是建立OT正则性的重要条件。</p>
<p>式(3.1)可能导致算法复杂度为<span class="math inline">\(O ( N
)\)</span>。需要指出的是，该结果仅对实际线路情况成立。在一般情况下，<span class="math inline">\(W_2\)</span>没有明确的表达式。由于直接基于优化问题(2.1)和(2.3)的现有方法涉及到高达<span class="math inline">\(O ( N^ 3
)\)</span>的计算复杂度，因此需要有效的新方法来计算运输成本。然而，作为例外，我们在下文中表明，对于<span class="math inline">\(\mathbb{S}^1 \subset
\mathbb{R}^2\)</span>，最优运输问题可以归结为实线上的问题。</p>
<p>考虑<span class="math inline">\(M=\mathbb{S}^1 \cong
\mathbb{T}=\mathbb{R}/ \mathbb{Z}\)</span>，对于<span class="math inline">\(\mathbb{S}^1\)</span>上的密度函数<span class="math inline">\(f\)</span>，将其定义域从区间<span class="math inline">\([0,1)\)</span>扩展到<span class="math inline">\(\mathbb{R}\)</span>，通过使<span class="math inline">\(f ( t )\)</span>周期化：<span class="math inline">\(f(t+1)=f(t)\)</span>。由于密度函数<span class="math inline">\(f\)</span>在每个单位区间上具有单位质量，因此分布函数和逆分布函数可以通过推广得到:</p>
<p><span class="math display">\[
F(t+1)=F(t)+1, \qquad F^{-1}(t+1)=F^{-1}(t)+1
\]</span></p>
<p>相应地，我们将测地距离<span class="math inline">\(d\)</span>从<span class="math inline">\(\mathbb{S}^1\)</span>投影到<span class="math inline">\(\mathbb{R}\)</span>：</p>
<p><span class="math display">\[
d(x,y):=\underset{k \in \mathbb{Z}}{\min}|x-y+k|, \quad x,y \in
\mathbb{R}
\]</span></p>
<p>定理3.1将<span class="math inline">\(\mathbb{S}^1\)</span>上的OT问题转化为一维优化问题：</p>
<p><strong>定理3.1</strong>   设<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>是<span class="math inline">\(\mathbb{S}^1\)</span>上的两个概率密度函数，其累积分布函数<span class="math inline">\(F\)</span>，<span class="math inline">\(G\)</span>和逆分布函数<span class="math inline">\(F^{ - 1}，G ^{-
1}\)</span>分别由(3.2)，(3.3)和(3.5)定义。令<span class="math inline">\(G^\alpha\)</span>表示函数<span class="math inline">\(G + \alpha\)</span>。则<span class="math inline">\(\mathbb{S}_1\)</span>上的二次Wasserstein距离为:</p>
<p><span class="math display">\[
W^2_2(f,g)=\underset{\alpha \in \mathbb{R}}{\inf}
\int^1_0|F^{-1}-(G^{\alpha})^{-1}|^2 \, dt \qquad (3.7)
\]</span></p>
<p>此外，最优映射由<span class="math inline">\(T(t)=( ( G^{\alpha^* })
^{-1} \circ F ) ( t )\)</span>给出，其中<span class="math inline">\(α^*\)</span>为(3.7)式中的下确界点。</p>
<p>我们注意到，在更一般的设定下，(3.7)式也是通过一种不同的方法得到的[19]，即Aubry
- Mather定理。受文献[42]的启发，我们在这里的思路是基于c
-周期单调性，证明在<span class="math inline">\(\mathbb{S}^1\)</span>上的OT可以通过在某一点处切割圆来化简为(3.1)，这样更简单、更直接。</p>
<p>引理3.1涉及公式(3.7)的有用性质，特别是提供了一种直观的方法来寻找(3.7)中的下确界点<span class="math inline">\(\alpha^*\)</span></p>
<p><strong>引理3.1</strong>   设<span class="math inline">\(f，g \in L^1
( \mathbb{S}^1 )\)</span>为<span class="math inline">\(\mathbb{S}^1\)</span>上的概率密度函数，定义 <span class="math display">\[
I(\alpha,f,g):=\int^1_0|F^{-1}(t)-(G^{\alpha})^{-1}(t)|^2 \, dt
\]</span> 则<span class="math inline">\(I(\alpha,f,g)\)</span>满足以下性质：</p>
<p>  (i) 对于固定的<span class="math inline">\(f，g\)</span>，令<span class="math inline">\(I ( \alpha ):= I( \alpha; f , g)\)</span>     (a)
<span class="math inline">\(I (\alpha)\)</span>关于<span class="math inline">\(\alpha\)</span>是凸的。若<span class="math inline">\(f，g\in D\)</span>，则<span class="math inline">\(I (\alpha)\)</span>是严格凸的     (b) <span class="math inline">\(I(\alpha)\)</span>在区间<span class="math inline">\([ - 1,1]\)</span>上取得全局最小值，当<span class="math inline">\(f，g \in D\)</span>时，<span class="math inline">\(I (\alpha)\)</span>是唯一的。     (c) 若<span class="math inline">\(f，g\in D\)</span>，则<span class="math inline">\(I (\alpha)\)</span>关于<span class="math inline">\(\alpha\)</span>在<span class="math inline">\(\mathbb{R}\)</span>上二次可微且 <span class="math display">\[
I&#39;(\alpha)=2\int^1_0 F^{-1}(G(t)+\alpha) \, dt-1 \qquad (3.9)\\
I&#39;&#39;(\alpha)=\int^1_0 \frac{2}{f(F^{-1}(G(t)+\alpha))} \, dt =
\int^1_0 \frac{2}{g(G^{-1}(F(t)-\alpha))} \, dt \qquad (3.10)
\]</span>   (ii) 对于固定的<span class="math inline">\(\alpha \in
\mathbb{R}，I( \alpha, f , g)\)</span>关于<span class="math inline">\(f,g \in
L^1(\mathbb{S}^1)\)</span>是Lipschitz连续的，Lipschitz常数<span class="math inline">\(C_{\alpha}：= 4 + 2 \lceil | α |
\rceil\)</span>，其中<span class="math inline">\(\lceil \cdot
\rceil\)</span>表示取整函数。</p>
<p><strong>引理3.1表示关于<span class="math inline">\(\alpha\)</span>的优化问题是凸的，等价于求解非线性方程<span class="math inline">\(I&#39;(\alpha) = 0\)</span></strong>
。利用已知的最优映射，我们可以推导出梯度公式<span class="math inline">\(\frac{\delta W^2_2(f,g)}{\delta
f}\)</span>如下：</p>
<p><strong>注3.1</strong> (<span class="math inline">\(W_2\)</span>在<span class="math inline">\(\mathbb{S}^1\)</span>上的Fréchet梯度)   对于<span class="math inline">\(f，g \in D\)</span>，根据定理2.2，梯度<span class="math inline">\(\frac{\delta W^2_2(f,g)}{\delta
f}\)</span>恰好为Kantorovich势。基于定理2.1中最优映射与Kantorovich势之间的联系，<span class="math inline">\(W^2_2\)</span>在<span class="math inline">\(\mathbb{S}^1\)</span>上的Fréchet梯度由下面沿圆周的积分给出：
<span class="math display">\[
\frac{\delta W^2_2(f,g)}{\delta f}=2 \int^t_0(\tau-T(\tau)) \, d\tau + c
\qquad (3.11)
\]</span> 其中<span class="math inline">\(T(\tau)=G^{-1}(F(\tau)-\alpha^*)\)</span>是<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的最优输运映射，且<span class="math inline">\(c\)</span>是任意常数.</p>
<h2 id="数值方法">数值方法</h2>
<p>为了计算的目的，定义在<span class="math inline">\([0,1)\)</span>上的密度函数f<span class="math inline">\(，g \in D\)</span>在节点<span class="math inline">\(\tau_i=i*h, \,
i=0,1,2,...,N-1\)</span>上离散，其中<span class="math inline">\(h =
\frac{1}{N}\)</span>。为方便起见，我们将区间$[0,1) <span class="math inline">\(扩展到\)</span>[-1 ,2]<span class="math inline">\(，将其离散化：\)</span>-1 = t_{-N} &lt; t_{- ( N-1
)} &lt; · · · t_0 &lt; · · · &lt; t_{2N} &lt; t_{2N + 1} = 2，t_i = (
2i-1 ) * ，i = -N &lt; i &lt; 2N + 1$。</p>
<p>在区间<span class="math inline">\([-1，2)\)</span>上定义： <span class="math display">\[
f_h(t)=f_t:=\frac{1}{m}f(\tau_{j(i)}) \, \text{for} \, t\in I_i , \quad
j(i)=i \, \text{mod} \, N , \quad i=-N,...,2N
\]</span></p>
<p>其中<span class="math inline">\(I_i：= [ t_i，t_{i + 1}
)\)</span>，重标度参数<span class="math inline">\(m =\sum ^{N - 1}_{i =
0}f ( \tau_i ) h\)</span>为周期上的质量。显然，<span class="math inline">\(f_h ( t
)\)</span>是一个周期的分片常数函数，且在每个周期内具有单位质量。根据(3.2)，分别给出了累积分布函数和逆累积分布函数
<span class="math display">\[
F_h(t)=f_i(t-t_i)+F_i, \, t\in I_i, \quad
F^{-1}_h(y)=\frac{y-F_i}{f_i}+t_i, \, y \in [F_i,F_{i+1}) \quad (4.1)
\]</span> 式中，<span class="math inline">\(F_{-N} = -1，F_{- ( N-1 )} =
-1 + \frac{h}{2}f_{-N}\)</span>，且对于<span class="math inline">\(- (
N-1 ) &lt; i\le 2N\)</span>，<span class="math inline">\(F_i = F_{i-1} +
h * f_{i - 1}\)</span>。密度函数<span class="math inline">\(g\)</span>可以按照同样的方式进行离散化，得到<span class="math inline">\(g_h\)</span>和<span class="math inline">\(G_h\)</span>。</p>
<p>我们现在准备计算( 3.9 )的离散版本，以解决最优运输问题。难点在于( 3.9
)涉及到<span class="math inline">\(F\)</span>与<span class="math inline">\(G\)</span>的复合形式的逆，这就需要对<span class="math inline">\(F^{ - 1}\)</span>和<span class="math inline">\(G\)</span>的节点进行集体排序。为了完整起见，我们提供下面的细节。</p>
<p>给定<span class="math inline">\(α \in [ -
1,1]\)</span>，存在整数<span class="math inline">\(i_\alpha\)</span>使得<span class="math inline">\(α∈[ F_{i_\alpha}，F_{i_\alpha + 1}
)\)</span>，因此<span class="math inline">\(α + 1 \in [ F_{i_\alpha +
N}，F_{i_\alpha + N + 1} )\)</span>。对于两个递增序列<span class="math inline">\(\lbrace H^0_i:= G_i \rbrace ^N_{i =
1}\)</span>和<span class="math inline">\(\lbrace H^1_i:= F_{i_\alpha +
i}-\alpha \rbrace^N_{i =
1}\)</span>，我们将它们的值排序为一个递增序列，记为<span class="math inline">\(\lbrace H_n \rbrace
^{2N}_{n=1}\)</span>。排序过程自动定义一个双射<span class="math inline">\(\sigma：( i , j)\to n\)</span>，使得<span class="math inline">\(H_i^j\)</span>在新的序列中被重新排序为<span class="math inline">\(H_n\)</span> 。现在定义索引序列<span class="math inline">\(\lbrace l^0_n \rbrace ^{2N}_{n=1}\)</span>和<span class="math inline">\(\lbrace l^1_n \rbrace^{2N}_{n=1}\)</span>为：
<span class="math display">\[
\begin{cases}
   l^j_n=i \quad \text{where}(i,j)=\sigma^{-1}(n) \\
   l^{1-j}_n=n-l^j_n
\end{cases}
\]</span></p>
<p>因此，节点序列<span class="math inline">\(\lbrace T_n:= G^{-1}_h (
H_n ) \rbrace ^{2N}_{n=1}\)</span>容易计算： <span class="math display">\[
T_n=\begin{cases}
   \frac{H_n-G_{l^0_n}}{g_{l^0_n}}+t_{l^0_n}  \, &amp;,\text{if} \, j=1
\\
   t_i \, &amp;, \text{if} \, j=0
\end{cases}
\text{where}(i,j)=\sigma^{-1}(n)
\]</span></p>
<p>在序列中加入<span class="math inline">\(T_0 = 0\)</span>和<span class="math inline">\(T_{2N + 1} = 1\)</span>，我们得到<span class="math inline">\(\lbrace T_n \rbrace ^{2N + 1}_{n =
0}\)</span>。重置<span class="math inline">\(\lbrace l^1_n :=l^1_n +
i_\alpha \rbrace^{2N}_{n=1}\)</span>。则对于<span class="math inline">\(t \in [ T_n,T_n + 1 )\)</span>， <span class="math display">\[
F^{-1}_h(G_h(t)+\alpha)=K_nt + B_n
\]</span></p>
<p>它是<span class="math inline">\([0,1)\)</span>上的分段线性函数。参数<span class="math inline">\(K_n\)</span>和<span class="math inline">\(B_n\)</span>的计算公式为 <span class="math display">\[
K_n=\frac{g_{l^0_n}}{f_{l^1_n}}, \quad
B_n=\frac{\alpha+G_{l^0_n-g_{l^0_n}
t_{l^0_n}}-F_{l^1_n}}{f_{l^1_n}}+t_{l^1_n}
\]</span> <strong>最后，将积分式( 3.9 )离散为</strong> <span class="math display">\[
I&#39;_h:=2\int^1_0
F^{-1}_h(G_h(t)+\alpha)\,dt-1=2\sum^{2N}_{n=0}(\frac{1}{2}K_n(T^2_{n+1}-T^2_n)+B_n(T_{n+1}-T_n))-1
\quad (4.2)
\]</span> 二阶导数<span class="math inline">\(I&#39;&#39;_h\)</span>可以按照同样的方式计算。</p>
]]></content>
      <categories>
        <category>论文精读</category>
      </categories>
      <tags>
        <tag>最优化</tag>
        <tag>EIT</tag>
      </tags>
  </entry>
  <entry>
    <title>Lagrange对偶（Lagrange duality）</title>
    <url>/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/</url>
    <content><![CDATA[<h2 id="问题背景">问题背景</h2>
<p>在一个优化问题中，原始问题通常会带有很多约束条件，这样直接求解原始问题往往是很困难的，于是考虑将原始问题转化为它的对偶问题，通过求解它的对偶问题来得到原始问题的解。<strong>对偶性</strong>（Duality）是凸优化问题的核心内容。
<span id="more"></span></p>
<h2 id="原始问题及其转化">原始问题及其转化</h2>
<p><strong>原始问题</strong></p>
<p>将一个原始最优化问题写成如下形式 <span class="math display">\[
\underset{x}{\min} \quad f_0(x) \\ s.t. \quad f_i(x) \le 0,i=1,2,...,m
\\ \qquad h_j(x)=0,j=1,2,...,p
\]</span> 在求解原问题的对偶问题时，并不要求原始问题一定是凸问题，<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>可以是一般函数而不一定非得是凸函数。</p>
<p><strong>拉格朗日函数</strong></p>
<p>将原始问题的拉格朗日函数定义为 <span class="math display">\[
L(x,\lambda,\nu)=f_0(x)+\sum^m_{i=1}\lambda_i f_i(x)+\sum^p_{j=1}\nu_j
h_j(x)
\]</span> 其中，<span class="math inline">\(x\in \mathbb
R^n,\lambda\in\mathbb R^m,\nu\in\mathbb R^p\)</span>
可以看到，拉格朗日函数<span class="math inline">\(L\)</span>相当于原始问题引入了两个新变量<span class="math inline">\(\lambda,\nu\)</span>，称为拉格朗日乘子</p>
<p><strong>拉格朗日对偶函数</strong></p>
<p>拉格朗日对偶函数通过对拉格朗日函数<span class="math inline">\(x\)</span>取下确界得到，即 <span class="math display">\[
g(\lambda,\nu)=\underset{x}{\inf}L(x,\lambda,\nu)
\]</span> 对偶函数有如下两条重要性质
1.对偶函数一定是凹函数，其凹性与原目标函数和约束函数凹凸与否无关
2.对<span class="math inline">\(\forall\lambda\ge0,\forall\nu\)</span>，如果原问题最优解对应的目标函数值为<span class="math inline">\(P^*\)</span>，则<span class="math inline">\(g(\lambda,\nu)\le p^*\)</span></p>
<h2 id="拉格朗日对偶问题">拉格朗日对偶问题</h2>
<p>根据对偶函数的重要性质2，对<span class="math inline">\(\forall\lambda\ge0,\forall\nu\)</span>，对偶函数<span class="math inline">\(g(\lambda,\nu)\)</span>是原问题最优值<span class="math inline">\(P^*\)</span>的一个下界，最好的下界就是最大化对偶函数，因此构造原问题的对偶问题：
<span class="math display">\[
\underset{\lambda,\nu}{\max}\quad g(\lambda,\nu) \\ s.t.\quad \lambda
\ge 0
\]</span>
由于对偶函数是凹函数，故拉格朗日对偶问题一定是凸优化问题，其对应的最优解为<span class="math inline">\(\lambda^*,\nu^*\)</span>，若对应的最优值为<span class="math inline">\(d^*\)</span>，则总有<span class="math inline">\(d^* \le p^*\)</span></p>
<p>当<span class="math inline">\(d^* \le p^*\)</span>时，称为弱对偶
当<span class="math inline">\(d^* = p^*\)</span>时，称为强对偶 将<span class="math inline">\(p^*-d^*\)</span>称为对偶间隙</p>
<blockquote>
<p>在解存在的情况下，弱对偶总是成立的。
满足强对偶时，可以通过求解对偶问题来得到原始问题的解</p>
</blockquote>
<h2 id="slater条件">Slater条件</h2>
<p>Slater条件用于判断什么情况下强对偶是成立的。
在<strong>原问题是凸问题</strong>的情况下，若<span class="math inline">\(\exists x \in
relint(D)\)</span>，使得约束条件满足： <span class="math display">\[
f_i(x)&lt;0,h_j(x)=0\quad i=1,2,...,p
\]</span> 则强对偶成立 &gt;<span class="math inline">\(relint(D)表示原始凸问题定义域的相对内部，即在定义域上除了边界点以外的所有点\)</span></p>
<h2 id="kkt条件">KKT条件</h2>
<p>在强对偶且<span class="math inline">\(L\)</span>对<span class="math inline">\(x\)</span>可微的前提下，设<span class="math inline">\(x^*,\lambda^*,\nu^*\)</span>分别是原问题和对偶问题的最优解，则以下四组条件称为KKT条件
<span class="math display">\[
\begin{cases}
   \frac{\partial L(x^*,\lambda^*,\nu^*)}{\partial x^*} |_{x=x^*}=0
&amp;\text{(稳定性条件)} \\ \lambda^*_i f_i(x^*)=0
&amp;\text{(互松弛条件)}\\ f_i(x^*)\le 0,h_j(x^*)=0
&amp;\text{(原问题可行性)} \\ \lambda_i^* \ge 0
&amp;\text{(对偶问题可行性)}
    &amp;\text{if } d
\end{cases}
\]</span></p>
<p>对<strong>一般的原问题</strong>，KKT
条件是$x<sup><em>,<sup><em>,<sup>* <span class="math inline">\(为最优解的必要条件，即只要\)</span>x</sup></em>,</sup></em>,</sup>*$为最优解，则一定满足
KKT 条件。</p>
<p>对<strong>原问题为凸问题</strong>，KKT条件是<span class="math inline">\(x^*,\lambda^*,\nu^*\)</span>
为最优解的充要条件</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title>最优传输(Optimal Transportation)</title>
    <url>/2025/02/09/OT1/</url>
    <content><![CDATA[<h2 id="什么是最优传输">什么是最优传输？</h2>
<p>最优传输最开始由Monge于1781年提出。一个典型的Monge问题是考虑将一堆具有一定形状的沙子搬运到指定的另外一个形状所需要的具有最小代价的搬运方法。
如下图所示，我们想将左边红色区域的沙堆搬运到右边，形成右边绿色的沙堆的形状。我们想要找到消耗最少的搬运方式。
<img src="/2025/02/09/OT1/1.jpg" alt="image">
一句话来概括，就是<strong>如何用最少的代价将一个质量分布转为另一个质量分布。</strong>
<span id="more"></span> ## 质量分布 质量分布其实就是两个测度空间<span class="math inline">\((X,\mu),(Y,\nu)\)</span>。一般情况下，质量不会凭空产生，所以我们会要求这两个分布的“总质量”是一样的，即<span class="math inline">\(\int_x d\mu=\int_Yd\nu\)</span>。
问题有了考虑的对象，我们还可以定义成本函数<span class="math inline">\(c(x,y):X \times Y \to \mathbb
R^+\)</span>，一般是有界的，来衡量将质量从点<span class="math inline">\(x\)</span>运到点<span class="math inline">\(y\)</span>的成本。那么如何去进行移动？主要有两个角度去考虑，分别是Monge问题和Kantorovich问题。</p>
<h2 id="monge问题">Monge问题</h2>
<p>Monge问题就是寻找一个保测度的映射<span class="math inline">\(T:X \to
Y\)</span> <span class="math display">\[
\underset{T} {\min} \int_X c(x,T(x))d \mu(x),T_{ \# } \mu= \nu
\]</span> <span class="math inline">\(T_{
\#  }\)</span>是前推算子（<span class="math inline">\(T_{ \#
}\)</span>的作用对象是<span class="math inline">\(\mu\)</span>，表示把测度<span class="math inline">\(\mu\)</span>推到<span class="math inline">\(\nu\)</span>），这个“推”的过程就是一个保测度的过程，即
<span class="math display">\[
T_{ \#  }\mu = \nu \iff \forall B \subset Y,\nu (B)=\mu (T^{-1}(B))
\]</span> <img src="/2025/02/09/OT1/2.jpg" alt="Monge Map">
但是映射的定义就限制了我们不能实现“一对多”的操作，这就导致了一个很严重的问题，Monge问题不一定有解。比如一个狄拉克分布（在包含某个点的集合测度是1，其余是0）就不可能保测度地映射到高斯分布。
我们可以让质量“可分”，即以概率的形式去进行“移动”。这就是Kantorovich问题。</p>
<h2 id="kantorovich问题">Kantorovich问题</h2>
<p>在Kantorovich问题中，Kantorovich问题中，我们对Monge问题进行松弛，不再寻找一个映射，而是寻找一个联合分布（耦合coupling），其中它的边界分布分别是<span class="math inline">\(\mu,\nu\)</span>。从而最小化总成本 <span class="math display">\[
\underset{\pi}{\min}\int_{X \times Y}c(x,y)d\pi(x,y),P_{x
\#  }\pi=\mu,P_{y \# }\pi=\nu
\]</span> <img src="/2025/02/09/OT1/3.jpg" alt="Kantorovich Relaxation"> <span class="math inline">\(\pi(x,y)\)</span>就是从<span class="math inline">\(x\)</span>移动到<span class="math inline">\(y\)</span>的概率，上式是总成本的期望。
这样的松弛之后，Kantorovich本质上变成了一个无限维的线性规划问题（如果分布是离散的，比如一堆点，我们要做的就是在两个点云之间做matching，那么<span class="math inline">\(\pi\)</span>就变成了一个矩阵，就变成了有限维的线性规划问题）
线性规划理论告诉我们，如果耦合集合非空且紧，目标函数是下半连续的，那么线性规划一定可以取到最小值。也就保证了Kantorovich一定有解。</p>
<h2 id="分布之间的度量-wasserstein距离">分布之间的度量-Wasserstein距离</h2>
<p>比较两种分布的一些方法：</p>
<p><strong>交叉熵</strong>：对应分布为<span class="math inline">\(p(x)\)</span>的随机变量，熵<span class="math inline">\(H(p)\)</span>表示其最优编码长度。交叉熵是按照概率分布<span class="math inline">\(q\)</span>的最优编码对真实分布为<span class="math inline">\(p\)</span>的信息进行编码的长度 交叉熵定义为 <span class="math display">\[
H(p,q)=E_q[-logq(x)]=-\displaystyle\sum_{x}p(x)logq(x)
\]</span> 在给定<span class="math inline">\(p\)</span>的情况下，如果<span class="math inline">\(q\)</span>和<span class="math inline">\(p\)</span>越接近，交叉熵越小；如果<span class="math inline">\(q\)</span>和<span class="math inline">\(p\)</span>越远，交叉熵越大</p>
<p><strong>KL散度</strong>:是用概率分布<span class="math inline">\(q\)</span>来近似<span class="math inline">\(p\)</span>时所造成的信息损失量。KL散度是按照概率分布<span class="math inline">\(q\)</span>的最优编码对真实分布为<span class="math inline">\(p\)</span>的信息进行编码，其平均编码长度<span class="math inline">\(H(p,q)\)</span>和<span class="math inline">\(p\)</span>的最优平均编码长度<span class="math inline">\(H(p)\)</span>之间的差异。对于离散概率分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>，从<span class="math inline">\(q\)</span>到<span class="math inline">\(p\)</span>的KL散度定义为: <span class="math display">\[
D_{KL}(p\parallel q)=H(p,q)-H(p)=\sum_x p(x)log{\frac {p(x)} {q(x)} }
\]</span>
KL散度可以是衡量两个概率分布之间的距离。KL散度总是非负的，只有当<span class="math inline">\(p=q\)</span>时，<span class="math inline">\(D_{KL}(p\parallel
q)=0\)</span>。两个分布越接近，KL散度越小；两个分布越远，KL散度越大。但KL散度并不是一个真正的度量或距离，一是KL散度不满足距离的对称性，二是KL散度不满足距离的三角不等式性质。</p>
<p><strong>JS散度</strong>：
JS散度是一种对称的衡量两个分布相似度的度量方式，定义为 <span class="math display">\[
D_{JS}(p\parallel q)={\frac 1 2}D_{KL}(p \parallel m)+{\frac 1
2}D_{KL}(q \parallel m)
\]</span> 其中，<span class="math inline">\(m={\frac 1 2}(p+q)\)</span>
JS 散度是KL散度一种改进。但两种散度都存在一个问题，即如果两个分布p, q
没有重叠或者重叠非常少时，KL散度和JS 散度都很难衡量两个分布的距离</p>
<p><strong>Wasserstein距离</strong>： Wasserstein 距离（Wasserstein
Distance）也用于衡量两个分布之间的距离。对于两个分布<span class="math inline">\(q_1,q_2,p-Wasserstein\)</span>距离定义为 <span class="math display">\[
W_p(q_1,q_2)=(\underset{\pi(x,y) \in U(x,y)} {\inf}E_{(x,y)\sim \pi
(x,y)}[d(x,y)^p]) ^{1/p}
\]</span> 其中，<span class="math inline">\(U(x,y)\)</span>是边际分布为<span class="math inline">\(q_1\)</span>和<span class="math inline">\(q_2\)</span>的所有可能的联合分布集合，<span class="math inline">\(d(x,y)\)</span>为<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>的距离 Wasserstein距离相比KL散度和JS
散度的优势在于：即使两个分布没有重叠或者重叠非常少，Wasserstein
距离仍然能反映两个分布的远近。</p>
<p>我们可以发现，如果令<span class="math inline">\(d(x,y)^p=c(x,y)\)</span>，Wasserstein距离实际上就是从一个分布转换为另一个分布所要付出的代价。</p>
<p>Wasserstein
GAN就是将W-1距离作为损失函数，解决了GAN的许多问题，比如训练不稳定，判别器不能训练的“太好”等。究其原因主要是因为W-1度量比KL度量更“弱”：也就是说在K-L散度下收敛的序列在W-1距离下也一定收敛。这样的性质就保证了W-1可以捕捉到序列更多的几何信息（比如不重叠的分布的KL散度永远是0，但W-1距离不然。），训练会更鲁棒。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title>两个曲线之间的距离</title>
    <url>/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<h2 id="两个曲线之间的距离">两个曲线之间的距离</h2>
<p>搜Fréchet怎么打的时候看到了Fréchet距离，感觉这两天接触的距离比较多，做下整理（说不定以后换个距离就是个创新点）</p>
<p>本篇主要介绍两个距离：<strong>Fréchet距离</strong>和<strong>Hausdorff距离</strong>。以后看到了比较两个曲线的方法再做补充（之前介绍过的两个分布之间的距离也会写篇博客整理出来）
<span id="more"></span></p>
<h2 id="fréchet距离">Fréchet距离</h2>
<p>Fréchet distance(弗雷歇距离)是法国数学家Maurice René
Fréchet在1906年提出的一种路径空间相似形描述，这种描述同时还考虑进路径空间距离的因素，对于空间路径的相似性比较适用。</p>
<p><strong>直观的理解，Fréchet distance就是最短的狗绳长度</strong>： ·
主人走路径A，狗走路径B，他们有不同的配速方案。 ·
主人和狗各自走完这两条路径过程中所需要的最短狗绳长度。（在某一种配速下需要的狗绳长度，但其他配速下需要的狗绳长度更长）
<img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/1.jpg" alt="image"></p>
<p><strong>严格的数学定义：</strong> 设<span class="math inline">\((\mathbb S ,d)\)</span>是一个度量空间，<span class="math inline">\(d\)</span>是<span class="math inline">\(\mathbb{S}\)</span>上的度量。 1. <span class="math inline">\(A\)</span>是<span class="math inline">\(\mathbb{S}\)</span>上的一个将单位区间映射到<span class="math inline">\(\mathbb{S}\)</span>的连续映射，例如：<span class="math inline">\(A:[0,1] \to \mathbb S\)</span>. 2.
从单位区间到其自身的重参数化映射<span class="math inline">\(\alpha
:[0,1]\to [0,1]\)</span>满足如下三个条件：1）<span class="math inline">\(\alpha\)</span>是连续的。 2）<span class="math inline">\(\alpha\)</span>是非降的，即对于任意的<span class="math inline">\(x,y \in [0,1],x\le y\)</span>，都有<span class="math inline">\(\alpha(x) \le \alpha(y)\)</span>。 3）<span class="math inline">\(\alpha\)</span>是满射。此时有<span class="math inline">\(\alpha(0)=0,\alpha(1)=1\)</span> 3. 设<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是<span class="math inline">\(\mathbb{S}\)</span>上的两条曲线，即<span class="math inline">\(A:[0,1]\to \mathbb{S},B:[0,1]\to
\mathbb{S}\)</span>。又<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是单位区间的两个重参数化映射，则曲线<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>之间的Fréchet距离定义为： <span class="math display">\[
F(A,B)=\underset{\alpha ,\beta}{\inf}\underset{t \in [0,1]}{\max}\lbrace
d(A(\alpha(t)),B(\beta(t)))
\]</span></p>
<p>解释：</p>
<ul>
<li>两条曲线<span class="math inline">\(A(\alpha(t))\)</span>和<span class="math inline">\(B(\beta(t))\)</span>之间距离最大值的下确界
<ul>
<li>t理解为时间</li>
<li><span class="math inline">\(\alpha(t)\)</span>和<span class="math inline">\(\beta(t)\)</span>理解为人和狗随时间变化的速度</li>
<li><span class="math inline">\(A(\alpha(t))\)</span>和<span class="math inline">\(B(\beta(t))\)</span>代表t时刻人和狗的位置</li>
<li>最大值的下确界意思为,每一种人狗速度方案下，都有对应的距离最大值；那么对于所有的速度方案，这些距离最大值中最小的是哪个？</li>
</ul></li>
</ul>
<p><strong>离散化</strong></p>
<p>设定<span class="math inline">\(t\)</span>是时间点，该时刻，曲线<span class="math inline">\(A\)</span> 上的采样点为<span class="math inline">\(A(\alpha(t))\)</span>, 曲线<span class="math inline">\(B\)</span>上采样点为<span class="math inline">\(B(\alpha(t))\)</span>.
如果使用欧氏距离，则容易定义<span class="math inline">\(d(A(\alpha(t)),B(\beta(t)))\)</span>.
在每次采样中<span class="math inline">\(t\)</span>离散的遍历区间<span class="math inline">\([0,1]\)</span>, 得到该种采样下的最大距离<span class="math inline">\(\underset{t\in [0,1]}{\max} \lbrace
d(A(\alpha(t)),B(\beta(t))) \rbrace\)</span>.
Fréchet距离就是使该最大距离最小化的采样方式下的值。
易于理解的，在离散方式下，我们不可能得到真实的Fréchet距离，而可以无限的趋近。但是越精确的值需要越大的计算量。</p>
<p><img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/2.jpg"></p>
<h2 id="hausdorff距离">Hausdorff距离</h2>
<p>衡量两个集合之间的距离</p>
<p><strong>定义</strong>: 给定欧氏空间中的两点集<span class="math inline">\(A=\lbrace a_1,a_2,... \rbrace , B=\lbrace
b_1,b_2,...
\rbrace\)</span>，豪斯多夫（Hausdorff）距离就是用来衡量这两个点集间的距离。定义公式如下:
<span class="math display">\[
H(A,B)=\max[h(A,B),h(B,A)]
\]</span> 其中， <span class="math display">\[
h(A,B)=\underset{a \in A}{\max}\underset{b\in B}{\min} \lVert a-b \rVert
\\
h(B,A)=\underset{b \in B}{\max}\underset{a\in A}{\min} \lVert b-a \rVert
\]</span> <span class="math inline">\(H(A,B)\)</span>称为双向Hausdorff
距离，<span class="math inline">\(h(A,B)\)</span>称为从点集A到点集B的单向 Hausdorff
距离。相应地<span class="math inline">\(h(B,A)\)</span>称为从点集B到点集A的单向Hausdorff距离</p>
<p><strong>一些图例</strong></p>
<p><img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/3.jpg"> <img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/4.jpg"></p>
<p><strong>性质</strong></p>
<ul>
<li><p>双向Hausdorff距离 <span class="math inline">\(H(A,B)\)</span>是单向 Hausdorff 距离<span class="math inline">\(h(A,B)\)</span>和<span class="math inline">\(h(B,A)\)</span>两者中较大者，显然它度量了两个点集间的最大不匹配程度。</p></li>
<li><p>当<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>都是闭集的时候,Hausdorff距离满足度量的三个定理：</p></li>
</ul>
<ol type="1">
<li><span class="math inline">\(H(A,B)\ge 0\)</span>，当且仅当<span class="math inline">\(A=B\)</span>时，<span class="math inline">\(H(A,B)= 0\)</span></li>
<li><span class="math inline">\(H(A,B)=H(B,A)\)</span></li>
<li><span class="math inline">\(H(A,B)+H(B,C)\ge H(A,C)\)</span></li>
</ol>
<ul>
<li><p>若凸集<span class="math inline">\(A,B\)</span>满足<span class="math inline">\(A \nsubseteq B,B \nsubseteq A\)</span>并记<span class="math inline">\(\partial A,\partial B\)</span>分别为<span class="math inline">\(A,B\)</span>边界的集合，则<span class="math inline">\(A,B\)</span>的Hausdorff距离等于<span class="math inline">\(\partial A,\partial
B\)</span>的Hausdorff距离</p></li>
<li><p>Hausdorff距离易受到突发噪声的影响。</p></li>
</ul>
<p><img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/5.jpg"></p>
<p>当图像受到噪声污染或存在遮挡等情况时，原始的Haudorff距离容易造成误匹配。所以，在1933年，Huttenlocher提出了部分
Hausdorff距离的概念。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>各种距离</tag>
      </tags>
  </entry>
  <entry>
    <title>最优传输（二）</title>
    <url>/2025/02/10/OT2/</url>
    <content><![CDATA[<h1 id="kantorovich问题的对偶问题">Kantorovich问题的对偶问题</h1>
<p>这篇文章我们讲讲对偶理论，并且探究对偶问题和Kantorovich问题之间的关系
<span id="more"></span></p>
<h2 id="离散版本的kantorovich问题对偶">离散版本的Kantorovich问题对偶</h2>
<p>有两个离散分布<span class="math inline">\(a=\sum_i
a_i\delta_{x_i},b=\sum_j b_i\delta_{y_i}\)</span>，分别包含离散点<span class="math inline">\(\lbrace {x_i} \rbrace_{i=1}^n, \lbrace {y_j}
\rbrace_{j=1}^m\)</span></p>
<p>有一个成本矩阵<span class="math inline">\(C \in \mathbb R^{n\times
m}\)</span>,<span class="math inline">\(C_{ij}\)</span>就是从<span class="math inline">\(x_i\)</span>运到<span class="math inline">\(y_j\)</span>的成本</p>
<p>我们寻找一个联合分布的矩阵<span class="math inline">\(P\in \mathbb
R^{n \times m}\)</span>使得总成本最小化 <span class="math display">\[
\underset{p\in U(a,b)}{\min}\langle C,P \rangle = \underset{p\in
U(a,b)}{\min} \sum_{i,j}C_{ij}P_{ij}
\]</span> 其中，<span class="math inline">\(U(a,b)=\{\pi \in \mathbb
R^{n\times m}_+|\pi1_m=a\in\mathbb R^n,\pi^T1_m=b\in\mathbb
R^m\}\)</span></p>
<p>拉格朗日对偶形式： <span class="math display">\[
\underset{P\ge 0}{\min}\underset{(f,g)\in \mathbb R^n \times \mathbb
R^m}{\max}\langle C,P \rangle + \langle a-P1_m,f \rangle + \langle
b-P^T1_n,g \rangle
\]</span></p>
<p>交换min，max，有</p>
<p><span class="math display">\[
\underset{(f,g)\in \mathbb R^n\times \mathbb R^m}{\max}\langle a,f
\rangle + \langle b,g \rangle + \underset{P\ge 0}{\min}\langle
C-f1^T_m-1_ng^T,P \rangle
\]</span></p>
<p>可以将后面min的变为约束</p>
<p><span class="math display">\[
C-f1^T_m-1_ng^T=C-f \oplus g \ge 0
\]</span></p>
<p>综上，我们得到了对偶问题：</p>
<p><span class="math display">\[
\underset{(f,g)\in R(a,b)}{max}\langle f,a \rangle + \langle g,b \rangle
\]</span></p>
<p>其中，<span class="math inline">\(R(a,b)=\{(f,g)\in \mathbb R^n\times
R^m:f_i+g_i \le C_{ij}\}\)</span></p>
<h2 id="连续版本的kantorovich问题对偶">连续版本的Kantorovich问题对偶</h2>
<p>现在两个分布变为连续的<span class="math inline">\(\alpha(x),x\in
X,\beta(y),y\in Y\)</span>，成本函数连续化<span class="math inline">\(c(x_i,y_j)=C_{ij}\)</span>，我们将上面的乘子向量<span class="math inline">\(f,g\)</span>推广成<span class="math inline">\(f(x_i)=f_i,g(y_j)=g_j\)</span>。得到对偶形式为
<span class="math display">\[
\underset{(f,g)\in R(c)}{\sup}\int_X f(x) \, d\alpha(x)+\int_Y g(y) \,
d\beta(y)
\]</span> 其中<span class="math inline">\(R(c)=\{(f,g):\forall
(x,y),f(x_i)+g(y_j) \le c(x_i,y_j)\}\)</span></p>
<h3 id="通俗理解原问题和对偶问题">通俗理解原问题和对偶问题</h3>
<p>假设有一个运营商运营着<span class="math inline">\(n\)</span>个仓库和<span class="math inline">\(m\)</span>个工厂，每个仓库有<span class="math inline">\(a_i\)</span>质量的商品，每个工厂需要<span class="math inline">\(b_j\)</span>质量的商品，从<span class="math inline">\(i\)</span>仓库到<span class="math inline">\(j\)</span>工厂运输单位质量的商品需要成本<span class="math inline">\(C_{ij}\)</span>。</p>
<p><strong>原问题就是站在运营商的角度考虑</strong>：找出最优的传输方案<span class="math inline">\(P^*\)</span>使得传输总成本<span class="math inline">\(\sum_{i,j}C_{ij}P_{ij}\)</span>最小。</p>
<p>假设这个运营商外包给了一个供应商，供应商只需要给每个仓库和工厂定价：单位质量的商品第<span class="math inline">\(i\)</span>个仓库收取<span class="math inline">\(f_i\)</span>的费用，第<span class="math inline">\(j\)</span>个工厂收取<span class="math inline">\(g_j\)</span>的费用。</p>
<p>供应商不能随便定价，供应商在定价过程中需要保证所有<span class="math inline">\(f_i+g_j \le
C_{ij}\)</span>，一旦超过这个价格，那在某些路径上运营商就要付出额外的价格，还不如自己运呢。</p>
<p>所以对偶问题就是站在供应商的角度来考虑：在<span class="math inline">\(f_i+g_j \le C_{ij}\)</span>的情况下希望收费<span class="math inline">\(\langle f,a \rangle + \langle g,b
\rangle\)</span>最大。</p>
<h2 id="互补松弛条件">互补松弛条件</h2>
<p>考虑原问题的最优解<span class="math inline">\(P^*\)</span>和对偶问题最优对<span class="math inline">\((f^*,g^*)\)</span>的关系，并且证明他们满足互补松弛条件</p>
<p>设<span class="math inline">\(\bar z\)</span>为原问题的最优解，<span class="math inline">\(\underline
z\)</span>为对偶问题的最优解，先证明<strong>弱对偶性</strong>满足，即<span class="math inline">\(\bar z \ge \underline z\)</span></p>
<p>首先对原始问题进行一下改写，把决策变量<span class="math inline">\(P\)</span>“拉直”为<span class="math inline">\(p\)</span>： <span class="math display">\[
P\in \mathbb R^{n \times m}\in U(a,b) \iff p \in \mathbb R^{nm}_+,Ap=
\begin{bmatrix}
   a  \\
   b
\end{bmatrix}
\]</span> 其中 <span class="math display">\[
A=
\begin{bmatrix}
   1^T_n \otimes I_m  \\
   I_n \otimes1^T_m
\end{bmatrix},A \in \mathbb R^{(n+m)\times nm}
\]</span> 定义<span class="math inline">\(c\)</span>也为成本矩阵对应的展平形式，则原始问题的拉格朗日函数：
<span class="math display">\[
H(h)=\underset{p\in \mathbb R^{nm}_+}{\min}(c^Tp-h^T(Ap-q))
\]</span> 其中<span class="math inline">\(q=\begin{bmatrix}
   a  \\
   b
\end{bmatrix}\)</span>。这是一个松弛问题，因为约束<span class="math inline">\(Ap=q\)</span>被软化为罚函数形式。对于原问题的最优解<span class="math inline">\(p^*\)</span>，对于任何<span class="math inline">\(h\)</span>，有： <span class="math display">\[
H(h)\le c^T p^*-h^T(Ap^*-q)=c^T p^*=\bar z
\]</span> 当<span class="math inline">\(h\)</span>满足对偶可行性(<span class="math inline">\(A^Th\le c\)</span>)时，对偶问题的目标值<span class="math inline">\(h^Tq\)</span>是拉格朗日函数的下界。</p>
<p>因此结合上式： <span class="math display">\[
\underline z=\underset{h,A^Th \le c}{\max}h^Tq\le
\underset{h}{\max}h^Tq+\underset{p\in \mathbb
R^{mn}_+}{\min}(c^T-h^TA)p=\underset{h\in \mathbb R^{n+m}}{\max}H(h)\le
c^T p^*=\bar z
\]</span>
这表明对偶问题的最优值不超过原始问题的最优值，从而证明了弱对偶性。</p>
<p>可以证明强对偶性也是满足的，证明过程需要slater条件。</p>
<p>对于连续型，也是有如下定理保证了强对偶性：</p>
<blockquote>
<p>设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>是定义在完备可分度量空间<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>上的概率测度，<span class="math inline">\(c:X\times Y \to \mathbb
R_+\)</span>是一个可测函数，那么： <span class="math display">\[
\underset{\pi\in\Pi(\mu,\nu)}{\inf}\int_{X\times Y}c \, d\pi =
\underset{(\varphi,\psi)}{\sup}\lbrack \int_X \varphi \, d\mu + \int_Y
\psi \, d\nu \rbrack
\]</span></p>
</blockquote>
<p>最优化理论告诉我们，如果强对偶性满足，那么我们可以使用KKT中的经典的互补松弛条件得到：
<span class="math display">\[
P^*(x,y)(f^*(x)+g^*(y)-c(x,y))=0
\]</span></p>
<p>这就是原问题的最优解<span class="math inline">\(P^*\)</span>和对偶问题最优对<span class="math inline">\((f^*,g^*)\)</span>的关系。即在支撑集上<span class="math inline">\(P^*&gt;0\)</span>，对偶问题的最优函数永远满足<span class="math inline">\(f^*(x)+g^*(y)=c(x,y)\)</span></p>
<h2 id="c-变换">c-变换</h2>
<p>现在我们知道对偶问题两个函数的一个很重要的性质，就是在<span class="math inline">\(P^*\)</span>支撑集上有<span class="math inline">\(f^*(x)+g^*(y)=c(x,y)\)</span></p>
<p>下面假设这个函数对不是最优的，如果我们固定了<span class="math inline">\(f(x)\)</span>，能否得到一个“最好”的<span class="math inline">\(g(y)\)</span>使得对偶问题的目标函数尽可能大？c-变换就回答了这个问题</p>
<p>因为 <span class="math display">\[
f(x)+g(y)\le c(x,y)\\
\iff g(y)\le c(x,y)-f(x)\\
\iff g(y)\le \underset{x}{\inf} \{c(x,y)-f(x)\}
\]</span> 而<span class="math inline">\(f(x)\)</span>已经固定，我们希望<span class="math inline">\(\int g(y) \, d\beta(y)\)</span>越大越好，所以<span class="math inline">\(g(y)\)</span>越大越好，自然就得到了一个“最好的”函数：
<span class="math display">\[
\bar f(y)=\underset{x}{\inf}\{c(x,y)-f(x)\}
\]</span> 这个<span class="math inline">\(\bar f(y)\)</span>就叫做<span class="math inline">\(f\)</span>的c-变换</p>
<p>如果一个函数<span class="math inline">\(f\)</span>可以写成某个<span class="math inline">\(g\)</span>的c-变换，那么就称<span class="math inline">\(f\)</span>是c-凹的</p>
<p><strong>注</strong>：对偶问题的最优解<span class="math inline">\((f^*,g^*)\)</span>一定是c-凹的。否则令另一个是其c-变换，目标函数变大，矛盾。</p>
<p>下面定理还保证了最优对的存在性： &gt;设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>是<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>上的概率测度使得 <span class="math display">\[
\int_{X\times Y}c(x,y) \, d\mu(x) \, d\nu(y)&lt;\infty
\]</span> 则对偶 Kantorovich 问题存在一个最优对<span class="math inline">\((\varphi,\psi)\)</span>且<span class="math inline">\(\bar \varphi = \psi\)</span>，<span class="math inline">\(\bar \psi=\varphi\)</span>几乎处处成立。</p>
<h2 id="kantorovichrubinstein定理">Kantorovich–Rubinstein定理</h2>
<p>虽然对偶问题要求两个函数<span class="math inline">\(f,g\)</span>
，但通过c-transform可以将这两个函数“联系”起来，对偶问题从 <span class="math display">\[
\underset{(f,g)\in R(c)}{\sup}\int_X f \, d\mu + \int_Y g \, d\nu
\]</span> 变为 <span class="math display">\[
\underset{f}{\sup}\int_X f \, d\mu + \int_Y \bar f \, d\nu
\]</span> 我们可以证明，如果在度量空间<span class="math inline">\(X=Y\)</span>上，<span class="math inline">\(d(x,y)\)</span>是度量，且<span class="math inline">\(c(x,y)=d(x,y)\)</span>。如果<span class="math inline">\(f=\bar g\)</span>是一组c-变换，那么<span class="math inline">\(f\)</span>是d-Lipschitz的。</p>
<p>证明如下： <span class="math display">\[
f(z)=\underset{y}{\inf} \lbrace d(z,y)-g(y) \rbrace \le
\underset{y}{\inf} \lbrace d(z,x)+d(x,y)-g(y) \rbrace=f(x)+d(z,x)
\]</span> 互换<span class="math inline">\(z,x\)</span>即得到<span class="math inline">\(|f(x)-g(z)|\le d(x,z)\)</span></p>
<p>然后我们可以继续证明，如果<span class="math inline">\(f\)</span>是d-Lipschitz的，那么其c-变换<span class="math inline">\(\bar f = -f\)</span>： &gt;令<span class="math inline">\(x=y\)</span>，有<span class="math inline">\(\bar
f(x)=\underset{x}{\inf}\{d(x,x)-f(x)\}\le -f(x)\)</span>，又因为<span class="math inline">\(f\)</span>是d-Lipschitz的。所以<span class="math inline">\(\bar f(y)=\underset{x}{\inf}\{d(x,y)-f(x)\}\ge
-f(y)\)</span>。所以<span class="math inline">\(\bar f=-f\)</span></p>
<p>综上，结合强对偶性，我们有 <span class="math display">\[
\underset{\pi\in U(a,b)}{\inf}\int_{X^2}d(x,y)\,
d\pi=\underset{f}{\sup}\int_X f \, d\mu + \int_X \bar f \, d\nu =
\underset{\lVert f \rVert_{\text{Lip}}\le 1}{\sup}\int_X f \, d\mu -
\int_X  f \, d\nu
\]</span> 其中，<span class="math inline">\(\lVert f
\rVert_{\text{Lip}}=\underset{x \not = y}{\sup}\frac {\lVert
f(x)-f(y)\rVert}{d(x,y)}\)</span></p>
<p>我们现在得到了一个非常好非常好的结论：当成本函数是一个度量的时候，对偶问题可以变成上面的形式，我们只需要求解一个d-Lipschitz函数就可以了。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>最优化</tag>
      </tags>
  </entry>
</search>
