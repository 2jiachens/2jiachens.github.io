<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Active_Contours_Without_Edges</title>
    <url>/2025/03/03/Active-Contours-Without-Edges/</url>
    <content><![CDATA[<h1 id="active-contours-without-edges">Active Contours Without
Edges</h1>
<p>期刊：IEEE Transactions on image processin</p>
<p>时间：2001</p>
<h2 id="摘要">摘要</h2>
<p>在本文中，我们提出了一种新的活动轮廓模型，用于检测给定图像中的对象，基于曲线演化技术，用于分割和水平集的Mumford-Shah函数。我们的模型可以检测其边界不一定由梯度定义的对象。我们最小化了能量，这可以看作是最小分区问题的一个特殊情况。在水平集公式中，问题变成了一个
“平均曲率流”
--类似演化活动等值线，它将在所需的边界上停止。然而，停止项并不依赖于图像的梯度，就像经典的主动轮廓模型那样，而是与图像的特定分割有关。我们将给出一个使用有限差分的数值算法。最后，我们将介绍各种实验结果，特别是一些基于梯度的经典蛇方法不适用的示例。此外，初始曲线可以位于图像中的任何位置，并且会自动检测内部轮廓。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>活动轮廓模型或蛇的基本思想是在给定图像的约束下，演化一条曲线来检测图像中的物体。例如，从被检测物体周围的曲线开始，曲线向其内部法线方向移动，并且必须停在物体的边界上。</p>
<p>设<span class="math inline">\(\Omega\)</span>是<span class="math inline">\(\mathbb{R}^2\)</span>的有界开子集，<span class="math inline">\(\partial \Omega\)</span>其边界。设 <span class="math inline">\(u_0: \bar{\Omega} \rightarrow
\mathbb{R}\)</span>为给定图像，并且<span class="math inline">\(C(s):[0,1] \rightarrow
\mathbb{R}^2\)</span>为参数化曲线。</p>
<p>在经典的蛇和活动轮廓模型中，根据图像<span class="math inline">\(u_0\)</span>的梯度，使用一个边界检测算子来停止在期望物体边界上的演化曲线。接下来我们简要回顾这些模型。</p>
<p>蛇模型[9]为：<span class="math inline">\(\inf _C
J_1(C)\)</span>，其中 <span class="math display">\[
\begin{aligned}
J_1(C)= &amp; \alpha \int_0^1\left|C^{\prime}(s)\right|^2 d s+\beta
\int_0^1\left|C^{\prime \prime}(s)\right| d s \\
&amp; -\lambda \int_0^1\left|\nabla u_0(C(s))\right|^2 d s \qquad(1)
\end{aligned}
\]</span></p>
<p>式中，<span class="math inline">\(\alpha, \beta\)</span>和<span class="math inline">\(\lambda\)</span>为正参数。前两项控制轮廓的光滑性（内能），第三项则吸引轮廓朝向图像内的物体（外部能量）。观察到，通过最小化能量(1)，我们试图将曲线定位在极大值点<span class="math inline">\(|\nabla
u_0|\)</span>，作为一个边界检测算子，同时保持曲线的平滑性(对象边界)。</p>
<p>一般的边界检测算子可以通过一个递减函数<span class="math inline">\(g\)</span>来定义，取决于图像的梯度，使得 <span class="math display">\[
\lim _{z \rightarrow \infty} g(z)=0
\]</span></p>
<p>例如 <span class="math display">\[
g\left(\left|\nabla u_0(x, y)\right|\right)=\frac{1}{1+\left|\nabla
G_\sigma(x, y) * u_0(x, y)\right|^p}, \quad p \geq 1
\]</span></p>
<p>其中，<span class="math inline">\(G_\sigma * u_0\)</span>是<span class="math inline">\(u_0\)</span>更平滑的版本，是图像与高斯<span class="math inline">\(G_\sigma(x, y)=\)</span> <span class="math inline">\(\sigma^{-1 / 2} e^{-\left|x^2+y^2\right| / 4
\sigma}\)</span>的卷积。该函数<span class="math inline">\(g\left(\left|\nabla
u_0\right|\right)\)</span>在同质区域为正，在边缘处为零</p>
<p>在曲线演化问题中，水平集方法，特别是Osher和Sethian[19]的平均曲率运动得到了广泛的应用，因为它允许尖点、角点和自动拓扑变化。此外，问题的离散是在固定的矩形网格上进行的。曲线<span class="math inline">\(C\)</span>由Lipschitz函数<span class="math inline">\(\phi\)</span>隐式表示，由<span class="math inline">\(C=\{(x, y) \mid \phi(x,
y)=0\}\)</span>，曲线的演化由函数时刻的零水平曲线给出。将曲线沿法线方向以速度演化，就相当于求解了微分方程[19]
<span class="math display">\[
\frac{\partial \phi}{\partial t}=|\nabla \phi| F ; \phi(0, x,
y)=\phi_0(x, y)
\]</span></p>
<p>其中集合<span class="math inline">\(\left\{(x, y) \mid \phi_0(x,
y)=0\right\}\)</span>定义了初始轮廓。当<span class="math inline">\(F=\operatorname{div}(\nabla \phi(x, y) /|\nabla
\phi(x, y)|)\)</span>是通过<span class="math inline">\((x,
y)\)</span>的<span class="math inline">\(\phi\)</span>的水平曲线的曲率时，一种特殊情况是平均曲率运动。方程变为
<span class="math display">\[
\begin{cases}\frac{\partial \phi}{\partial t}=|\nabla \phi|
\operatorname{div}\left(\frac{\nabla \phi}{|\nabla \phi|}\right), &amp;
t \in(0, \infty), x \in \mathbb{R}^2 \\ \phi(0, x, y)=\phi_0(x, y),
&amp; x \in \mathbb{R}^2\end{cases}
\]</span></p>
<p>基于平均曲率运动的几何活动轮廓模型由以下演化方程[3]给出 <span class="math display">\[
\left\{\begin{array}{l}
\frac{\partial \phi}{\partial t}=g\left(\left|\nabla
u_0\right|\right)|\nabla \phi|\left(\operatorname{div}\left(\frac{\nabla
\phi}{|\nabla \phi|}\right)+\nu\right) \\
\text { in }(0, \infty) \times \mathbb{R}^2 \\
\phi(0, x, y)=\phi_0(x, y) \text { in } \mathbb{R}^2
\end{array}\right. \qquad(2)
\]</span></p>
<p>其中 <span class="math inline">\(g\left(\left|\nabla
u_0\right|\right) \quad\)</span> 边函数 <span class="math inline">\(p=2\)</span>; <span class="math inline">\(\nu \geq
0 \quad\)</span> 常数； <span class="math inline">\(\phi_0
\quad\)</span> 初始水平集函数。</p>
<p>它的零水平集曲线以速度<span class="math inline">\(g\left(\left|\nabla
u_0\right|\right)(\operatorname{curv}(\phi)(x,
y)+\mu)\)</span>沿法线方向移动，因此在期望的边界上停止，<span class="math inline">\(g\)</span>在那里消失。常数<span class="math inline">\(\nu\)</span>是为了使量<span class="math inline">\((\operatorname{div}(\nabla \phi(x, y) /|\nabla
\phi(x,
y)|)+\nu)\)</span>始终保持为正而选择的修正项。当曲率变为零或负时，这个常数可以解释为将曲线推向物体的力。同时，<span class="math inline">\(\nu&gt;0\)</span>也是对曲线内部面积的约束，提高了传播速度。</p>
<p>另外两个基于水平集的活动轮廓模型在[13]中被提出，再次使用图像梯度停止曲线。第一个是
<span class="math display">\[
\left\{\begin{array}{l}
\frac{\partial \phi}{\partial t}=|\nabla
\phi|\left(-\nu+\frac{\nu}{\left(M_1-M_2\right)}\left(\left|\nabla
G_\sigma * u_0\right|-M_2\right)\right), \\
\phi(0, x, y)=\phi_0(x, y) \text { in }[0, \infty) \times \mathbb{R}^2
\end{array}\right.
\]</span></p>
<p>其中<span class="math inline">\(\nu\)</span>是一个常数，并且<span class="math inline">\(M_1\)</span>和<span class="math inline">\(M_2\)</span>分别是图像梯度<span class="math inline">\(\mid \nabla G_\sigma{ }^*u_0
\mid\)</span>幅值的最大值和最小值。再次，在梯度最高的点上，演化曲线的速度变为零，因此曲线停止在强梯度定义的期望边界上。第二个模型[13]与几何模型[3]当<span class="math inline">\(p=1\)</span>类似，其他相关的工作有[14]和[15]</p>
<p>测地线模型[4]为 <span class="math display">\[
\inf _C J_2(C)=2 \int_0^1\left|C^{\prime}(s)\right| \cdot g\left(\mid
\nabla u_0(C(s) \mid) d s\right. \qquad(3)
\]</span></p>
<p>这是一个黎曼空间中的测地线计算问题，根据图像<span class="math inline">\(u_0\)</span>诱导的一个度量。求解最小化问题(3)，就是在该度量中寻找新长度最小的路径。当<span class="math inline">\(g\left(\mid \nabla u_0(C(s)
\mid)\right.\)</span>消失时，即当曲线在物体的边界上时，将得到一个极小点<span class="math inline">\(C\)</span>。文献[4]中的测地活动轮廓模型(3)也具有水平集形式
<span class="math display">\[
\left\{\begin{array}{l}
\frac{\partial \phi}{\partial t}=|\nabla
\phi|\left(\operatorname{div}\left(g\left(\left|\nabla u_0\right|\right)
\frac{\nabla \phi}{|\nabla \phi|}\right)+\nu g\left(\left|\nabla
u_0\right|\right)\right) \\
\text { in }\left[0, \infty\left[\times \mathbb{R}^2\right.\right. \\
\phi(0, x, y)=\phi_0(x, y) \text { in } \mathbb{R}^2 .
\end{array}\right. \qquad(4)
\]</span></p>
<p>由于所有这些经典的Snakes和活动轮廓模型都依赖于边函数，依赖于图像梯度<span class="math inline">\(\left|\nabla
u_0\right|\)</span>，以停止曲线演化，因此这些模型只能检测具有梯度定义的边缘的物体。在实际中，离散梯度是有界的，那么停止函数在边上永远不为零，曲线可能会穿过边界，特别是对于[3]，[13]
-
[15]中的模型。如果图像是非常噪声的，那么各向同性平滑高斯必须是强的，这将平滑边缘。在本文中，我们提出了一种不同的活动轮廓模型，没有一个停止边函数，即一个不基于图像梯度的停止过程的模型。停止项基于Mumford-Shah分割技术[18]。通过这种方式，我们获得了一个模型，它可以检测有梯度或无梯度的轮廓，例如具有非常光滑边界的物体，甚至具有不连续边界的物体。此外，我们的模型具有水平集形式，内部轮廓被自动检测，初始曲线可以在图像中的任何位置。</p>
<p>论文的大纲如下。在下一节中，我们将我们的模型引入能量最小化，并讨论其与用于分割的Mumford-
Shah泛函的关系。此外，我们用水平集函数来描述模型并计算相关的Euler-Lagrange方程。在第三节中我们给出了求解该问题的迭代算法及其离散化。在第四节中，我们通过在合成图像和真实图像上的各种数值结果来验证我们的模型，展示了我们前面所描述的模型的优势，并通过一个简短的总结部分来结束本文。</p>
<p>其他相关的工作还有[29]，[10]，[26]，[24]关于活动轮廓和分割，[28]，[11]关于从无组织点重建形状，最后是最近的工作[20]和[21]，其中提出了一种基于概率的测地活动区域模型结合经典的基于梯度的活动轮廓技术。</p>
<h2 id="模型的描述">模型的描述</h2>
<p>我们定义<span class="math inline">\(\Omega\)</span>中的演化曲线<span class="math inline">\(C\)</span>作为<span class="math inline">\(\Omega\)</span>的一个开子集<span class="math inline">\(\omega\)</span>的边界。(即 <span class="math inline">\(\omega \subset \Omega\)</span>, <span class="math inline">\(C=\partial \omega\)</span> )。下边，inside <span class="math inline">\((C)\)</span>表示区域<span class="math inline">\(\omega\)</span>，outside <span class="math inline">\((C)\)</span>表示区域<span class="math inline">\(\Omega \backslash \bar{\omega}\)</span></p>
<p>我们的方法是基于能量最小化的分割方法。让我们首先在一个简单的案例中解释模型的基本思想。假设图像<span class="math inline">\(u_0\)</span>由两个强度近似为分段常数的区域组成，且具有不同的值<span class="math inline">\(u_0^i\)</span>和<span class="math inline">\(u_0^o\)</span>。进一步假设待检测物体由具有值<span class="math inline">\(u_0^i\)</span>的区域表示。令其边界为<span class="math inline">\(C_0\)</span>。那么我们就有了对象内部的 <span class="math inline">\(u_0 \approx u_0^i\)</span>，和对象外部的<span class="math inline">\(u_0 \approx
u_0^o\)</span>。现在让我们考虑下面拟合项： <span class="math display">\[
\begin{aligned}
F_1(C)+F_2(C)= &amp; \int_{\text {inside }(C)}\left|u_0(x,
y)-c_1\right|^2 d x d y \\
&amp; +\int_{\text {outside }(C)}\left|u_0(x, y)-c_2\right|^2 d x d y
\end{aligned}
\]</span></p>
<p>这里<span class="math inline">\(C\)</span>是任何其他变量曲线，常数<span class="math inline">\(c_1,
c_2\)</span>视分别为内部和外部的平均值。在这种简单的情况下，很明显，对象的边界<span class="math inline">\(C_0\)</span>是拟合项的最小值。 <span class="math display">\[
\inf _C\left\{F_1(C)+F_2(C)\right\} \approx 0 \approx
F_1\left(C_0\right)+F_2\left(C_0\right)
\]</span></p>
<p>如果曲线<span class="math inline">\(C\)</span>在对象之外，那么<span class="math inline">\(F_1(C)&gt;0\)</span>且<span class="math inline">\(F_2(C) \approx 0\)</span>。如果曲线<span class="math inline">\(C\)</span>在物体内部，那么 <span class="math inline">\(F_1(C) \approx 0\)</span>但是<span class="math inline">\(F_2(C)&gt;0\)</span>。如果曲线<span class="math inline">\(C\)</span>既在物体内部，又在物体外部，则有<span class="math inline">\(F_1(C)&gt;0\)</span>且<span class="math inline">\(F_2(C)&gt;0\)</span>。最后，如果<span class="math inline">\(C=C_0\)</span>，拟合能量最小，即如果曲线在物体的边界上。这些基本注释如图1所示
<img src="/2025/03/03/Active-Contours-Without-Edges/Active-Contours-Without-Edges\1.png" alt="iamge"></p>
<p>在我们的活动轮廓模型中，我们将最小化上述拟合项，并添加一些正则化项，例如曲线<span class="math inline">\(C\)</span>的宽度，和(或)<span class="math inline">\(C\)</span>内部区域的面积。因此，我们引入能量泛函<span class="math inline">\(F\left(c_1, c_2, C\right)\)</span>，定义为</p>
<p><span class="math display">\[
\begin{aligned}
F\left(c_1, c_2, C\right)= &amp; \mu \cdot \text { Length }(C)+\nu \cdot
\operatorname{Area}(\text { inside }(C)) \\
&amp; +\lambda_1 \int_{\text {inside }(C)}\left|u_0(x, y)-c_1\right|^2 d
x d y \\
&amp; +\lambda_2 \int_{\text {outside }(C)}\left|u_0(x, y)-c_2\right|^2
d x d y
\end{aligned}
\]</span></p>
<p>其中，<span class="math inline">\(\mu \geq 0, \nu \geq 0, \lambda_1,
\lambda_2&gt;0\)</span>为固定参数。在我们几乎所有的数值计算中(见更进一步)，我们固定<span class="math inline">\(\lambda_1=\lambda_2=1\)</span>和<span class="math inline">\(\nu=0\)</span></p>
<p>因此，我们考虑极小化问题： <span class="math display">\[
\inf _{c_1, c_2, C} F\left(c_1, c_2, C\right)
\]</span></p>
<p>Remark 1: 在我们的模型中，Length <span class="math inline">\((C)\)</span>可以被改写为<span class="math inline">\((\text { Length }(C))^p\)</span>，<span class="math inline">\(p \geq 1\)</span>。如果我们考虑任意维数<span class="math inline">\(N&gt;1\)</span>(即<span class="math inline">\(\Omega \subset
\mathbb{R}^N\)</span>)的情形，那么<span class="math inline">\(p\)</span>可以有如下的值：对所有<span class="math inline">\(N\)</span>，<span class="math inline">\(p=1\)</span>或者<span class="math inline">\(p=N
/(N-1)\)</span>。对于最后一个表达式，我们使用等周不等式[7]，在某种意义上说Length与Area是"可比"的
<span class="math display">\[
\operatorname{Area}(\operatorname{inside}(C)) \leq c
\cdot(\operatorname{Length}(C))^{N /(N-1)}
\]</span></p>
<p>式中<span class="math inline">\(c\)</span>为仅依赖于<span class="math inline">\(N\)</span>的常数</p>
<p>A. 与Mumford-Shah泛函的关系</p>
<p>用于分割的Mumford-Shah泛函是[18] <span class="math display">\[
\begin{aligned}
F^{\mathrm{MS}}(u, C)= &amp; \mu \cdot \text { Length }(C) \\
&amp; +\lambda \int_{\Omega}\left|u_0(x, y)-u(x, y)\right|^2 d x d y \\
&amp; +\int_{\Omega \backslash C}|\nabla u(x, y)|^2 d x d y
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(u_0: \bar{\Omega} \rightarrow
\mathbb{R}\)</span>是给定的图像，<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\lambda\)</span>都是正参数。通过最小化该泛函得到的解图像是由光滑区域<span class="math inline">\(R_i\)</span>和具有尖锐边界的区域<span class="math inline">\(C\)</span>组成的。</p>
<p>这个问题的一个简化形式是将<span class="math inline">\(F^{\mathrm{MS}}\)</span>的限制简化为分段常数函数<span class="math inline">\(u\)</span>，即<span class="math inline">\(u=\)</span> constant <span class="math inline">\(c_i\)</span>在<span class="math inline">\(\Omega
\backslash C\)</span>的每个连通分支<span class="math inline">\(R_i\)</span>上。因此，正如D. Mumford和J . Shah
[18]所指出的那样，在每个连通分支<span class="math inline">\(R_i\)</span>上<span class="math inline">\(c_i=\operatorname{average}\left(u_0\right)\)</span>。简化后的情形称为最小划分问题。</p>
<p>我们的活动轮廓模型当<span class="math inline">\(\nu=0\)</span>和<span class="math inline">\(\lambda_1=\lambda_2=\lambda\)</span>时是极小分割问题的特例，在该问题中，我们寻找的是<span class="math inline">\(u_0\)</span>的最佳逼近<span class="math inline">\(u\)</span>，是一个只取两个值的函数，即 <span class="math display">\[
u=\left\{\begin{array}{l}
\operatorname{average}\left(u_0\right) \text { inside } C \\
\operatorname{average}\left(u_0\right) \text { outside } C
\end{array}\right. \qquad(5)
\]</span></p>
<p>并且具有一条边缘<span class="math inline">\(C\)</span>，由蛇或活动轮廓表示。</p>
<p>最小划分问题的这种特殊情况可以用水平集方法来描述和解决[19]。这将在下一节介绍。</p>
<p>B. 模型的水平集设定</p>
<p>在水平集方法[19]中，用一个Lipschitz函数的零水平集表示<span class="math inline">\(C \subset \Omega\)</span>，使得</p>
<p><span class="math display">\[
\left\{\begin{array}{l}
C=\partial \omega=\{(x, y) \in \Omega: \phi(x, y)=0\} \\
\text { inside }(C)=\omega=\{(x, y) \in \Omega: \phi(x, y)&gt;0\} \\
\text { outside }(C)=\Omega \backslash \bar{\omega}=\{(x, y) \in \Omega:
\phi(x, y)&lt;0\}
\end{array}\right.
\]</span></p>
<p><span class="math inline">\(\omega \subset
\Omega\)</span>是开的，且<span class="math inline">\(C=\partial
\omega\)</span>。我们在图2中说明了上述关于水平集函数<span class="math inline">\(\phi\)</span>的假设和符号，定义了演化曲线<span class="math inline">\(C\)</span>。 <img src="/2025/03/03/Active-Contours-Without-Edges/Active-Contours-Without-Edges\2.png" alt="iamge"></p>
<p>对于我们的变分活动轮廓模型的水平集形式，我们用未知变量<span class="math inline">\(\phi\)</span>代替未知变量<span class="math inline">\(C\)</span>，我们遵循[ 27 ]。</p>
<p>利用Heaviside函数<span class="math inline">\(H\)</span>和一维Dirac测度<span class="math inline">\(\delta_0\)</span>，并分别定义</p>
<p><span class="math display">\[
H(z)=\left\{\begin{array}{ll}
1, &amp; \text { if } z \geq 0 \\
0, &amp; \text { if } z&lt;0
\end{array} \quad \delta_0(z)=\frac{d}{d z} H(z)\right.
\]</span></p>
<p>我们用下面的方式来表示能量<span class="math inline">\(F\)</span>中的项： <span class="math display">\[
\begin{aligned}
\operatorname{Length}\{\phi=0\} &amp; =\int_{\Omega}|\nabla H(\phi(x,
y))| d x d y \\
&amp; =\int_{\Omega} \delta_0(\phi(x, y))|\nabla \phi(x, y)| d x d y \\
\text { Area }\{\phi \geq 0\} &amp; =\int_{\Omega} H(\phi(x, y)) d x d y
\end{aligned}
\]</span></p>
<p>且 <span class="math display">\[
\begin{aligned}
&amp; \int_{\phi&gt;0}\left|u_0(x, y)-c_1\right|^2 d x d y \\
&amp; \quad=\int_{\Omega}\left|u_0(x, y)-c_1\right|^2 H(\phi(x, y)) d x
d y \\
&amp; \int_{\phi&lt;0}\left|u_0(x, y)-c_2\right|^2 d x d y \\
&amp; \quad=\int_{\Omega}\left|u_0(x, y)-c_2\right|^2(1-H(\phi(x, y))) d
x d y
\end{aligned}
\]</span></p>
<p>那么，能量<span class="math inline">\(F\left(c_1, c_2,
\phi\right)\)</span>可以写成</p>
<p><span class="math display">\[
\begin{aligned}
&amp; F\left(c_1, c_2, \phi\right) \\
&amp; \quad=\quad \mu \int_{\Omega} \delta(\phi(x, y))|\nabla \phi(x,
y)| d x d y \\
&amp; \quad+\nu \int_{\Omega} H(\phi(x, y)) d x d y \\
&amp; \quad+\lambda_1 \int_{\Omega}\left|u_0(x, y)-c_1\right|^2
H(\phi(x, y)) d x d y \\
&amp; \quad+\lambda_2 \int_{\Omega}\left|u_0(x,
y)-c_2\right|^2(1-H(\phi(x, y))) d x d y
\end{aligned}
\]</span></p>
<p>我们注意到，正如(5)中所定义的<span class="math inline">\(u\)</span>那样，我们模型的解作为Mumford-Shah极小划分问题的一个特例，可以简单地写成使用水平集公式为
<span class="math display">\[
u(x, y)=c_1 H(\phi(x, y))+c_2(1-H(\phi(x, y))),(x, y) \in \bar{\Omega}
\]</span></p>
<p>保持<span class="math inline">\(\phi\)</span>固定和最小化关于常数<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>的能量<span class="math inline">\(F\left(c_1, c_2,
\phi\right)\)</span>，很容易表达这些常数的函数通过 <span class="math display">\[
c_1(\phi)=\frac{\int_{\Omega} u_0(x, y) H(\phi(x, y)) d x d
y}{\int_{\Omega} H(\phi(x, y)) d x d y} \qquad(6)
\]</span></p>
<p>若<span class="math inline">\(\int_{\Omega} H(\phi(x, y)) d x d
y&gt;0\)</span>(即如果曲线在<span class="math inline">\(\Omega\)</span>中具有非空内部)，且</p>
<p><span class="math display">\[
c_2(\phi)=\frac{\int_{\Omega} u_0(x, y)(1-H(\phi(x, y))) d x d
y}{\int_{\Omega}(1-H(\phi(x, y))) d x d y} \qquad(7)
\]</span></p>
<p>若<span class="math inline">\(\int_{\Omega}(1-H(\phi(x, y))) d x d
y&gt;0\)</span> (即如果曲线在<span class="math inline">\(\Omega\)</span>中有一个非空的外部)。对于相应的'退化'情况，对<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>的取值没有任何限制。那么，<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>实际上由下式给出 <span class="math display">\[
\left\{\begin{array}{l}
c_1(\phi)=\operatorname{average}\left(u_0\right) \text { in }\{\phi \geq
0\} \\
c_2(\phi)=\operatorname{average}\left(u_0\right) \text { in
}\{\phi&lt;0\}
\end{array}\right.
\]</span></p>
<p>Remark 2: 由前面的公式可知，能量只能写成关于<span class="math inline">\(H(\phi)\)</span>的函数，也就是集合<span class="math inline">\(\omega\)</span>的特征函数，用<span class="math inline">\(\chi_\omega\)</span>表示。那么我们就可以将能量改写成新的形式<span class="math inline">\(\mathcal{F}\)</span> <span class="math display">\[
\begin{aligned}
\mathcal{F}\left(\chi_\omega\right)= &amp; \mu \int_{\Omega}\left|\nabla
\chi_\omega(x, y)\right| d x d y+\nu \int_{\Omega} \chi_\omega(x, y) d x
d y \\
&amp; +\lambda_1 \int_{\Omega}\left(u_0(x, y)-c_1\left(\chi_\omega(x,
y)\right)\right)^2 \chi_\omega(x, y) d x d y \\
&amp; +\lambda_2 \int_{\Omega}\left(u_0(x, y)\right. \\
&amp; \left.-c_2\left(\chi_\omega(x,
y)\right)\right)^2\left(1-\chi_\omega(x, y)\right) d x d y
\end{aligned}
\]</span></p>
<p>因此，我们可以考虑新的极小化问题</p>
<p><span class="math display">\[
\inf _{\chi_\omega} \mathcal{F}\left(\chi_\omega\right), \chi_\omega(x,
y) \in\{0,1\} \mathcal{L}-\text { a.e. } \qquad(8)
\]</span></p>
<p>在<span class="math inline">\(\Omega\)</span>有限周长集合的特征函数中。这里，<span class="math inline">\(\mathcal{L}-\)</span>
a.e.表示关于Lebesgue测度几乎处处。</p>
<p>当然，我们期望能量<span class="math inline">\(F\left(c_1, c_2,
C\right)\)</span>极小元的存在性，因为有几个一般性的结果：我们的模型是极小划分问题的一个特例，它的存在性已经在[18]
(假设<span class="math inline">\(u_0\)</span>在<span class="math inline">\(\bar{\Omega}\)</span>上是连续的)，以及更一般的数据<span class="math inline">\(u_0\)</span>在[16]和[17]中得到了证明。同时，一般的Mumford-Shah分割问题的存在性已经在[5]中得到了证明。另一方面，由全变分
<span class="math inline">\(\int_{\Omega}\left|\nabla \chi_\omega\right|
d x d
y\)</span>的下半连续性和变分法的经典引理很容易证明，我们的极小化问题(8)存在极小点(这可以作为存在性的另一种证明)。
在本文中，水平集函数<span class="math inline">\(\phi\)</span>仅用于表示曲线，具有许多数值上的优势，但问题也只能用特征函数来描述和求解。</p>
<p>为了计算未知函数<span class="math inline">\(\phi\)</span>的Euler-Lagrange方程，我们考虑函数<span class="math inline">\(H\)</span>和<span class="math inline">\(\delta_0\)</span>的稍微正则化版本，这里用<span class="math inline">\(H_{\varepsilon}\)</span>和<span class="math inline">\(\delta_{\varepsilon}\)</span>，<span class="math inline">\(\varepsilon \rightarrow 0\)</span>表示。令<span class="math inline">\(H_{\varepsilon}\)</span>表示<span class="math inline">\(H\)</span>在<span class="math inline">\(C^2(\bar{\Omega})\)</span>上的正则化，且<span class="math inline">\(\delta_{\varepsilon}=H_{\varepsilon}^{\prime}\)</span>。我们将进一步给出这类近似的例子。用相关联的正则化泛函表示<span class="math inline">\(F_{\varepsilon}\)</span>，定义为 <span class="math display">\[
\begin{aligned}
&amp; F_{\varepsilon}\left(c_1, c_2, \phi\right) \\
&amp;= \mu \int_{\Omega} \delta_{\varepsilon}(\phi(x, y))|\nabla \phi(x,
y)| d x d y \\
&amp;+\nu \int_{\Omega} H_{\varepsilon}(\phi(x, y)) d x d y \\
&amp;+\lambda_1 \int_{\Omega}\left|u_0(x, y)-c_1\right|^2
H_{\varepsilon}(\phi(x, y)) d x d y \\
&amp;+\lambda_2 \int_{\Omega}\left|u_0(x,
y)-c_2\right|^2\left(1-H_{\varepsilon}(\phi(x, y))\right) d x d y
\end{aligned}
\]</span></p>
<p>保持<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>固定，并对<span class="math inline">\(\phi\)</span>最小化<span class="math inline">\(F_{\varepsilon}\)</span>，我们推导出<span class="math inline">\(\phi\)</span>相应的Euler-Lagrange方程。通过人工时间<span class="math inline">\(t \geq 0\)</span>参数化下降方向，<span class="math inline">\(\phi(t, x, y)\)</span>(通过定义初始轮廓<span class="math inline">\(\phi(0, x, y)=\phi_0(x, y)\)</span>)中的方程为</p>
<p><span class="math display">\[
\begin{gathered}
\frac{\partial \phi}{\partial t}=\delta_{\varepsilon}(\phi)\left[\mu
\operatorname{div}\left(\frac{\nabla \phi}{|\nabla
\phi|}\right)-\nu-\lambda_1\left(u_0-c_1\right)^2\right. \\
\left.+\lambda_2\left(u_0-c_2\right)^2\right]=0 \text { in }(0, \infty)
\times \Omega \\
\phi(0, x, y)=\phi_0(x, y) \text { in } \Omega \\
\frac{\delta_{\varepsilon}(\phi)}{|\nabla \phi|} \frac{\partial
\phi}{\partial \vec{n}}=0 \text { on } \partial \Omega\qquad(9)
\end{gathered}
\]</span></p>
<p>其中<span class="math inline">\(\vec{n}\)</span>表示边界<span class="math inline">\(\partial \Omega\)</span>的外法线，<span class="math inline">\(\partial \phi / \partial
\vec{n}\)</span>表示边界处<span class="math inline">\(\phi\)</span>的法向导数</p>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>图像分割</tag>
      </tags>
  </entry>
  <entry>
    <title>Lagrange对偶（Lagrange duality）</title>
    <url>/2025/02/10/Lagrange%E5%AF%B9%E5%81%B6/</url>
    <content><![CDATA[<h2 id="问题背景">问题背景</h2>
<p>在一个优化问题中，原始问题通常会带有很多约束条件，这样直接求解原始问题往往是很困难的，于是考虑将原始问题转化为它的对偶问题，通过求解它的对偶问题来得到原始问题的解。<strong>对偶性</strong>（Duality）是凸优化问题的核心内容。
<span id="more"></span></p>
<h2 id="原始问题及其转化">原始问题及其转化</h2>
<p><strong>原始问题</strong></p>
<p>将一个原始最优化问题写成如下形式 <span class="math display">\[
\underset{x}{\min} \quad f_0(x) \\ s.t. \quad f_i(x) \le 0,i=1,2,...,m
\\ \qquad h_j(x)=0,j=1,2,...,p
\]</span> 在求解原问题的对偶问题时，并不要求原始问题一定是凸问题，<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>可以是一般函数而不一定非得是凸函数。</p>
<p><strong>拉格朗日函数</strong></p>
<p>将原始问题的拉格朗日函数定义为 <span class="math display">\[
L(x,\lambda,\nu)=f_0(x)+\sum^m_{i=1}\lambda_i f_i(x)+\sum^p_{j=1}\nu_j
h_j(x)
\]</span> 其中，<span class="math inline">\(x\in \mathbb
R^n,\lambda\in\mathbb R^m,\nu\in\mathbb R^p\)</span>
可以看到，拉格朗日函数<span class="math inline">\(L\)</span>相当于原始问题引入了两个新变量<span class="math inline">\(\lambda,\nu\)</span>，称为拉格朗日乘子</p>
<p><strong>拉格朗日对偶函数</strong></p>
<p>拉格朗日对偶函数通过对拉格朗日函数<span class="math inline">\(x\)</span>取下确界得到，即 <span class="math display">\[
g(\lambda,\nu)=\underset{x}{\inf}L(x,\lambda,\nu)
\]</span> 对偶函数有如下两条重要性质
1.对偶函数一定是凹函数，其凹性与原目标函数和约束函数凹凸与否无关
2.对<span class="math inline">\(\forall\lambda\ge0,\forall\nu\)</span>，如果原问题最优解对应的目标函数值为<span class="math inline">\(P^*\)</span>，则<span class="math inline">\(g(\lambda,\nu)\le p^*\)</span></p>
<h2 id="拉格朗日对偶问题">拉格朗日对偶问题</h2>
<p>根据对偶函数的重要性质2，对<span class="math inline">\(\forall\lambda\ge0,\forall\nu\)</span>，对偶函数<span class="math inline">\(g(\lambda,\nu)\)</span>是原问题最优值<span class="math inline">\(P^*\)</span>的一个下界，最好的下界就是最大化对偶函数，因此构造原问题的对偶问题：
<span class="math display">\[
\underset{\lambda,\nu}{\max}\quad g(\lambda,\nu) \\ s.t.\quad \lambda
\ge 0
\]</span>
由于对偶函数是凹函数，故拉格朗日对偶问题一定是凸优化问题，其对应的最优解为<span class="math inline">\(\lambda^*,\nu^*\)</span>，若对应的最优值为<span class="math inline">\(d^*\)</span>，则总有<span class="math inline">\(d^* \le p^*\)</span></p>
<p>当<span class="math inline">\(d^* \le p^*\)</span>时，称为弱对偶
当<span class="math inline">\(d^* = p^*\)</span>时，称为强对偶 将<span class="math inline">\(p^*-d^*\)</span>称为对偶间隙</p>
<blockquote>
<p>在解存在的情况下，弱对偶总是成立的。
满足强对偶时，可以通过求解对偶问题来得到原始问题的解</p>
</blockquote>
<h2 id="slater条件">Slater条件</h2>
<p>Slater条件用于判断什么情况下强对偶是成立的。
在<strong>原问题是凸问题</strong>的情况下，若<span class="math inline">\(\exists x \in
relint(D)\)</span>，使得约束条件满足： <span class="math display">\[
f_i(x)&lt;0,h_j(x)=0\quad i=1,2,...,p
\]</span> 则强对偶成立 &gt;<span class="math inline">\(relint(D)表示原始凸问题定义域的相对内部，即在定义域上除了边界点以外的所有点\)</span></p>
<h2 id="kkt条件">KKT条件</h2>
<p>在强对偶且<span class="math inline">\(L\)</span>对<span class="math inline">\(x\)</span>可微的前提下，设<span class="math inline">\(x^*,\lambda^*,\nu^*\)</span>分别是原问题和对偶问题的最优解，则以下四组条件称为KKT条件
<span class="math display">\[
\begin{cases}
   \frac{\partial L(x^*,\lambda^*,\nu^*)}{\partial x^*} |_{x=x^*}=0
&amp;\text{(稳定性条件)} \\ \lambda^*_i f_i(x^*)=0
&amp;\text{(互松弛条件)}\\ f_i(x^*)\le 0,h_j(x^*)=0
&amp;\text{(原问题可行性)} \\ \lambda_i^* \ge 0
&amp;\text{(对偶问题可行性)}
    &amp;\text{if } d
\end{cases}
\]</span></p>
<p>对<strong>一般的原问题</strong>，KKT
条件是$x<sup><em>,<sup><em>,<sup>* <span class="math inline">\(为最优解的必要条件，即只要\)</span>x</sup></em>,</sup></em>,</sup>*$为最优解，则一定满足
KKT 条件。</p>
<p>对<strong>原问题为凸问题</strong>，KKT条件是<span class="math inline">\(x^*,\lambda^*,\nu^*\)</span>
为最优解的充要条件</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title>Electrical impedance tomography and Calderón’s problem</title>
    <url>/2025/03/16/Electrical%20impedance%20tomography%20and%20Calderon%E2%80%99s%20problem/</url>
    <content><![CDATA[<h1 id="electrical-impedance-tomography-and-calderóns-problem">Electrical
impedance tomography and Calderón’s problem</h1>
<h2 id="摘要">摘要</h2>
<p>我们回顾了电阻抗断层成像逆方法的数学发展，该方法通过在介质边界测量电压和电流来确定介质的电特性。在数学文献中，这个问题也被称为Calderón问题，来源于Calderón的开创性贡献[23]
.我们将这篇文章集中在复杂几何光学解决方案的主题上，这些解决方案已经导致了该领域的许多进展。在最后一节中，我们回顾了一些Calderón问题的反例，这些反例由于与隐匿性和不可见性的联系而引起了人们的兴趣。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>考虑到人体器官和组织具有截然不同的电导率，EIT也出现在医学成像中[71]。一个令人兴奋的潜在应用是乳腺癌的早期诊断[149]。乳腺恶性肿瘤的电导率通常为0.2
mho，明显高于正常组织的0.03mho。另一个应用是监测肺功能[62]。关于EIT的其他医学成像应用，参见该书[52]和《生理学测量》[53]。这种反演方法也被用于检测埋地管道的泄漏[70]。</p>
<p>我们现在更精确地描述这个数学问题。</p>
<p>设<span class="math inline">\(\Omega \subseteq
\mathbb{R}^n\)</span>是一个边界光滑的有界区域(我们将描述的许多结果对于具有Lipschitz边界的区域是有效的)。<span class="math inline">\(\Omega\)</span>的电导率用一个有界的正函数<span class="math inline">\(\gamma(x)\)</span>表示.在没有电流汇或电流源的情况下，给出了电势的方程
<span class="math display">\[
\nabla \cdot(\gamma \nabla u)=0 \text { in } \Omega \qquad(1)
\]</span></p>
<p>根据欧姆定律，<span class="math inline">\(\gamma \nabla
u\)</span>表示当前的电流通量。</p>
<p>给定边界上的一个势函数<span class="math inline">\(f \in
H^{\frac{1}{2}}(\partial \Omega)\)</span>，诱导势<span class="math inline">\(u \in
H^1(\Omega)\)</span>是下面Dirichlet问题的解</p>
<p><span class="math display">\[
\begin{aligned}
\nabla \cdot(\gamma \nabla u) &amp; =0 \text { in } \Omega \\
\left.u\right|_{\partial \Omega} &amp; =f .
\end{aligned} \qquad(2)
\]</span></p>
<p>Dirichlet to Neumann映射，或电压到电流映射，由</p>
<p><span class="math display">\[
\Lambda_\gamma(f)=\left.\left(\gamma \frac{\partial u}{\partial
\nu}\right)\right|_{\partial \Omega} \qquad(3)
\]</span></p>
<p>给出，其中<span class="math inline">\(\nu\)</span>表示<span class="math inline">\(\partial \Omega\)</span>的单位外法线。</p>
<p>反问题是已知<span class="math inline">\(\Lambda_\gamma\)</span>确定<span class="math inline">\(\gamma\)</span>。很难找到一种系统的方法来规定边界处的电压测量值，以便能够找到电导率。Calderón转而采取了不同的路线。</p>
<p>利用散度定理，我们有 <span class="math display">\[
Q_\gamma(f):=\int_{\Omega} \gamma|\nabla u|^2 d x=\int_{\partial \Omega}
\Lambda_\gamma(f) f d S \qquad(4)
\]</span></p>
<p>式中<span class="math inline">\(d S\)</span>为表面测度，<span class="math inline">\(u\)</span>为式(2)的解。换言之，<span class="math inline">\(Q_\gamma(f)\)</span>是与线性映射<span class="math inline">\(\Lambda_\gamma(f)\)</span>相联系的二次型，且对任意的<span class="math inline">\(f \in H^{\frac{1}{2}}(\partial
\Omega)\)</span>，知道<span class="math inline">\(\Lambda_\gamma(f)\)</span>或<span class="math inline">\(Q_\gamma(f)\)</span>是等价的。 <span class="math inline">\(Q_\gamma(f)\)</span>衡量了在边界处维持电势<span class="math inline">\(f\)</span>所需的能量。Calderón的观点是，如果人们着眼于<span class="math inline">\(Q_\gamma(f)\)</span>，问题就变成了寻找方程(1)的足够多的解<span class="math inline">\(u \in H^1(\Omega)\)</span>，以便在内部寻找<span class="math inline">\(\gamma\)</span>。我们将在接下来的章节中进一步解释这种方法，其中我们研究了映射的线性化。
<span class="math display">\[
\gamma \xrightarrow{Q} Q_\gamma \qquad(5)
\]</span></p>
<p>这里我们把<span class="math inline">\(Q_\gamma\)</span>看成是与二次型(4)相联系的双线性形式。</p>
<p>第2节介绍Calderón的文章，以及他如何利用复指数证明(5)的线性化在恒定电导率下是单射的。他还给出了一个重建电导率的近似公式，这个电导率先验地接近于一个恒定的电导率。在第3节中，我们给出了Calderón方法在确定孔洞和夹杂物中的应用。在第4节中，我们描述了关于电导率及其法向导数的边界值的唯一性，稳定性和重构的结果。</p>
<p>在第5节中，我们描述了Sylvester和Uhlmann[132]，[131]构造的与有界位势相关的Schrödinger方程的复几何光学解。这些解表现为卡Calderón关于大复频率的复指数解。在第6节中，我们利用这些解证明了在维数<span class="math inline">\(n \geq
3\)</span>时，反问题的全局可辨识性结果、稳定性估计和重构方法。我们还描述了可辨识性结果对非线性电导率的扩展[124]，并给出了复杂几何光学解的其他应用。</p>
<p>在第7节中，我们考虑了部分数据问题，即当DN映射在边界的一部分上测量时的情况.我们描述文献[75]对三维及以上非线性问题的结果。这使用了更大类的CGO解，具有使用Carleman估计构造的非线性相函数。对于部分数据的线性化问题，我们也回顾了文献[29]。</p>
<p>在第8节中，我们考虑二维情况。特别是，我们简要描述了Astala和Päivärinta最近证明有界可测系数唯一性的工作，以及Bukhgeim从与薛定谔方程相关的柯西数据中证明势唯一性的研究。最后，我们描述了Imanuvilov、Uhlmann和Yamamoto在部分数据问题上的工作[59]。</p>
<p>第2~8节讨论了各向同性电导率的情况。在第九节中，我们考虑了各向异性电导率的情况，即电导率也依赖于方向。在两个维度上，对各向异性问题的理解已经有了实质性的进展，因为人们通常可以通过使用等温坐标将问题简化为各向同性的情况。在维度3中，文献[85]指出的问题具有几何性质。我们回顾了[83]，[31]的结果.</p>
<p>在第10节中，我们描述了Calderón问题和从边界点之间的旅行时间恢复介质声速问题之间的联系。最后在第11节中讨论了变换光学和不可见性的思想在静电学中的应用。</p>
<p>我们在引言的最后提到，我们不讨论EIT中的其他一些重要的发展，包括Borcea等人在离散电阻网络上的工作和对连续情况的近似[17]，以及边界未知时的情况[78]。我们主要集中在涉及复杂几何光学解的应用中。关于EIT的其他综述见[16]，[48]，[27]和[140]。</p>
<h2 id="calderón的论文">Calderón的论文</h2>
<p>Calderón在文献[23]中证明了映射<span class="math inline">\(Q\)</span>是解析的。<span class="math inline">\(Q\)</span>在<span class="math inline">\(\gamma=\gamma_0\)</span>处沿<span class="math inline">\(h\)</span>方向的Fréchet导数由下式给出</p>
<p><span class="math display">\[
\left.d Q\right|_{\gamma=\gamma_0}(h)(f, g)=\int_{\Omega} h \nabla u
\cdot \nabla v d x \qquad(6)
\]</span></p>
<p>其中<span class="math inline">\(u, v \in H^1(\Omega)\)</span>
是下面的解</p>
<p><span class="math display">\[
\left\{\begin{array}{l}
\nabla \cdot\left(\gamma_0 \nabla u\right)=\nabla \cdot\left(\gamma_0
\nabla v\right)=0 \text { in } \Omega \\
\left.u\right|_{\partial \Omega}=f \in H^{\frac{1}{2}}(\partial
\Omega),\left.\quad v\right|_{\partial \Omega}=g \in
H^{\frac{1}{2}}(\partial \Omega) .
\end{array}\right. \qquad(7)
\]</span></p>
<p>因此，如果<span class="math inline">\(H^1(\Omega)\)</span>的解<span class="math inline">\(\nabla \cdot\left(\gamma_0 \nabla
u\right)=0\)</span>的乘积在<span class="math inline">\(L^2(\Omega)\)</span>中稠密，则线性化映射是单射的。</p>
<p>Calderón证明了线性化映射在<span class="math inline">\(\gamma_0=\)</span>常数的情况下的单射性，为了简单起见，我们假设它是常值函数1。问题归结为调和函数的梯度积在<span class="math inline">\(L^2(\Omega)\)</span>中是否稠密</p>
<p>Calderón取如下调和函数：</p>
<p><span class="math display">\[
u=e^{x \cdot \rho}, \quad v=e^{-x \cdot \bar{\rho}} \qquad(8)
\]</span></p>
<p>其中 <span class="math inline">\(\rho \in \mathbb{C}^n\)</span>
有</p>
<p><span class="math display">\[
\rho \cdot \rho=0 \qquad(9)
\]</span></p>
<p>我们注意到条件(9)等价于下面</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \rho=\frac{\eta+i k}{2}, \eta, k \in \mathbb{R}^n \\
&amp; |\eta|=|k|, \eta \cdot k=0
\end{aligned} \qquad(10)
\]</span></p>
<p>然后将(8)的解代入(6)中，我们得到如果<span class="math inline">\(\left.d Q\right|_{\gamma_0=1}(h)=0\)</span></p>
<p><span class="math display">\[
|k|^2\left(\chi_{\Omega} h\right)^{\wedge}(k)=0 \quad \forall k \in
\mathbb{R}^n
\]</span></p>
<p>式中<span class="math inline">\(\chi_{\Omega}\)</span>为<span class="math inline">\(\Omega\)</span>的特征函数，<span class="math inline">\({
}^{\wedge}\)</span>为傅里叶变换。然后由Fourier反演公式得出<span class="math inline">\(h=0\)</span>在<span class="math inline">\(\Omega\)</span>上。然而，由于在<span class="math inline">\(Q\)</span>的取值范围内允许使用隐函数定理的条件要么是错误的，要么是未知的，因此不能应用隐函数定理得出<span class="math inline">\(\gamma\)</span>在常数附近可逆的结论。</p>
<p>Calderón还观察到，如果使用解(8)，我们可以找到电导率<span class="math inline">\(\gamma\)</span>的一个近似</p>
<p><span class="math display">\[
\gamma=1+h \qquad(11)
\]</span></p>
<p>并且<span class="math inline">\(h\)</span>在<span class="math inline">\(L^{\infty}\)</span>范数下足够小。</p>
<p>我们给出 <span class="math display">\[
G_\gamma=Q_\gamma\left(\left.e^{x \cdot \rho}\right|_{\partial
\Omega},\left.e^{-x \cdot \bar{\rho}}\right|_{\partial \Omega}\right)
\]</span></p>
<p><span class="math inline">\(\rho \in
\mathbb{C}^n\)</span>如(2.4)所示，现在</p>
<p><span class="math display">\[
\begin{aligned}
G_\gamma &amp; =\int_{\Omega}(1+h) \nabla u \cdot \nabla v d x \\
&amp; +\int_{\Omega} h(\nabla \delta u \cdot \nabla v+\nabla u \cdot
\nabla \delta v) d x \\
&amp; +\int_{\Omega}(1+h) \nabla \delta u \cdot \nabla \delta v d x
\end{aligned} \qquad(12)
\]</span></p>
<p><span class="math inline">\(u, v\)</span>如(8)并且</p>
<p><span class="math display">\[
\begin{aligned}
\nabla \cdot(\gamma \nabla(u+\delta u)) &amp; =\nabla \cdot(\gamma
\nabla(v+\delta v))=0 \text { in } \Omega \\
\left.\delta u\right|_{\partial \Omega} &amp; =\left.\delta
v\right|_{\partial \Omega}=0
\end{aligned} \qquad(13)
\]</span></p>
<p>现在应用于(13)式的标准椭圆估计表明</p>
<p><span class="math display">\[
\|\nabla \delta u\|_{L^2(\Omega)}, \quad\|\nabla \delta
v\|_{L^2(\Omega)} \leq C\|h\|_{L^{\infty}(\Omega)}|k| e^{\frac{1}{2}
r|k|} \qquad(14)
\]</span></p>
<p>对于某个<span class="math inline">\(C&gt;0\)</span>，其中<span class="math inline">\(r\)</span>表示包含<span class="math inline">\(\Omega\)</span>的最小球的半径。</p>
<p>将<span class="math inline">\(u, v\)</span>代入(2.7)中，我们得到</p>
<p><span class="math display">\[
\widehat{\chi \Omega \gamma}(k)=-2
\frac{G_\gamma}{|k|^2}+R(k)=\widehat{F}(k)+R(k) \qquad(15)
\]</span></p>
<p>式中<span class="math inline">\(F\)</span>由<span class="math inline">\(G_\gamma\)</span>决定，因此已知。利用(14)，我们可以证明<span class="math inline">\(R(k)\)</span>满足估计</p>
<p><span class="math display">\[
|R(k)| \leq C\|h\|_{L^{\infty}(\Omega)}^2 e^{r|k|} \qquad(16)
\]</span></p>
<p>也就是说，我们知道<span class="math inline">\(\widehat{\chi \Omega
\gamma}(k)\)</span>直到一个对<span class="math inline">\(k\)</span>足够小的项。更确切地说，令<span class="math inline">\(1&lt;\alpha&lt;2\)</span> .则有 <span class="math display">\[
|k| \leq \frac{2-\alpha}{r} \log \frac{1}{\|h\|_{L^{\infty}}}=: \sigma
\qquad(17)
\]</span></p>
<p>我们有 <span class="math display">\[
|R(k)| \leq C\|h\|_{L^{\infty}(\Omega)}^\alpha \qquad(18)
\]</span></p>
<p>对于某些 <span class="math inline">\(C&gt;0\)</span>.</p>
<p>我们取<span class="math inline">\(\widehat{\eta}\)</span> 是 <span class="math inline">\(C^{\infty}\)</span>的一个截断使得<span class="math inline">\(\widehat{\eta}(0)=1, \operatorname{supp}
\widehat{\eta}(k) \subset\left\{k \in \mathbb{R}^n,|k| \leq
1\right\}\)</span>且<span class="math inline">\(\eta_\sigma(x)=\sigma^n
\eta(\sigma x)\)</span>。然后我们得到</p>
<p><span class="math display">\[
\widehat{\chi \Omega \gamma}(k)
\widehat{\eta}\left(\frac{k}{\sigma}\right)=\frac{-2 G_\gamma
\gamma}{|k|^2} \widehat{\eta}\left(\frac{k}{\sigma}\right)+R(k)
\widehat{\eta}\left(\frac{k}{\sigma}\right) .
\]</span></p>
<p>利用这一点我们得到下面的估计</p>
<p><span class="math display">\[
|l(x)| \leq C\|h\|_{L^{\infty}(\Omega)}^\alpha\left[\log
\frac{1}{\|h\|_{L^{\infty}(\Omega)}}\right]^n \qquad(19)
\]</span></p>
<p>其中<span class="math inline">\(l(x)=\left(\chi_{\Omega} \gamma *
\eta_\sigma\right)(x)-\left(F *
\eta_\sigma\right)(x)\)</span>。式(19)给出了当<span class="math inline">\(h\)</span>足够小时，平滑后的电导率<span class="math inline">\(\chi_{\Omega} \gamma *
\eta_\sigma\)</span>的近似表达式。</p>
<p>这个Calderón的近似估计和它的修正已经在数值上进行了尝试[60]。</p>
<p>使用这些指数解的另一个完全不同的反问题是具有角平均测量的反输运问题[10]。</p>
<p>这个估计使用了低频的谐波指数。在第四节中，我们考虑电导率方程的高(复)频率解
<span class="math display">\[
L_\gamma=\nabla \cdot(\gamma \nabla u)=0
\]</span></p>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>EIT</tag>
      </tags>
  </entry>
  <entry>
    <title>NAS-PINN</title>
    <url>/2025/02/11/NAS-PINN/</url>
    <content><![CDATA[<h1 id="nas-pinn-neural-architecture-search-guided-physics-informed-neural-network-for-solving-pdesnas-pinn神经网络结构搜索引导的物理信息神经网络用于求解偏微分方程">NAS-PINN:
Neural architecture search-guided physics-informed neural network for
solving
PDEs(NAS-PINN：神经网络结构搜索引导的物理信息神经网络，用于求解偏微分方程)</h1>
<h2 id="摘要">摘要</h2>
<p>物理信息神经网络( PINN
)自提出以来一直是求解偏微分方程的主流框架。通过损失函数将物理信息融入到神经网络中，它可以以无监督的方式预测PDEs的解。然而，神经网络结构的设计基本依赖于先验知识和经验，这造成了很大的麻烦和较高的计算开销。因此，<strong>我们提出了一种神经结构搜索引导的方法，即NAS
- PINN，用于自动搜索求解某些PDEs的最佳神经结构</strong>。
通过将搜索空间松弛为连续空间，并利用掩码实现不同形状张量的添加，NAS -
PINN可以通过双层优化进行训练，其中内层循环优化神经网络的权重和偏置，外层循环优化网络结构参数。我们通过包括Poisson，Burgers和Advection方程在内的几个数值实验来验证NAS
-
PINN的能力。总结了求解不同PDE的有效神经网络结构的特点，可用于指导PINN中神经网络的设计。研究发现，更多的隐藏层并不一定意味着更好的性能，有时可能是有害的。
特别是对于Poisson和Advection，在PINNs中更适合采用神经元数目较多的浅层神经网络。研究还表明，对于复杂问题，具有残差连接的神经网络可以提高PINNs的性能。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>神经网络结构搜索( Neural Architecture Search，NAS
)是一种在特定搜索空间中搜索最优神经网络结构的算法。传统的NAS算法通过神经网络模块的排列组合来构建架构，并对这些架构进行训练和测试以确定其性能，然后根据性能排序选择最佳的神经网络架构。这样的离散过程面临着计算效率低和计算成本高的问题。因此，减少计算开销、提高搜索效率一直是NAS的主要研究热点之一。
<strong>本文将NAS融入PINN的框架中，提出了一种神经架构搜索引导的物理信息神经网络(
NAS-PINN)</strong>。我们实现了用少量数据自动搜索求解给定PDE的最佳神经网络结构。掩码用于张量的添加，以帮助搜索每层中不同数量的神经元。通过对一系列PDE的数值实验，验证了所提方法的有效性.通过对数值结果的分析，总结了高效神经网络结构的特点，为PINNs的进一步研究提供了指导。</p>
<h2 id="方法">方法</h2>
<h3 id="pinn的框架">PINN的框架</h3>
<p><img src="/2025/02/11/NAS-PINN/1.jpg"></p>
<h3 id="可微nas">可微NAS</h3>
<p>在传统的NAS算法中，神经网络的层数通常是固定的，并为每一层提供特定的操作选择。这样的配置使得搜索空间不连续，无法通过基于梯度的方法进行优化，极大地限制了算法的收敛速度和效率。
Liu等人[ 29 ]提出了DARTS，并引入了可微NAS的概念。设<span class="math inline">\(O\)</span>是一个由候选操作组成的集合，其中任何一个操作都表示关于输入x的某个函数<span class="math inline">\(o(x)\)</span>.通过对候选操作施加松弛，可以使搜索空间连续：
<span class="math display">\[
\bar{o}^{(i,j)}(x)=\sum_{o\in
O}\frac{exp(\alpha^{(i,j)}_o)}{\textstyle\sum_{o&#39;\in
O}exp(\alpha^{(i,j)_{o&#39;}})}o(x) \qquad (6)
\]</span> 其中<span class="math inline">\(\bar
o^{(i,j)}(x)\)</span>为松弛后第<span class="math inline">\(i\)</span>层与第<span class="math inline">\(j\)</span>层之间的混合运算，<span class="math inline">\(\alpha^{(i,j)}_o\)</span>为运算<span class="math inline">\(o\)</span>的权.现在测试和比较所有可能的操作组合的离散过程可以简化为通过基于梯度的优化方法学习一组合适的权重<span class="math inline">\(\alpha^{(i,j)}_o\)</span>。当算法收敛时，通过选择权重最高的候选操作，可以将松弛的搜索空间提取到离散的神经架构中。</p>
<h3 id="掩码">掩码</h3>
<p>虽然式(6)将搜索空间缩小为一个连续的空间，张量运算只允许相同形状的张量相加，使得搜索神经元个数不切实际，如图2(a)所示。受卷积神经网络中零填充的启发，我们可以将神经元填充到最大数量k，如图2
(b)所示。在图2(c)中，通过将填充的神经元乘以一个零张量掩码，我们将额外的神经元去激活，以模拟不同数量的神经元。最后，通过共享权重，可以将可选的隐藏层减少为一个，输出y可以表示为：
<span class="math display">\[
\mathbf{y} = \sigma(\mathbf{w}\cdot\mathbf{x}+\mathbf{b})\cdot
\begin{pmatrix}
[g_1,g_2,g_3]\times     
\begin{bmatrix}
   \mathbf{mask}_1 \\
   \mathbf{mask}_2 \\
   \mathbf{mask}_3
\end{bmatrix}^T
\end{pmatrix}
\]</span> 其中<span class="math inline">\(\sigma(\cdot)\)</span>为激活函数，<span class="math inline">\(\mathbf{w}\)</span>和<span class="math inline">\(\mathbf{b}\)</span>分别为单隐层的权值和偏置，<span class="math inline">\(g_i\)</span>为标量，为每个神经元个数的权值，<span class="math inline">\(\mathbf{mask}_i\)</span>为每个神经元个数的掩码，形状为<span class="math inline">\(1\times k\)</span>。假设神经元个数为<span class="math inline">\(j\)</span>，则<span class="math inline">\(\mathbf{mask}\)</span>的前<span class="math inline">\(j\)</span>个元素为1，其余<span class="math inline">\((k-j)\)</span>个元素为0。 <img src="/2025/02/11/NAS-PINN/2.jpg" alt="fig.2">
为了确定层数，我们引入身份变换作为操作，即跳过该层，且输出<span class="math inline">\(\mathbf{y}\)</span>变为: <span class="math display">\[
\mathbf{y} = \alpha_1 \cdot
\mathbf{x}+\alpha_2\cdot\sigma(\mathbf{w}\cdot\mathbf{x}+\mathbf{b})\cdot
\begin{pmatrix}
[g_1,g_2,g_3]\times     
\begin{bmatrix}
   \mathbf{mask}_1 \\
   \mathbf{mask}_2 \\
   \mathbf{mask}_3
\end{bmatrix}^T
\end{pmatrix} \qquad (8)
\]</span> 其中，<span class="math inline">\(\alpha_1\)</span>为身份转换权重，表示跳过该层，<span class="math inline">\(\alpha_2\)</span>为保留该层的权重。
式(8)给出了每层输入和输出之间的映射关系，通过反复应用，可以建立一个DNN模型，其中最合适的层可以根据权重<span class="math inline">\(\alpha\)</span>来选择，最合适的每层神经元可以根据权重<span class="math inline">\(g\)</span>来决定。这里，我们将<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(g\)</span>统称为<span class="math inline">\(\boldsymbol{\alpha}\)</span>，<span class="math inline">\(\bf{w}\)</span>和<span class="math inline">\(\bf{b}\)</span>统称为<span class="math inline">\(\boldsymbol{\theta}\)</span>。</p>
<h3 id="mas-pinn">MAS-PINN</h3>
<p>现在我们可以得到NAS -
PINN的整体框架，如图3所示，它可以被认为是一个<strong>双层优化问题</strong>。在内循环中，对DNN的权值和偏置<span class="math inline">\(\boldsymbol{\theta}\)</span>进行优化，而在外循环中，优化目标是寻找最佳的<span class="math inline">\(\boldsymbol{\alpha}\)</span>。其过程可以表示为：
<span class="math display">\[
\underset{\boldsymbol{\alpha}}{min}MSE(\boldsymbol{\theta}^*,\boldsymbol{\alpha})
\\ s.t. \boldsymbol{\theta}^*=
\underset{\boldsymbol{\theta}}{argmin}Loss(\boldsymbol{\theta},\boldsymbol{\alpha})
\]</span> 内环的损失函数可以设计为式( 2 ) ~ ( 5
)（PINN损失函数：数据匹配损失，PDE残差损失，边界条件损失）和外环的损失函数可以写成：
<span class="math display">\[
MSE=\frac{1}{n}\sum^n_{i=1}(\hat u-u)^2  \qquad(10)
\]</span>
其中u是已知的解析解或数值解，n是数据点的个数，对于外循环，所需的n可以很小。这样的双层优化问题可以通过交替优化来解决，相应的过程在算法1中展示。
当训练结束时，可以根据<span class="math inline">\(\boldsymbol{\alpha}\)</span>推导出离散的神经网络模型。基本上，我们可以首先通过比较<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>来决定是否跳过某一层。如果保留该层，我们可以根据<span class="math inline">\(g\)</span>来决定神经元的数量。如果跳过某一层，则无需考察其权重<span class="math inline">\(g\)</span>。 在一些<span class="math inline">\(a_1\)</span>和<span class="math inline">\(a_2\)</span>相对接近的情况下，我们假设跳过层和保留层同样重要，并给出了一个混合模型。在混合模型中，层是身份变换和神经网络操作的组合。神经元的数量决定于一个离散的神经元，因此这些层可以表示为：
<span class="math display">\[
\mathbf{y} = \alpha_1 \cdot
\mathbf{x}+\alpha_2\cdot\sigma(\mathbf{w}\cdot\mathbf{x}+\mathbf{b})\cdot
(g_{max}\times \boldsymbol{mask}_{max})^T
\]</span> 式中,<span class="math inline">\(g_{max}\)</span>为所有权值<span class="math inline">\(g\)</span>中的最大值，<span class="math inline">\(\boldsymbol{mask}_{max}\)</span>为对应于<span class="math inline">\(g_{max}\)</span>的零张量掩码。 <img src="/2025/02/11/NAS-PINN/3.jpg" alt="fig.3"> <img src="/2025/02/11/NAS-PINN/4.jpg" alt="algorithm 1"></p>
<h2 id="实验">实验</h2>
<h3 id="possion-equation">Possion equation</h3>
<p>泊松方程是一类描述电磁场和热场的基本偏微分方程，在电磁学和机械工程中有着广泛的应用。这里，我们考虑一个带有Dirichlet边界条件的二维Poisson方程：
<img src="/2025/02/11/NAS-PINN/5.jpg" alt="possion equation"> 该方程有解析解：
<img src="/2025/02/11/NAS-PINN/6.jpg" alt="analytical solution">
我们首先考虑正方形计算域中的泊松方程，以验证所提出的NAS -
PINN的有效性。我们构造了一个相对较小的搜索空间，它是一个最多包含5个隐藏层的神经网络，每层包含30、50或70个神经元。对离散搜索空间中的每一种可能的神经架构分别进行训练和测试，作为网格搜索的近似实例。然后，我们使用NAS
-
PINN来搜索一个神经结构，并研究它是否是最好的。所有离散搜索空间中的363个架构由Adam训练，其中500个配置点在域内随机采样，100个边界点均匀分布在边界上。
在架构搜索阶段，1000个配置点和200个边界点采用与之前相同的策略进行采样，以搜索最佳的神经架构，并将Adam应用于架构搜索阶段。然后以与363架构相同的方式从头开始训练得到的神经架构。</p>
<p>为了进行更全面的比较，还对传统的Auto
ML方法SMAC进行了测试。SMAC是一个通用的用于超参数优化的贝叶斯优化包，对于所讨论的问题，需要优化的超参数是隐藏层的数量和神经元的数量。对于SMAC，一个相当小的研究空间包括15种不同的神经结构，其中每个隐藏层的神经元数量只能是相同的。SMAC使用与架构搜索阶段相同的1000个配置点和200个边界点。
最后，均匀采样1000000个点，测试所有收敛的神经架构。不同架构的预测解和误差分布如图4所示，L2误差如表1所示。所有实验均重复5次，L2误差由5次重复的平均值得到。
这些结构以序列的形式描述在表1中。序列的第一个和最后一个元素代表输入和输出通道，而其他元素代表每一层的神经元数目。例如，98号架构的输入大小为n×2，其中n为批次大小，2代表坐标x和y，第一层隐含层有70个神经元。通过NAS
- PINN得到的架构为No.358。</p>
<p>从表1和图4中，我们可以清楚地看到NAS-PINN的神经架构具有最小的L2误差和最小的最大误差值，并且其误差分布相比于其他架构也有所改善。因此，所提出的NASPINN确实可以在给定的搜索空间中找到最佳的神经网络结构。此外，虽然No.98也表现出相对较好的性能(在363种可能的体系结构中,它是第二好的体系结构)，但它比NAS-PINN的架构拥有更多的参数，这表明更多的参数并不一定意味着更好的性能，一个适当设计的神经架构显得尤为重要。
此外，在PINNs中，更深的神经网络总是更好的这一常识似乎并不是在所有情况下都是正确的。至少对于给定的泊松方程，浅层但宽的神经网络(隐含层较少,但每层神经元较多的神经网络)优于深层的神经网络。</p>
<p>与SMAC相比，NAS - PINN可以在更大、更灵活的搜索空间中进行搜索，从而NAS
-
PINN更有可能找到真正最佳的神经架构。值得注意的是，虽然SMAC在只有15个神经结构的较小搜索空间中进行搜索，但它需要SMAC
2。08 h找到357号架构，而NAS -
PINN使用1.57h，从363个不同的架构中找到358号架构。所有的数值实验均在Intel(R)Core
i9-9900 K @ 3.60 Ghz /NVIDIA GeForce Rtx 3090 上进行 <img src="/2025/02/11/NAS-PINN/7.jpg" alt="fig.4"> <img src="/2025/02/11/NAS-PINN/8.jpg" alt="table.1"></p>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>神经网络架构</tag>
        <tag>PINN</tag>
      </tags>
  </entry>
  <entry>
    <title>Numerical Solution of Inverse Problems by Weak Adversarial  Networks</title>
    <url>/2025/03/04/Numerical-Solution-of-Inverse-Problems-by-Weak-Adversarial-Networks/</url>
    <content><![CDATA[<h1 id="numerical-solution-of-inverse-problems-by-weak-adversarial-networks弱对抗网络对反问题的数值求解">Numerical
Solution of Inverse Problems by Weak Adversarial
Networks（弱对抗网络对反问题的数值求解）</h1>
<p>期刊：Inverse Problems</p>
<p>时间：2020</p>
<h2 id="摘要">摘要</h2>
<p>本文发展了一种弱对抗网络方法来数值求解一类反问题，包括电阻抗层析成像和动态电阻抗层析成像问题。<strong>利用给定反问题的PDE的弱形式，其中解和测试函数被参数化为深度神经网络。然后，弱形式和边界条件诱导出一个关于网络参数的鞍点函数的极小极大问题</strong>。随着参数的交替更新，网络逐渐逼近反问题的解。对所提算法的收敛性给出了理论证明。
所提出的方法是完全无网格的，无需任何空间离散，特别适用于高维数和解的低正则性问题。对各种测试反问题的数值实验表明了该方法具有较高的精度和效率。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>反问题(Inverse
Problems，IP)普遍存在于大量的科学学科中，包括地球物理[73]、信号处理与成像[9]、计算机视觉[61]、遥感与控制[87]、统计学[53]和机器学习[38]等。设<span class="math inline">\(\Omega\)</span>是<span class="math inline">\(\mathbb{R}^d\)</span>中的开集和有界集，则定义在<span class="math inline">\(\Omega\)</span>上的IP可表示为一般形式： <span class="math display">\[
\begin{aligned}
\mathcal{A}[u, \gamma]=0, &amp; &amp; \text { in } \Omega  \qquad(1a)\\
\mathcal{B}[u, \gamma]=0, &amp; &amp; \text { on } \partial \Omega
\qquad(1b)
\end{aligned}
\]</span> 其中<span class="math inline">\(\mathcal{A}[u,
\gamma]\)</span>指定一个微分方程，<strong><span class="math inline">\(u\)</span>是反介质问题中的解，<span class="math inline">\(\gamma\)</span>是反源问题中的源函数</strong>。方程<span class="math inline">\(\mathcal{A}\)</span>可以是常微分方程(ODE)，也可以是偏微分方程(PDE)，还可以是积分微分方程(IDE)，<span class="math inline">\((u, \gamma)\)</span>在区域<span class="math inline">\(\Omega\)</span>内需要满足(几乎)处处成立。边界值(以及如果适用的初始值)由<span class="math inline">\(\mathcal{B}[u, \gamma]\)</span>在<span class="math inline">\(\partial
\Omega\)</span>上给出。根据具体的应用，<span class="math inline">\(u\)</span>和/或<span class="math inline">\(\gamma\)</span>的部分信息可以在<span class="math inline">\(\Omega\)</span>的内部获得.那么IP
(1)就是找到同时满足(1a)和(1b)的<span class="math inline">\((u,
\gamma)\)</span>。</p>
<p>为了实例化我们的方法，我们主要以电阻抗成像(Electrical Impedance
Tomography，EIT)中经典的电导率逆问题[19、55]为例，介绍了本文的主要思想和推导过程。然而，我们的方法可以很容易地通过修改应用于其他类型的IP。一个动态EIT问题的例子将在第4节中给出。EIT的目标是根据电势<span class="math inline">\(u\)</span>，电流<span class="math inline">\(-\gamma \partial_{\vec{n}}
u\)</span>的测量和区域<span class="math inline">\(\Omega\)</span>的边界<span class="math inline">\(\partial \Omega\)</span>上/附近的<span class="math inline">\(\gamma\)</span> (也即<span class="math inline">\(\partial_{\vec{n}}
u\)</span>)的知识，确定定义在<span class="math inline">\(\Omega\)</span>上的未知介质的电导率分布<span class="math inline">\(\gamma(x)\)</span>：</p>
<p><span class="math display">\[
\begin{aligned}
-\nabla \cdot(\gamma \nabla u)-f=0, &amp; \text { in }
\Omega  \qquad(2a)\\
u-u_b=0, \gamma-\gamma_b=0, \partial_{\vec{n}} u-u_n=0, &amp; \text { on
} \partial \Omega \qquad(2b)
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(u_b\)</span>为测量电压，<span class="math inline">\(\gamma_b\)</span>为物体表面附近的电导率，<span class="math inline">\(u_n \triangleq \nabla u \cdot \vec{n}\)</span>
，其中 <span class="math inline">\(\vec{n}\)</span>为<span class="math inline">\(\partial
\Omega\)</span>的外法线.值得注意的是，<strong>我们的方法并不像EIT问题[23、36、59]的经典方法那样估计与电导率函数相关的Dirichlet
- to - Neumann
(DtN)映射。相反，我们的目标是直接利用给定的数据数值求解一般的一类IPs(1)</strong>，以EIT问题(2)为原型例子，而不利用其特殊结构(例如,
DtN映射)。为了使我们的介绍简洁而有重点，我们只考虑(1a)中具有PDEs特征的<span class="math inline">\(\mathcal{A}[u,
\gamma]\)</span>的IP，并假设给定的IP是定义良好的且至少有一个(弱)解。</p>
<p>我们的方法是<strong>训练能够表示给定IP的解<span class="math inline">\((u,
\gamma)\)</span>的深度神经网络</strong>，与经典的数值方法相比有很大的改进，特别是对于高维问题。更具体地说，<strong>我们利用PDE
(1a)的弱形式，将IP转化为<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>的算子范数最小化问题。然后将<span class="math inline">\(u\)</span>、未知系数<span class="math inline">\(\gamma\)</span>和测试函数<span class="math inline">\(\varphi\)</span>分别参数化为深度神经网络<span class="math inline">\(u_\theta, \gamma_\theta\)</span>, 和 <span class="math inline">\(\varphi_\eta\)</span>，网络参数为<span class="math inline">\((\theta, \eta)\)</span>，并形成参数为<span class="math inline">\((\theta,
\eta)\)</span>的鞍函数的极小极大问题。最后，我们应用随机梯度下降法交替更新网络参数，使得(<span class="math inline">\(u_\theta,
\gamma_\theta\)</span>)逐步逼近IP的解</strong>。利用深度神经网络对<span class="math inline">\((u,
\gamma)\)</span>进行参数化，不需要对空间域和时间域进行离散化，因此是完全无网格的。与经典的有限差分方法(FDM)和有限元法(FEM)相比，这是一种很有前途的替代方法，它们首先在[8]中使用了"维数灾难"这一术语。此外，<strong>我们的方法结合了弱解(原始网络)<span class="math inline">\((u, \gamma)\)</span>和测试函数(对抗网络) <span class="math inline">\(\varphi\)</span>的训练，由PDE的弱形式控制，这对解<span class="math inline">\((u,
\gamma)\)</span>的正则性要求较低，当解具有奇异性时，在许多实际应用中可能更有优势</strong>。</p>
<p>本论文的其余部分组织如下。我们首先在第2节中回顾了基于深度学习的正问题和反问题解决方案的近期工作。在第三节中，我们给出了我们方法的详细推导和一系列理论结果来支持所提出方法的有效性。在第4节中，我们讨论了几种可以提高实际性能的实现技术，并进行了一系列的数值实验来证明所提出方法的有效性。第五部分对本文进行了总结，并给出了一些一般性的注记。</p>
<h2 id="相关工作">相关工作</h2>
<p>在过去的几年中，使用基于深度学习的方法来解决正问题和反问题已经成为一种新的趋势。这些方法大致可以分为两类。第一类包括基于监督学习方法来近似求解给定问题的方法。这些方法通过数值模拟和实验需要大量的输入-输出对来训练所需的网络。在这一类中，深度神经网络用于从测量数据中生成近似的中间结果，用于进一步精化[34、66、69、77、81、88]，应用于改进经典数值方法在后处理阶段的求解[6、41、42、45、51、56、67、84]，或者用于近似从反问题的给定参数到其解的映射，但需要空间离散，无法应用于高维问题[2、50、63]。</p>
<p>第二类是基于问题表述直接求解正问题或反问题的无监督学习方法，而不是额外的训练数据，在实际应用中可能比第一类更有优势。例如，在[24]中，前馈神经网络用于参数化系数函数，并通过最小化性能函数进行训练。在[58]中，提出了一种名为SwitchNet的神经网络架构，通过散射体和散射场之间的映射来解决逆散射问题。文献[32]提出了一种针对2D和3D
EIT问题的深度学习方法，通过紧凑的神经网络架构来表示DtN图。前向问题中PDE对应的倒向随机微分方程(BSDE)是通过神经网络部分参数化，使得对域[13、30、43]中目标点的BSDE积分得到PDE的解。在[31]中，一个正问题的解被参数化为一个深度神经网络，它通过最小化与PDE相关的能量泛函和边值条件上的惩罚项组成的损失函数来训练。在[74]中提出了另一种无网格框架，称为物理信息神经网络(PINN)，用于使用基于PDEs的强大公式的深度神经网络来解决正问题和反问题，其中反问题部分考虑常系数函数。具体来说，PINN使用深度神经网络对给定的PDE的未知量进行参数化，这些网络通过最小化在域和边界条件采样点处违反PDE的最小二乘法形成的损失函数进行训练。PINN的一些实证研究也在[26]中进行。在[52]中也考虑了在问题域中给定数据的基于PINN的IPs的解决方案，并且在[5]中提出了使用自适应采样的配置点来精化解决方案。
在[89]中，PDE的弱形式被用作目标函数，其中PDE的解和测试函数都被参数化为深度神经网络，分别试图最小化和最大化目标函数。在[54]中，使用了类似的变分形式，其中测试函数是固定基，而不是要学习的神经网络。在[68]中，我们使用了三个神经网络，其中一个用于低保真度数据，另外两个用于高保真度数据的线性和非线性函数。具有多保真网络结构的PINN也被提出用于随机PDE情形，其中多项式混沌展开式用于表示解，即作为随机基与待学习系数函数的线性组合[18]。
在[12]中，IP的解被深度神经网络参数化，并通过最小化一个代价函数来学习，该代价函数执行IP和额外的正则化条件，其中PDE的解在训练过程中被要求。</p>
<p>最近，基于元学习的前向问题求解方法也被考虑[18、33、64]。在[33]中，我们利用小波变换的压缩形式来学习从微分算子的系数到伪微分算子(e.g.
,格林函数)的映射。在[64]中，我们引入了一个由分支网络和主干网络组成的深度算子网络。该网络将有限个位置上的输入函数(branch-net)和输出函数的位置(dry-net)进行编码，输出函数由两者的内积加上一个偏置给出。学习网络的宽度和深度参数也在[18]中使用贝叶斯优化来考虑。</p>
<p>我们的IP方法沿用了我们先前针对正问题的工作[89]，这与前面提到的现有方法在使用偏微分方程的弱形式上有所不同。<strong>弱形式是求解偏微分方程的一种强有力的方法，因为它要求更少的正则性，并允许解的必要奇异性，这在成像和异常检测等许多实际应用中都是一个重要的特征</strong>。
从理论的角度来看，<strong>我们的方法对解(作为原始网络)和测试函数(作为对抗网络)进行神经网络参数化，并以一种对抗训练的方式执行，即测试函数在不满足PDE的地方对解网络进行批判训练，解网络在这些地方进行自我修正，直到PDE在域中(几乎)处处被满足</strong>。然而，由于反问题往往是不适定的，且一般情况下比正问题更难求解，因此本文主要关注EIT中的反问题(2)。类似问题的一些实验结果也在第4节中给出。</p>
<p>本工作中的对抗训练与生成对抗网络[39]中使用的对抗训练相似，其中<strong>生成器网络旨在将通用的随机样本(如来自给定的多变量Gaussian的)映射到与训练样本具有相同分布的样本</strong>，而<strong>判别器网络则是将生成器网络产生的样本与真实样本区分开来</strong>。生成器和对抗网络作为零和博弈中的两个参与者，分别通过梯度下降和上升对目标函数进行交替更新，以达到均衡。
特别地，GAN的一个显著变体，称为Wasserstein生成式对抗网络[4]，也具有原始网络(生成器)和对抗网络(由生成分布和样本分布的Wasserstein距离决定的最优运输的对偶函数)的min-max结构作为我们的提法。然而，WGAN要求其在max问题中的对偶函数是1-Lipschitz的，这在数值上是很难实现的，并产生了一系列的后续工作来克服问题[40、70]，谱归一化生成对抗网络。相比之下，<strong>我们工作中的弱解与测试函数的结构自然地产生于PDE理论中的弱公式，它在不对对抗网络(测试函数)施加限制性约束的情况下，为PDE的IP求解提供了大量的理论证明和计算益处</strong>。</p>
<p>与许多现有的深度学习方法需要大量的演示数据(例如,系数/边界值和解对)进行训练不同，我们的方法遵循无监督的学习策略，只需要在给定的IP中制定PDE和边界条件。在[83]中，一项无监督学习研究表明，一般的卷积神经网络(CNN)自动偏向于平滑信号，并且可以在没有任何训练数据的情况下产生类似于图像去噪中一些复杂重建的结果。这种现象被称为深度图像先验(Deep
Image Prior，DIP)，在[29、46]中被进一步利用。
DIP与现有工作最显著的区别在于，我们的方法是完全无网格的，不需要任何空间离散，适用于高维问题。另一方面，在DIP及其后续工作中，重建网络被应用于离散化的2D或3D图像。此外，我们的目标是利用深度网络的表征能力来参数化连续空间中IP的解，而DIP的主要兴趣在于其有趣的自动正则化特性。</p>
<h2 id="针对反问题的弱对抗网络">针对反问题的弱对抗网络</h2>
<p>所提出的IPs弱对抗网络方法受到PDEs弱形式的启发。为了得到(1a)中偏微分方程的弱形式，我们将(1a)两边同时乘以一个任意的测试函数<span class="math inline">\(\varphi \in H_0^1(\Omega)\)</span>(在<span class="math inline">\(\Omega\)</span>中具有有界一阶弱导数和紧支撑的函数的Hilbert空间)，并在<span class="math inline">\(\Omega\)</span>上积分： <span class="math display">\[
\langle\mathcal{A}[u, \gamma], \varphi\rangle:=\int_{\Omega}
\mathcal{A}[u, \gamma](x) \varphi(x) \mathrm{d} x=0 \qquad(3)
\]</span></p>
<p><strong>弱形式(3)的主要优点之一是我们可以通过分部积分将<span class="math inline">\(\mathcal{A}[u,
\gamma]\)</span>中的某些梯度算子转移到<span class="math inline">\(\varphi\)</span>中，从而降低对<span class="math inline">\(u\)</span>(和<span class="math inline">\(\gamma\)</span>，若适用)正则性的要求</strong>。例如，在电导率逆问题(2)中，分部积分和<span class="math inline">\(\partial \Omega\)</span>上<span class="math inline">\(\varphi=0\)</span>的事实一起导致 <span class="math display">\[
\langle\mathcal{A}[u, \gamma], \varphi\rangle=\int_{\Omega}(\gamma
\nabla u \cdot \nabla \varphi-f \varphi) \mathrm{d} x=0 \qquad(4)
\]</span></p>
<p>其中<span class="math inline">\(\gamma \nabla
u\)</span>在经典意义下不一定像式(2)那样可微(在这篇文章中,我们用<span class="math inline">\(\nabla\)</span>表示关于<span class="math inline">\(x\)</span>的梯度算子,<span class="math inline">\(\nabla_\theta\)</span>表示关于<span class="math inline">\(\theta\)</span>的梯度等)。若对所有的<span class="math inline">\(\varphi \in H_0^1(\Omega)\)</span>，<span class="math inline">\((u,
\gamma)\)</span>满足边界条件(1b)和(3)，则称<span class="math inline">\((u, \gamma) \in H^1(\Omega) \times
L^2(\Omega)\)</span>为反问题(1)的弱解(或广义解)。这里<span class="math inline">\(L^2(\Omega)\)</span>是<span class="math inline">\(\Omega\)</span>上平方可积函数的Lebesgue空间，<span class="math inline">\(H^1(\Omega) \subset
L^2(\Omega)\)</span>是一阶弱导数有界的函数的Hilbert空间。注意到(1)式的任何经典(强)解也是弱解。
在这项工作中，我们寻求反问题(1)的弱解，以便我们可能能够提供问题的答案，即使它不存在经典意义上的解。</p>
<p>在文献[89]的基础上，我们考虑了(1)中PDE <span class="math inline">\(\mathcal{A}[u,
\gamma]=0\)</span>的弱形式。为了处理反问题中PDE的未知解<span class="math inline">\(u\)</span>和参数<span class="math inline">\(\gamma\)</span>，我们将<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>都参数化为深度神经网络，并将<span class="math inline">\(\mathcal{A}[u, \gamma]: H_0^1(\Omega) \rightarrow
\mathbb{R}\)</span>看作一个线性泛函，使得<span class="math inline">\(\mathcal{A}[u,
\gamma](\varphi):=\langle\mathcal{A}[u, \gamma],
\varphi\rangle\)</span>，如式(3)所定义。我们定义由<span class="math inline">\(H_1\)</span>范数诱导的<span class="math inline">\(\mathcal{A}[u, \gamma]\)</span>范数为 <span class="math display">\[
\|\mathcal{A}[u, \gamma]\|_{o p}:=\sup _{\varphi \in H_0^1, \varphi \neq
0} \frac{\langle\mathcal{A}[u, \gamma],
\varphi\rangle}{\|\varphi\|_{H^1}} \qquad(5)
\]</span></p>
<p>其中，<span class="math inline">\(\varphi\)</span>的<span class="math inline">\(H^1\)</span>范数由<span class="math inline">\(\|\varphi\|_{H^
1(\Omega)}^2=\int_{\Omega}\left(|\varphi(x)|^2+|\nabla
\varphi(x)|^2\right) \mathrm{d} x\)</span>给出。因此，<span class="math inline">\((u, \gamma)\)</span>是(1)的弱解当且仅当<span class="math inline">\(\partial \Omega\)</span>上<span class="math inline">\(\|\mathcal{A}[u, \gamma]\|_{o p}=0\)</span>，<span class="math inline">\(\mathcal{B}[u, \gamma]=0\)</span>。当<span class="math inline">\(\|\mathcal{A}[u, \gamma]\|_{o p} \geq
0\)</span>时，我们知道方程(1)的一个弱解<span class="math inline">\((u,
\gamma)\)</span>，从而解决了方程(5)的如下观测问题： <span class="math display">\[
\underset{u, \gamma}{\operatorname{minimize}}\|\mathcal{A}[u,
\gamma]\|_{o p}^2=\underset{u, \gamma}{\operatorname{minimize}} \sup
_{\varphi \in H_0^1, \varphi \neq 0} \frac{|\langle\mathcal{A}[u,
\gamma], \varphi\rangle|^2}{\|\varphi\|_{H^1}^2} \qquad(6)
\]</span></p>
<p>其中，<span class="math inline">\((u, \gamma) \in H^1(\Omega) \times
L^2(\Omega)\)</span>，且取得最小值0。这一结果在下面的定理中进行了总结，并在附录A.1中给出了证明。</p>
<p><strong>定理1</strong> 假设<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>满足边界条件<span class="math inline">\(\mathcal{B}\left[u^*,
\gamma^*\right]=0\)</span>，则<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>是(1)式的弱解当且仅当<span class="math inline">\(\left\|\mathcal{A}\left[u^*,
\gamma^*\right]\right\|_{o p}=0\)</span></p>
<p><strong>定理1意味着，由于算子范数的非负性，为了找到(1)的弱解，我们可以通过求取最小的算子范数值<span class="math inline">\(\left\|\mathcal{A}\left[u^*,
\gamma^*\right]\right\|_{o p}=0\)</span>来寻找满足<span class="math inline">\(\mathcal{B}\left[u^*,\gamma^*\right]=0\)</span>且同时最小化(6)的最优解<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>。也就是说，<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>是问题(1)的弱解当且仅当<span class="math inline">\(\left\|\mathcal{A}\left[u^*,
\gamma^*\right]\right\|_{o p}\)</span>和<span class="math inline">\(\left\|\mathcal{B}\left[u^*,
\gamma^*\right]\right\|_{L^2(\partial
\Omega)}\)</span>都消失。因此，我们可以从下面的最小化问题中求解<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>，等价于(1)</strong> <span class="math display">\[
\underset{u, \gamma}{\operatorname{minimize}} I(u,
\gamma)=\|\mathcal{A}[u, \gamma]\|_{o p}^2+\beta\|\mathcal{B}[u,
\gamma]\|_{L^2(\partial \Omega)}^2 \qquad(7)
\]</span></p>
<p>并且<span class="math inline">\(\beta
&gt;0\)</span>是平衡目标函数<span class="math inline">\(I(u,\gamma)\)</span>中两项的权重参数。注意到(7)式中目标函数的两项均为非负，且仅在(1)式的一个弱解<span class="math inline">\(\left(u^*,
\gamma^*\right)\)</span>处同时消失。</p>
<p>对于高维PDEs的经典数值方法，一个很有前途的替代方法是使用深度神经网络，因为它们不需要区域离散，并且是完全无网格的。深度神经网络是多个简单函数(称为层)的组合，因此它们可以逼近相当复杂的函数。考虑一个简单的多层神经网络<span class="math inline">\(u_\theta\)</span>如下： <span class="math display">\[
u_\theta(x)=w_K^{\top} l_{K-1} \circ \cdots \circ l_0(x)+b_K \qquad(8)
\]</span></p>
<p>其中，第<span class="math inline">\(k\)</span>层<span class="math inline">\(l_k: \mathbb{R}^{d_k} \rightarrow
\mathbb{R}^{d_{k+1}}\)</span>由<span class="math inline">\(l_k(z)=\sigma_k\left(W_k
z+b_k\right)\)</span>给出，权重<span class="math inline">\(W_k \in
\mathbb{R}^{d_{k+1} \times d_k}\)</span>，偏置<span class="math inline">\(b_k \in
\mathbb{R}^{d_{k+1}}\)</span>，所有层的网络参数用<span class="math inline">\(\theta\)</span>统一表示如下： <span class="math display">\[
\theta:=\left(w_K, b_K, W_{K-1}, b_{K-1}, \ldots, W_0, b_0\right)
\qquad(9)
\]</span></p>
<p>综上，本文所有向量默认为列向量。在(8)中，<span class="math inline">\(x \in \Omega\)</span>是网络的输入，<span class="math inline">\(d_0=d\)</span>是(1)的问题维数(也称为输入层的大小)，<span class="math inline">\(w_K \in \mathbb{R}^{d_K}\)</span>和<span class="math inline">\(b_K \in \mathbb{R}\)</span>是最后第<span class="math inline">\(K\)</span>层(也称为输出层)中的参数.非线性激活函数<span class="math inline">\(\sigma_k\)</span>的典型选择包括sigmoid函数<span class="math inline">\(\sigma(z)=\left(1+e^{-z}\right)^{-1}\)</span>、双曲正切(tanh)函数<span class="math inline">\(\sigma(z)=\left(e^z-e^{-z}\right)
/\left(e^z+e^{-z}\right)\)</span>和修正线性单元(ReLU)函数<span class="math inline">\(\sigma(z)=\max (0,
z)\)</span>，它们分别被应用。深度神经网络的训练是指利用可用的数据或约束优化<span class="math inline">\(\theta\)</span>的过程，使得函数<span class="math inline">\(u_\theta\)</span>可以逼近(未知)目标函数。关于深度神经网络的更多细节可参见文献[38]。</p>
<p>尽管有如式(8)的简单结构，但深度神经网络能够在紧支撑<span class="math inline">\(\bar{\Omega}\)</span>上均匀地逼近相当复杂的连续函数(以及必要时的导数)。这个重要的结果被称为万能逼近定理[47]
。由万能逼近定理保证的神经网络的表达能力表明(1)的弱解<span class="math inline">\((u,
\gamma)\)</span>)的无网格参数化是有希望的。接下来，我们对<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>都选取足够深度的神经网络结构，如式(8)所示。数值实验中使用的具体结构，即层数<span class="math inline">\(K\)</span>和尺寸<span class="math inline">\(\left\{d_1, \ldots,
d_{K-1}\right\}\)</span>将在第4节中给出。注意到<span class="math inline">\(u\)</span>和<span class="math inline">\(\gamma\)</span>是两个独立的网络，但我们用一个字母<span class="math inline">\(\theta\)</span>来表示它们的网络参数，而不是用<span class="math inline">\(\theta_u\)</span>和<span class="math inline">\(\theta_\gamma\)</span>来简化符号。<strong>也就是说，我们将<span class="math inline">\((u, \gamma)\)</span>参数化为深度神经网络<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>，并试图找到参数<span class="math inline">\(\theta\)</span>使得<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>解决(7)</strong>。为此，<strong>弱式(3)中的测试函数<span class="math inline">\(\varphi\)</span>也被参数化为深度神经网络<span class="math inline">\(\varphi_\eta\)</span>，其形式类似于(8)和(9)，参数用<span class="math inline">\(\eta\)</span>表示</strong>。通过参数化的<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>和<span class="math inline">\(\varphi_\eta\)</span>，我们遵循(3)中的内积记法并定义
<span class="math display">\[
E(\theta, \eta):=\left|\left\langle\mathcal{A}\left[u_\theta,
\gamma_\theta\right], \varphi_\eta\right\rangle\right|^2 \qquad(10)
\]</span></p>
<p>不像(平方)算子范数(5)的原始定义那样用<span class="math inline">\(\left\|\varphi_\eta\right\|_{H_1}^2\)</span>对
<span class="math inline">\(E(\theta,
\eta)\)</span>进行正规化，而是<strong>将(5)中的平方算子范数近似(上升到一个恒定的标度)为如下<span class="math inline">\(\theta\)</span>的极大值函数</strong>： <span class="math display">\[
L_{\mathrm{int}}(\theta):=\max _{|\eta|^2 \leq 2 B} E(\theta, \eta)
\qquad(11)
\]</span></p>
<p>其中<span class="math inline">\(B&gt;0\)</span>是一个限定网络参数<span class="math inline">\(\eta\)</span>大小的上界。这里<span class="math inline">\(|\eta|^2=\sum_k\left(\sum_{i j}\left[W_k\right]_{i
j}^2+\sum_i\left[b_k\right]_i^2\right),[M]_{i j} \in
\mathbb{R}\)</span>表示矩阵<span class="math inline">\(M\)</span>的<span class="math inline">\((i, j)\)</span>项， <span class="math inline">\([v]_i \in \mathbb{R}\)</span>表示向量<span class="math inline">\(v\)</span>的第<span class="math inline">\(i\)</span>个分量。值得注意的是，式(11)中对<span class="math inline">\(\eta\)</span>的<span class="math inline">\(\ell_2\)</span>-范数的界约束与WGAN
[4]中使用的权重裁剪(等价于关于<span class="math inline">\(\ell_{\infty}\)</span>-范数的界)方法类似。然而，它们服务于不同的目的：在(11)中引入约束，使得积分(如(4))是有界的(这个界的实际值可以是任意的)。在这种情况下，我们的数值实现中的蒙特卡洛近似得到的随机梯度具有有界方差，这在下面定理4的证明中是需要的。另一方面，WGAN中的权重裁剪是为了保证神经网络实现的对偶函数是1-Lipschitz函数类<span class="math inline">\(\mathcal{F}:=\{f: \Omega \rightarrow
\mathbb{R}:|f(x)-f(y)| \leq\)</span> <span class="math inline">\(|x-y|,
\forall x, y \in
\Omega\}\)</span>。正如文献[4]所指出的那样，权重裁剪是实现1-Lipschitz约束的一种简单但不合适的方法，因此有一系列的后续工作来解决这个问题，如[40、70]。</p>
<p>进一步，我们定义与边界条件(1b)相关的损失函数为 <span class="math display">\[
L_{\mathrm{bdry}}(\theta):=\left\|\mathcal{B}\left[u_\theta,
\gamma_\theta\right]\right\|_{L^2(\partial \Omega)}^2=\int_{\partial
\Omega}\left|\mathcal{B}\left[u_\theta, \gamma_\theta\right](x)\right|^2
\mathrm{~d} S(x) \qquad(12)
\]</span></p>
<p>例如，如果在(2b)中给出<span class="math inline">\((u,
\gamma)\)</span>的边界条件，且已知边界值<span class="math inline">\(\left(u_b, \gamma_b, u_n\right)\)</span>，则<span class="math inline">\(L_{\text {bdry }}(\theta)=\int_{\partial
\Omega}\left|u_\theta(x)-u_b(x)\right|^2+\left|\gamma_\theta(x)-\gamma_b(x)\right|^2+\left|\partial_{\vec{n}(x)}
u(x)-u_n(x)\right|^2 \mathrm{~d}
S(x)\)</span>。最后，定义总损失函数<span class="math inline">\(L(\theta)\)</span>，并求解如下关于其最优<span class="math inline">\(\theta^*\)</span>的最小化问题： <span class="math display">\[
\underset{\theta}{\operatorname{minimize}} L(\theta), \quad \text {
where } \quad L(\theta):=L_{\mathrm{int}}(\theta)+\beta
L_{\mathrm{bdry}}(\theta) \qquad(13)
\]</span></p>
<p>其中，我们还限制了参数<span class="math inline">\(\theta\)</span>的大小，使得对于同一个<span class="math inline">\(B\)</span>，<span class="math inline">\(|\theta|^2
\leq 2 B\)</span>，以简化记号。注意到这里<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\eta\)</span>都是有限维向量，且<span class="math inline">\(L_{\text {int }}(\theta), L_{\text {bdry
}}(\theta), E(\theta, \eta) \in
\mathbb{R}_{+}\)</span>，因此可以应用数值优化算法来寻找 <span class="math inline">\(L(\theta)\)</span>的最小值。</p>
<p>求解类似于(13)的极小化问题的标准方法是<strong>投影梯度下降法</strong>，该方法执行如下迭代：
<span class="math display">\[
\theta \leftarrow \Pi\left(\theta-\tau \nabla_\theta L(\theta)\right)
\qquad(14)
\]</span></p>
<p>其中<span class="math inline">\(\Pi(\theta)=\min (\sqrt{2
B},|\theta|) \cdot(\theta /|\theta|)\)</span>是<span class="math inline">\(\theta\)</span>到以原点为圆心，半径为<span class="math inline">\(\sqrt{2 B}\)</span>的球的投影，<span class="math inline">\(\tau&gt;0\)</span>是步长.可以看出，(14)式的主要计算是在梯度<span class="math inline">\(\nabla_\theta L(\theta)=\nabla_\theta
L_{\mathrm{int}}(\theta)+\beta \nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>上进行的。<span class="math inline">\(\nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>的计算简单明了，如下所示。然而，损失<span class="math inline">\(L_{\text {int
}}(\theta)\)</span>被定义为一个最大化问题(11)，我们需要先将其梯度写成关于<span class="math inline">\(\theta\)</span>的函数。为此，我们有下面的引理来计算梯度<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)\)</span>，证明见附录A.2。</p>
<p><strong>引理2.</strong> 假设<span class="math inline">\(L_{i n
t}(\theta)\)</span>在(11)式中定义。则任意<span class="math inline">\(\theta\)</span>处的梯度<span class="math inline">\(\nabla_\theta L_{i n t}(\theta)\)</span>由<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)=\partial_\theta E(\theta,
\eta(\theta))\)</span>给出，其中<span class="math inline">\(\eta(\theta)\)</span>为对指定的<span class="math inline">\(\theta\)</span>求<span class="math inline">\(\max
_{|\eta|^2 \leq 2 B} E(\theta, \eta)\)</span>的解。</p>
<p><strong>注释</strong> 引理2表明，对任意给定的<span class="math inline">\(\theta\)</span>，若要得到<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)\)</span>，我们可以先对<span class="math inline">\(\eta\)</span>取<span class="math inline">\(E\)</span>关于<span class="math inline">\(\theta\)</span>的偏导数，然后利用<span class="math inline">\(\theta\)</span>和最大化问题(11)的解<span class="math inline">\(\eta(\theta)\)</span>求偏导数</p>
<p><span class="math inline">\(L_{\mathrm{int}}(\theta)\)</span>和<span class="math inline">\(L_{\mathrm{bdry}}(\theta)\)</span>的精确梯度需要在连续空间<span class="math inline">\(\Omega\)</span>和<span class="math inline">\(\partial
\Omega\)</span>上对深度神经网络参数化的函数进行积分，这在实际中是计算难以解决的。因此，我们采用这些积分的蒙特卡罗分析(MC)近似。为此，我们需要下面关于利用样本逼近积分的结果，其证明见附录A.3.</p>
<p><strong>引理3</strong> 假设<span class="math inline">\(\Omega \subset
\mathbb{R}^d\)</span>是有界的，<span class="math inline">\(\rho\)</span>是定义在<span class="math inline">\(\Omega\)</span>上的概率密度，使得对所有<span class="math inline">\(x \in \Omega\)</span>，<span class="math inline">\(\rho(x)&gt;0\)</span>。给定函数<span class="math inline">\(\psi \in L^2(\Omega)\)</span>，记<span class="math inline">\(\Psi=\int_{\Omega} \psi(x) \mathrm{d}
x\)</span>。设<span class="math inline">\(x^{(1)}, \ldots,
x^{(N)}\)</span>是从<span class="math inline">\(\rho\)</span>中抽取的<span class="math inline">\(N\)</span>个独立样本。考虑<span class="math inline">\(\Psi\)</span>的如下估计量<span class="math inline">\(\hat{\Psi}\)</span>：</p>
<p><span class="math display">\[
\hat{\Psi}=\frac{1}{N} \sum_{i=1}^N
\frac{\psi\left(x^{(i)}\right)}{\rho\left(x^{(i)}\right)} \qquad(15)
\]</span></p>
<p>则<span class="math inline">\(\hat{\Psi}\)</span>的一阶矩和二阶矩由下式给出.</p>
<p><span class="math display">\[
\mathbb{E}[\hat{\Psi}]=\Psi \quad \text { and } \quad
\mathbb{E}\left[\hat{\Psi}^2\right]=\frac{N-1}{N} \Psi^2+\frac{1}{N}
\int_{\Omega} \frac{\psi(x)^2}{\rho(x)} \mathrm{d} x \qquad(16)
\]</span></p>
<p>因此，<span class="math inline">\(\hat{\Psi}\)</span>的方差为<span class="math inline">\(N^{-1} \cdot\left(\int_{\Omega}\left(\psi^2 /
\rho\right) \mathrm{d} x-\left(\int_{\Omega} \psi \mathrm{d}
x\right)^2\right)\)</span>。特别地，当均匀分布<span class="math inline">\(\rho(x)=1 /|\Omega|\)</span>时，<span class="math inline">\(\hat{\Psi}=(|\Omega| / N) \cdot \sum_i
\psi\left(x^{(i)}\right)\)</span>的方差为<span class="math inline">\(N^{-1} \cdot\left(|\Omega| \int_{\Omega} \psi^2
\mathrm{~d} x-\left(\int_{\Omega} \psi \mathrm{d}
x\right)^2\right)\)</span>。</p>
<p><strong>注释</strong> 关于引理3，有几点注记：</p>
<ul>
<li><p>积分<span class="math inline">\(\Psi\)</span>的估计量<span class="math inline">\(\hat{\Psi}\)</span>是无偏的</p></li>
<li><p>上述<span class="math inline">\(\hat{\Psi}\)</span>的方差在样本配点数<span class="math inline">\(N\)</span>中以<span class="math inline">\(O(1 /
N)\)</span>的速率递减。由Hölder不等式和<span class="math inline">\(\rho\)</span>是一个概率密度，我们知道.</p></li>
</ul>
<p><span class="math display">\[
\left|\int_{\Omega} \psi \mathrm{d} x\right| \leq \int_{\Omega}|\psi|
\mathrm{d} x=\int_{\Omega} \frac{|\psi|}{\sqrt{\rho}} \sqrt{\rho}
\mathrm{d} x \leq\left(\int_{\Omega} \frac{|\psi|^2}{\rho} \mathrm{~d}
x\right)^{1 / 2}\left(\int_{\Omega} \rho \mathrm{d} x\right)^{1 /
2}=\left(\int_{\Omega} \frac{|\psi|^2}{\rho} \mathrm{~d} x\right)^{1 /
2}
\]</span></p>
<p>这也验证了<span class="math inline">\(\mathrm{V}(\hat{\Psi}) \geq
0\)</span>。更重要的是，当<span class="math inline">\(\psi\)</span>不变号且<span class="math inline">\(\rho
\propto|\psi|\)</span>时，等式成立。因此，我们可以设置<span class="math inline">\(\rho\)</span>尽可能地接近<span class="math inline">\(|\psi|\)</span>
(直到一个归一化常数)，以减小方差，但同时保证<span class="math inline">\(\rho\)</span>易于从(15)中采样和评估。这与重要性抽样的概念密切相关。</p>
<ul>
<li>引理3中的结果(15)和(16)可以很容易地推广到无界区域<span class="math inline">\(\Omega\)</span>的情形，只要<span class="math inline">\(\psi / \sqrt{\rho} \in L^2(\Omega)\)</span></li>
</ul>
<p><strong>引理3为式(14)提供了一种可行的逼近<span class="math inline">\(L(\theta)\)</span>梯度的方法</strong>。例如，要计算<span class="math inline">\(\nabla_\theta L_{\text {bdry
}}(\theta)\)</span>，可以取式(12)关于<span class="math inline">\(\theta\)</span>的梯度，在边界<span class="math inline">\(\partial \Omega\)</span>上采样<span class="math inline">\(N_b\)</span>个配置点<span class="math inline">\(\left\{x_b^{(i)}: 1 \leq i \leq
N_b\right\}\)</span> ，通过对采样点处的函数值求和近似<span class="math inline">\(\nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>。如果我们取<span class="math inline">\(\mathcal{B}[u, \gamma]=\left(u-u_b,
\gamma-\gamma_b, \partial_{\vec{n}} u-u_n\right)\)</span>，且<span class="math inline">\(x_b^{(i)}\)</span>是一致样本，则估计变为</p>
<p><span class="math display">\[
\begin{gathered}
\nabla_\theta L_{\mathrm{bdry}}(\theta)=2 \int_{\partial
\Omega}\left(\left(u_\theta-u_b\right) \nabla_\theta
u_\theta+\left(\gamma_\theta-\gamma_b\right) \nabla_\theta
\gamma_\theta+\left(\partial_{\vec{n}} u_\theta-u_n\right) \nabla_\theta
\nabla u \cdot \vec{n}\right) \mathrm{d} S(x) \\
\approx \frac{2|\partial \Omega|}{N_b}
\sum_{i=1}^{N_b}\left(\left(u_\theta\left(x_b^{(i)}\right)-u_b\left(x_b^{(i)}\right)\right)
\nabla_\theta
u_\theta\left(x_b^{(i)}\right)+\left(\gamma_\theta\left(x_b^{(i)}\right)-\gamma_b\left(x_b^{(i)}\right)\right)
\nabla_\theta \gamma_\theta\left(x_b^{(i)}\right)\right. \\
\left.+\left(\partial_{\vec{n}}
u_\theta\left(x_b^{(i)}\right)-u_b\left(x_b^{(i)}\right)\right)
\nabla_\theta \nabla u_\theta x_b^{(i)} \cdot \vec{n} x_b^{(i)}\right)
\end{gathered} \qquad(17)
\]</span></p>
<p>类似地，我们也可以计算出<span class="math inline">\(\nabla_\theta
L_{\text {int }}(\theta)\)</span>的随机梯度。若给定<span class="math inline">\(f\)</span>，在区域<span class="math inline">\(\Omega\)</span>上取<span class="math inline">\(\mathcal{A}[u, \gamma]=\nabla \cdot(\gamma \nabla
u)-f\)</span>，且在区域<span class="math inline">\(\Omega\)</span>内均匀采样<span class="math inline">\(N_r\)</span>个配点<span class="math inline">\(\left\{x_r^{(i)}: 1 \leq i \leq
N_r\right\}\)</span> 在区域<span class="math inline">\(\Omega\)</span>内，则<span class="math inline">\(\nabla_\theta L_{\text {int
}}(\theta)\)</span>可由下式估计</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_\theta L_{\mathrm{int}}(\theta) &amp; =2 I(\theta)
\int_{\Omega}\left(\nabla_\theta \gamma_\theta\left(\nabla u_\theta
\cdot \nabla
\varphi_{\eta(\theta)}\right)+\gamma_\theta\left(\nabla_\theta \nabla
u_\theta \cdot \nabla \varphi_{\eta(\theta)}\right)\right) \mathrm{d}
S(x) \\
&amp; \approx \frac{2|\Omega| \hat{I}(\theta)}{N_r}
\sum_{i=1}^{N_r}\left(\nabla_\theta
\gamma_\theta\left(x_r^{(i)}\right)\left(\nabla
u_\theta\left(x_r^{(i)}\right) \nabla
\varphi_{\eta(\theta)}\left(x_r^{(i)}\right)\right)+\gamma_\theta\left(x_r^{(i)}\right)\left(\nabla_\theta
\nabla u_\theta\left(x_r^{(i)}\right) \nabla
\varphi_{\eta(\theta)}\left(x_r^{(i)}\right)\right)\right)
\end{aligned}  \qquad(18)
\]</span></p>
<p>其中<span class="math inline">\(I(\theta)\)</span>及其估计量<span class="math inline">\(\hat{I}(\theta)\)</span>由下式给出</p>
<p><span class="math display">\[
I(\theta)=\int_{\Omega} \gamma_\theta\left(\nabla u_\theta \cdot \nabla
\varphi_{\eta(\theta)}\right) \mathrm{d} x, \quad
\hat{I}(\theta)=\frac{|\Omega|}{2} \sum_{i=1}^{N_r}
\gamma_\theta\left(x_r^{(i)}\right)\left(\nabla
u_\theta\left(x_r^{(i)}\right) \cdot \nabla
\varphi_{\eta(\theta)}\left(x_r^{(i)}\right)\right)
\]</span></p>
<p>并且根据引理2，<span class="math inline">\(\eta(\theta)\)</span>是最大化问题(11)的一个解。梯度中的所有积分都可以用类似的方式进行近似。这些近似梯度实际上是随机梯度，它是无偏的，并且由于网络参数的有界性而具有有界的方差。通过这些近似，式(14)退化为随机投影梯度下降法，通过选择合适的步长，可以保证收敛到式(13)的局部稳定点。由于(13)是有约束的，梯度映射<span class="math inline">\(\mathcal{G}(\theta):=\tau^{-1}\left[\theta-\Pi\left(\theta-\tau
\nabla_\theta L(\theta)\right)\right]\)</span>被用作<span class="math inline">\(\theta\)</span>的收敛准则[37,62,75]。值得注意的是，梯度映射的定义考虑了步长<span class="math inline">\(\tau\)</span>的归一化。 此外，在没有投影<span class="math inline">\(\Pi\)</span>的情况下，梯度映射退化为<span class="math inline">\(\mathcal{G}(\theta)=\nabla_\theta
L(\theta)\)</span>，其大小是无约束情况下局部稳定点(即, <span class="math inline">\(\left|\nabla_\theta
L(\theta)\right|=0\)</span>)的评价标准.这一结果在下面的定理中给出，并在附录A.4.中给出证明。</p>
<p><span class="math display">\[
\begin{array}{l}
   \hline \
   \textbf{算法1} \quad \text{Inverse Problem Solver by Weak Adversarial
Network(IWAN)} \\
   \hline
   \qquad \textbf{输入：}反问题(1)的区域\Omega和数据 \\
   \qquad \textbf{初始化：}\left(u_\theta, \gamma_\theta\right),
\varphi_\eta \\
   \qquad \textbf{for} \, j=1, \ldots, J \,\textbf{do} \\
   \qquad \quad \text { Sample } X_r=\left\{x_r^{(i)}: 1 \leq i \leq
N_r\right\} \subset \Omega \text { and } X_b=\left\{x_b^{(i)}: 1 \leq i
\leq N_b\right\} \subset \partial \Omega \text {. }\\
   \qquad \quad \eta \leftarrow \operatorname{SGD}\left(-\nabla_\eta
E(\theta, \eta), X_r, \eta, \tau_\eta, J_\eta\right) \text {. }\\
   \qquad \quad \theta \leftarrow
\operatorname{SGD}\left(\partial_\theta E(\theta, \eta)+\beta
\nabla_\theta L_{\mathrm{bdry}}(\theta),\left(X_r, X_b\right), \theta,
\tau_\theta, 1\right)\\
   \qquad \textbf{end for} \\
   \qquad \textbf{输出：}\left(u_\theta, \gamma_\theta\right) \\
   \hline
\end{array}
\]</span></p>
<p><strong>定理4</strong> 对于任意的<span class="math inline">\(\varepsilon&gt;0\)</span>，令<span class="math inline">\(\left\{\theta_j\right\}\)</span>是由梯度下降算法(14)生成的<span class="math inline">\(\left(u_\theta,
\gamma_\theta\right)\)</span>中的网络参数序列，在每次迭代中用样本复杂度为<span class="math inline">\(N_r,
N_b=O\left(\varepsilon^{-1}\right)\)</span>的样本均值逼近<span class="math inline">\(\nabla_\theta L(\theta)\)</span>中的积分，则<span class="math inline">\(J=O\left(\varepsilon^{-1}\right)\)</span>次迭代后<span class="math inline">\(\min _{1 \leq j \leq J}
\mathbb{E}\left[\left|\mathcal{G}\left(\theta_j\right)\right|^2\right]
\leq \varepsilon\)</span></p>
<p><strong>注释</strong> 定理4建立了(14)到问题的所谓<span class="math inline">\(\varepsilon\)</span>-解的收敛性和迭代复杂性.该结果是基于梯度映射的期望大小，这是非凸约束随机优化中的一个标准收敛准则。然而，这只能保证在期望意义下逼近一个稳定点(不一定是局部的或全局的极小点)。在理论上，我们可以将额外的全局优化技术应用到(7)中，以便找到一个具有较高计算成本的全局最优解(最好只有高概率才有可能)。然而，我们在本工作中不会对这一问题做进一步的探讨。</p>
<p>现在我们总结了我们的算法使用弱对抗网络求解IPs的步骤。为了简化表述，我们引入如下符号来表示寻找损失函数<span class="math inline">\(L(\theta)\)</span>的极小点的随机梯度下降(SGD)过程：</p>
<p><span class="math display">\[
\theta^* \leftarrow \operatorname{SGD}\left(G(\theta), X, \theta_0,
\tau, J\right) \qquad(19)
\]</span></p>
<p>也就是说，意味着输出<span class="math inline">\(\theta^*\)</span>是对<span class="math inline">\(j=0, \ldots, J-1\)</span>执行步长为<span class="math inline">\(\tau\)</span>的(投影) SGD方案，有初始<span class="math inline">\(\theta_0\)</span>之后的结果<span class="math inline">\(\theta_J\)</span>:</p>
<p><span class="math display">\[
\theta_{j+1} \leftarrow \Pi\left(\theta_j-\tau \hat{G}\left(\theta_j ;
X\right)\right) \qquad(20)
\]</span></p>
<p>这里<span class="math inline">\(X=\left\{x^{(i)}: 1 \leq i \leq
N\right\}\)</span>是<span class="math inline">\(N\)</span>个采样配置点的集合，<span class="math inline">\(G(\theta):=\nabla_\theta
L(\theta)\)</span>是损失函数<span class="math inline">\(L(\theta)\)</span>的梯度，<span class="math inline">\(\hat{G}(\theta ; X)\)</span>表示<span class="math inline">\(G(\theta)\)</span>在任意给定的<span class="math inline">\(\theta\)</span>下的随机逼近，其中积分的估计如(15)中利用采样配置点<span class="math inline">\(X\)</span>。在第一步中，我们固定<span class="math inline">\(\theta\)</span>，通过对<span class="math inline">\(J_\eta\)</span>步施加随机梯度上升来求解目标函数<span class="math inline">\(E(\theta,
\eta)\)</span>在(11)中定义的最大化问题，从而得到一个近似的最大化子<span class="math inline">\(\eta\)</span>；在第二步中，我们固定这个<span class="math inline">\(\eta\)</span>，利用梯度<span class="math inline">\(\nabla_\theta L(\theta)=\partial_\theta E(\theta,
\eta)+\beta \nabla_\theta
L_{\mathrm{bdry}}(\theta)\)</span>通过一个随机梯度下降步来更新<span class="math inline">\(\theta\)</span>
。然后进入步骤1，开始下一次迭代。因此，我们的目标函数为<span class="math inline">\(E(\theta, \eta)+\beta
L_{\mathrm{bdry}}(\theta)\)</span>，我们通过<span class="math inline">\(\min _\theta \max _\eta E(\theta, \eta)+\beta
L_{\mathrm{bdry}}(\theta)\)</span>的min-max优化来寻找最优点<span class="math inline">\(\left(\theta^*,
\eta^*\right)\)</span>。这个过程被称为使用弱对抗网络(IWAN)的反问题求解器，并在算法1中总结。数值实现中的参数值在第4节中给出</p>
<h2 id="数值实验">数值实验</h2>
<h3 id="实施细则">实施细则</h3>
<p>在这一部分中，我们讨论了关于算法1的一些实现细节和修改。首先，为了避免在固定<span class="math inline">\(\theta\)</span>的情况下求解式(11)中的内部最大化问题<span class="math inline">\(\max _\eta E(\theta,
\eta)\)</span>花费过多的时间，我们只使用少量的迭代次数<span class="math inline">\(J_\eta\)</span>来计算 <span class="math inline">\(\eta\)</span>。然后我们切换到更新<span class="math inline">\(\theta\)</span>进行一次迭代。见算法1中的两个SGD步骤。这样可以提高整体效率，避免在<span class="math inline">\(\eta\)</span>的内部最大化问题上花费过多的时间，特别是当<span class="math inline">\(\theta\)</span>还远未达到最优时。事实上，我们可以采用两个单独的测试函数<span class="math inline">\(\varphi_\eta\)</span>和<span class="math inline">\(\bar{\varphi}_\eta\)</span>
(为了记号的简洁性,我们又用同样的<span class="math inline">\(\eta\)</span>)。在每次迭代<span class="math inline">\(j\)</span>中，我们按顺序交替更新<span class="math inline">\(\left(u_\theta, \varphi_\eta, \gamma_\theta,
\bar{\varphi}_\eta\right)\)</span>，每个<span class="math inline">\(\left(u_\theta, \varphi_\eta, \gamma_\theta,
\bar{\varphi}_\eta\right)\)</span>都有一个或几个SGD步(20)。我们将为下面的实验指定这些网络的步数。</p>
<p>在第3节的推导过程中，我们要求有界的网络参数<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\eta\)</span>，其中界<span class="math inline">\(B\)</span>可以任意大，以确保使用样本的积分估计量的有限方差，从而保证SGD收敛。另一种处理有界性约束的方法是在(7)的目标函数中添加<span class="math inline">\(|\theta|^2\)</span>和<span class="math inline">\(|\eta|^2\)</span>作为正则项。我们也可以使用分母为<span class="math inline">\(\|\varphi\|_2^2:=\int_{\Omega}|\varphi|^2
\mathrm{~d}
x\)</span>(用MC近似,类似于式(15))的算子范数(5)，这在我们的实现中也被采用。这种替换不会引起数值实现上的问题，因为测试函数<span class="math inline">\(\varphi_\eta\)</span>是由一个具有固定宽度/深度和有界参数的网络实现的，因此可以保证在<span class="math inline">\(H^1\)</span>中。</p>
<p>弱式(3)要求一个检验函数<span class="math inline">\(\varphi_\eta\)</span>在<span class="math inline">\(\partial
\Omega\)</span>上消失。保证这一点的一个简单技巧是，预先计算一个函数<span class="math inline">\(\varphi_0 \in C(\Omega)\)</span>，使得当<span class="math inline">\(x \in \partial \Omega\)</span>时，<span class="math inline">\(\varphi_0(x)=0\)</span>且当<span class="math inline">\(x \in \Omega\)</span>时，<span class="math inline">\(\varphi_0(x)&gt;0\)</span>(e.g.,一个到<span class="math inline">\(\partial
\Omega\)</span>的距离函数)。然后寻找一个对其任意边界无约束的参数化网络<span class="math inline">\(\varphi_\eta^{\prime}\)</span>，将测试函数<span class="math inline">\(\varphi_\eta\)</span>设置为<span class="math inline">\(\varphi_0 \varphi_\eta^{\prime}\)</span>，且在∂
<span class="math inline">\(\partial \Omega\)</span>上仍取零值。</p>
<p>我们使用TensorFlow [1]
(Python版本3.7)实现了我们的算法，这是一个先进的深度学习包，可以有效地利用GPU进行并行计算。通过TensorFlow内置的自动微分模块计算关于网络参数(<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\eta\)</span>)和输入(<span class="math inline">\(x\)</span>)的梯度。在训练过程中，我们还可以用它的许多变体来代替标准的SGD优化器，如AdaGrad，RMSprop算法，Adam，Nadam等。在我们的实验中，我们使用了TensorFlow包提供的AdaGrad，在我们的大部分测试中，AdaGrad似乎提供了比其他优化器更好的性能。其他所有参数，如网络结构(层数和神经元数)、步长(也称为学习率)、迭代次数等将在第4节中指定</p>
]]></content>
      <categories>
        <category>论文精读</category>
      </categories>
      <tags>
        <tag>EIT</tag>
        <tag>弱对抗网络</tag>
      </tags>
  </entry>
  <entry>
    <title>OPTIMAL TRANSPORTATION FOR ELECTRICAL IMPEDANCE  TOMOGRAPHY</title>
    <url>/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/</url>
    <content><![CDATA[<h1 id="optimal-transportation-for-electrical-impedance-tomography电阻抗断层成像的最优传输">OPTIMAL
TRANSPORTATION FOR ELECTRICAL IMPEDANCE
TOMOGRAPHY（电阻抗断层成像的最优传输）</h1>
<p>期刊：MATHEMATICS OF COMPUTATION</p>
<p>时间：September 2024</p>
<h2 id="摘要">摘要</h2>
<p>这项工作建立了一个用基于测地线的二次Wasserstein距离(<span class="math inline">\(W_2\)</span>)求解逆边界问题的框架。Fréchet梯度的一般形式由最优运输(
OT )理论系统推导得到。此外，基于OT在<span class="math inline">\(\mathbb{S}^1\)</span>上的新公式开发了一种快速算法来求解相应的最优运输问题。该算法的计算复杂度由传统方法的<span class="math inline">\(O(N^3)\)</span>降低到<span class="math inline">\(O(N)\)</span>。结合伴随状态方法，该框架为解决具有挑战性的电阻抗层析成像问题提供了一种新的计算方法。数值例子说明了我们方法的有效性。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>OT给出了一个比较两个概率测度的框架，通过寻求将一个测度重新排列到另一个测度的最小成本。它在机器学习、经济学、光学设计、成像科学、图形学[25、27、50]等不同领域都有广泛的应用。</p>
<p>在过去的几年中，最优传输已经被应用于求解反问题[1、14、26、36、53]。基于OT的一个通用框架是使用Wasserstein距离来衡量数据匹配问题中数据集的差异性。这是一种有吸引力的方法，因为Wasserstein距离，特别是二次Wasserstein距离<span class="math inline">\((W_2)\)</span>，具有捕获幅度和空间信息的能力。与传统的<span class="math inline">\(L^2\)</span>度量相比，<span class="math inline">\(W_2\)</span>具有更好的凸性，对噪声更加鲁棒[21]。</p>
<p>本文旨在<strong>发展一种基于二次Wasserstein距离的方法来求解严重不适定的反问题</strong>（EIT问题）。</p>
<p>对于许多反问题，观测数据是在边界上测量的，通常是欧氏空间中的低维流形。在度量中融入流形的几何信息是很自然的。在这项工作中，我们<strong>不使用传统的欧氏距离作为OT的代价函数，而是考虑流形上的运输问题，并采用相应的测地线距离作为其代价函数</strong>。这不仅提高了计算效率，而且更好地捕获了数据的几何特征。特别地，<strong>采用基于测地线的<span class="math inline">\(W_2\)</span>距离作为误匹配函数来解决数据在圆上测量的二维EIT问题。基于我们提出的<span class="math inline">\(\mathbb
S^1\)</span>上的OT新公式，设计了一种有效的算法来计算二次Wasserstein距离</strong>。本文方法的复杂度降低为<span class="math inline">\(O(N)\)</span>，而单纯形算法和Sinkhorn算法的复杂度分别为<span class="math inline">\(O(N^3)\)</span>和<span class="math inline">\(O(N^2)\)</span>。求解该优化问题的一个关键步骤是<strong>开发一种新的计算<span class="math inline">\(W_2\)</span>的Fréchet梯度的框架</strong>，该框架通过观察Kantorovich势和最优映射之间的显式联系来实现。该框架与现有的方法[14、53]形成了强烈的对比，[14、53]的梯度是通过对完全非线性Monge-Ampère方程的扰动得到的。最后，采用梯度下降算法求解EIT的优化问题。</p>
<h2 id="最优传输理论">最优传输理论</h2>
<p>这一节主要内容：开发了一种新的方法来推导<span class="math inline">\(W\)</span>的Fréchet梯度</p>
<h3 id="原问题和对偶问题">原问题和对偶问题:</h3>
<p>Monge的大批量运输问题就是最小化泛函 <span class="math display">\[
\int_X c(x,T(x)) \,d\mu(x) \qquad (2.1)
\]</span> 问题(2.1)的对偶问题是最大化 <span class="math display">\[
J(\varphi,\psi):= \int_X \varphi(x)\, d\mu(x) + \int_Y \psi(y) \,
d\nu(y) \qquad (2.3)
\]</span> 连续函数集<span class="math inline">\((\varphi,\psi)\in C(X)
\times C(Y)\)</span>满足<span class="math inline">\(\text{Lip}_c\)</span> <span class="math display">\[
\varphi(x) + \psi(y) \le c(x,y) , \forall (x,y) \in X \times Y  \qquad
(2.4)
\]</span>
标准对偶结果[48]表明(2.1)式的下确界等于(2.3)式的上确界。对偶形式是凸约束下的线性优化问题，这对于设计数值算法是可取的。</p>
<p><strong>注2.1</strong>：  事实上，<strong>对偶问题(2.3)只依赖于单一变量<span class="math inline">\(\varphi\)</span></strong>。对于<span class="math inline">\(\varphi \in C(X)\)</span>，其c-变换<span class="math inline">\(\varphi^c\)</span>定义为. <span class="math display">\[
\varphi^c(y):= \underset{x \in X}{\inf}\lbrace c(x,y)-\varphi(x) \rbrace
, \forall y \in Y \qquad (2.5)
\]</span> 对任意<span class="math inline">\((\varphi,\psi) \in
\text{Lip}_c\)</span>，我们有<span class="math inline">\(\varphi^c \ge
\psi\)</span>，进一步有<span class="math inline">\(\varphi^{cc}:=(\varphi^c)^c \ge
\varphi\)</span>。由此可知，<span class="math inline">\(J(\varphi^{cc},\varphi^c) \ge J(\varphi,\varphi^c)
\ge J(\varphi,\psi)\)</span>。<span class="math inline">\(J(\varphi,\psi)\)</span>的上确界是在一个较小的集合中得到的：
<span class="math display">\[
\Phi_c:=\lbrace (\varphi^{cc},\varphi^c),\varphi \in C(X) \rbrace \qquad
(2.6)
\]</span></p>
<p>集合<span class="math inline">\(\Phi_c\)</span>是良定义的，因为有<span class="math inline">\((\varphi^{cc})^c =
\varphi^c\)</span>。<strong>对<span class="math inline">\((\varphi,\psi)
\in \Phi_c\)</span>最大化(2.3)</strong>，我们<strong>称这个最优的<span class="math inline">\(\varphi\)</span>为Kantorovich势</strong>。</p>
<h3 id="wasserstein距离">Wasserstein距离</h3>
<p>当<span class="math inline">\(X=Y:=M\)</span>表示具有度量<span class="math inline">\(d\)</span>的同一度量空间时，最优运输问题自然地定义了概率测度空间上的距离，通常称为Wasserstein距离.
<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>之间的p-Wasserstein距离为： <span class="math display">\[
W_p(\mu,\nu)=(\underset{T \in \Pi(\mu,\nu)}{\inf} \int_M
d(x,T(x))^p\,d\mu(x))^\frac{1}{p}
\]</span>
它将μ和ν之间的距离度量为将一个分布重新排列到另一个分布的最优成本。特别地，我们主要关注二次Wasserstein距离(<span class="math inline">\(W_2\)</span>)，我们只考虑<span class="math inline">\((M,d)\)</span>的最常见情形 1. <span class="math inline">\(M=\mathbb{R}^d  \qquad d(x,y)=|x-y|\)</span> 2.
<span class="math inline">\(M\)</span>是紧致黎曼流形，<span class="math inline">\(d\)</span>是<span class="math inline">\(M\)</span>上的测地距离</p>
<p>为了说明最小化式(2.1)的最优运输映射<span class="math inline">\(T \in
\Pi(\mu,\nu)\)</span>和最大化式(2.3)的最优对偶对<span class="math inline">\((\varphi,\psi)\in
\Phi_c\)</span>之间的关系，我们考虑特殊情况<span class="math inline">\(M
=\mathbb R^d\)</span>和<span class="math inline">\(c(x,y) = \frac{1}{2}|
x-y |^2\)</span>。通过(2.2), <span class="math display">\[
\int_{\mathbb{R}^d} c(x,T(x))\, d\mu(x)=\int_{\mathbb{R}^d} \varphi(x)\,
d\mu(x) + \int_{\mathbb{R}^d} \psi(y)\, d\nu(y) \\ = \int_{\mathbb{R}^d}
(\varphi(x)+\psi(T(x)))\, d\mu(x)
\]</span></p>
<p>结合不等式(2.4),得到</p>
<p><span class="math display">\[
\varphi (x) + \psi (T(x)) = c(x,T(x)),\quad d\mu \, \text{almost
everywhere} \qquad (2.9)
\]</span> (这里是x和T(x)的关系，不等式（2.4）是任意x,y)</p>
<p>另一方面，在重新排列项后，我们从(2.4)中得到</p>
<p><span class="math display">\[
x \cdot y \le (\frac{1}{2}|x|^2-\varphi (x))+(\frac{1}{2}|y|^2-\psi
(y)), \qquad \forall x,y\in \mathbb{R}^d
\]</span></p>
<p>因此，注2.1中的C-变换,通过引入<span class="math inline">\(\varphi^*
(x):=\frac{1}{2}|x|^2-\varphi(x)\)</span> 和 <span class="math inline">\(\psi^*
(y):=\frac{1}{2}|y|^2-\psi(y)\)</span>可以转化为Legendre变换
<strong>(?)</strong>。由于<span class="math inline">\((\varphi,\psi) \in
\Phi_c\)</span>，<span class="math inline">\(\varphi^*\)</span>与<span class="math inline">\(\psi^*\)</span>之间的关系由下面的Legendre变换刻画</p>
<p><span class="math display">\[
\begin{cases}
   \psi^*(y):=\underset{x \in \mathbb {R}^d}{\sup} \lbrace x \cdot y -
\varphi^*(x) \rbrace \\
   \varphi^*(x):=\underset{y \in \mathbb {R}^d}{\sup} \lbrace x \cdot y
- \psi^*(y) \rbrace
\end{cases} \qquad (2.10)
\]</span></p>
<blockquote>
<p>证明： <span class="math display">\[
\begin{split}   
\psi^*(y) &amp;=\frac{1}{2}|y|^2-\psi(y)\\
&amp;=\frac{1}{2}|y|^2-\varphi^c(y) \\
&amp;= \frac{1}{2}|y|^2-\underset{x\in X}{\inf} \lbrace c(x,y) -
\varphi(x) \rbrace\\
&amp;= \underset{x\in X}{\sup}\lbrace
\frac{1}{2}|y|^2-\frac{1}{2}|x-y|^2+\varphi(x) \rbrace \\
&amp;=\underset{x\in X}{\sup}\lbrace x \cdot
y-\frac{1}{2}|x|^2+\varphi(x) \rbrace \\
&amp;=\underset{x\in X}{\sup}\lbrace x \cdot y-\varphi^*(x) \rbrace
\end{split}
\]</span></p>
</blockquote>
<p>$ ^* $和 $^* $都是凸函数，因为它们是一族线性函数的上确界。因此 $ ^*
<span class="math inline">\(几乎处处可微。对于使\)</span> <sup>* <span class="math inline">\(可微的\)</span>x</sup>d<span class="math inline">\(，我们寻求\)</span>y<sup>d<span class="math inline">\(，使得\)</span></sup>* ( x ) + ^ * ( y ) = x
y$。由(2.10)，该式成立当且仅当:</p>
<p><span class="math display">\[
\underset{z \in \mathbb{R}^d}{\sup} \lbrace \varphi^*(x)-\varphi^*(z)-y
\cdot (x-z) \rbrace = 0
\]</span></p>
<blockquote>
<p>证明： <span class="math display">\[
\varphi^*(x)+\psi^*(y)=x \cdot y \iff \varphi^*(x)+\psi^*(y)-x \cdot y
=0 \\
\iff \varphi^*(x) + \underset{z \in \mathbb {R}^d}{\sup} \lbrace z \cdot
y - \varphi^*(z) \rbrace - x \cdot y=0\\ \iff \underset{z \in
\mathbb{R}^d}{\sup} \lbrace \varphi^*(x)-\varphi^*(z)-y \cdot (x-z)
\rbrace = 0
\]</span></p>
</blockquote>
<p>那就是说，<span class="math inline">\(\varphi^*(z)\ge \varphi^*(x)+y
\cdot(z-x)\)</span>，<span class="math inline">\(\forall z \in
\mathbb{R}^d\)</span>。<span class="math inline">\(y\)</span>的这一特征与<span class="math inline">\(\varphi^*
(x)\)</span>的次梯度的定义是一致的。由于<span class="math inline">\(\varphi^* (x)\)</span>在<span class="math inline">\(x\)</span>处可微，所以<span class="math inline">\(y =\nabla\varphi^*
(x)\)</span>成立[43]。这里我们用原始的<span class="math inline">\((\varphi,\psi)\in
\Phi_c\)</span>来表示这种等价关系：</p>
<p><span class="math display">\[
\varphi(x)+\psi(y)=c(x,y) \quad \text{iff} \quad y=x-\nabla \varphi(x)
\quad \text{dx almost everywhere} \quad (2.11)
\]</span></p>
<p>式中<span class="math inline">\(dx\)</span>为<span class="math inline">\(\mathbb{R}^d\)</span>的Lebesgue测度元素。结合(2.9)和(2.11)，如果<span class="math inline">\(\mu\)</span>关于Lebesgue测度绝对连续，则方程<span class="math inline">\(T(x) = x-\nabla \varphi ( x
)\)</span>几乎处处成立。因此，<strong>最优映射<span class="math inline">\(T\)</span>在二次成本函数下具有关于Kantorovich势<span class="math inline">\(\varphi\)</span>的显式表达式</strong>。</p>
<blockquote>
<p>这里的意思是如果(x,y)满足<span class="math inline">\(\varphi(x)+\psi(y)=c(x,y)\)</span>，那么(x,y)应该满足<span class="math inline">\(y=T(x)(式2.9)\)</span>。上边的推导又说<span class="math inline">\(y=x-\nabla \varphi(x)\)</span>，那么<span class="math inline">\(T(x)=x-\nabla \varphi\)</span></p>
</blockquote>
<p>上述推导提供了最优映射与Kantorovich势之间的重要联系。更一般地，定理2.1总结了最优运输映射的存在性和特征：</p>
<p><strong>定理2.1</strong>:  令<span class="math inline">\(( M ,
d)\)</span>为式( 2.8 )定义的度量空间，<span class="math inline">\(c( x ,
y) = \frac{1}{2} d^2( x , y)\)</span>为二次成本。假设概率测度<span class="math inline">\(\mu\)</span>关于<span class="math inline">\(M\)</span>的体积测度绝对连续，则Monge问题( 2.1
)存在唯一解<span class="math inline">\(T\)</span>，其特征为<span class="math inline">\(T ( x ) = \exp_x ( -\nabla \varphi( x )
)\)</span>，其中<span class="math inline">\(\varphi：M→\mathbb{R}\)</span>为对偶问题(2.3)的Kantorovich势。此外，如果<span class="math inline">\(\text{supp}(\mu)\)</span>是某个连通开集的闭包，则<span class="math inline">\(\varphi\)</span>在可加常数下是唯一的。</p>
<p>这里<span class="math inline">\(\exp\)</span>表示切丛TM上的指数映射
<strong>(?)</strong>。记号<span class="math inline">\(\exp_p
X_p\)</span>是以<span class="math inline">\(p\in
M\)</span>为起点，沿<span class="math inline">\(X_p∈TM\)</span>的方向，长度为<span class="math inline">\(| X_p
|\)</span>的测地线段的端点。特别地，在欧氏空间<span class="math inline">\(\exp_p X_p = x + X_p\)</span>中，等价于结果<span class="math inline">\(T ( x ) = x-\nabla \varphi ( x
)\)</span>。<strong>定理2.1假设<span class="math inline">\(\mu\)</span>不给<span class="math inline">\(M\)</span>的体积测度的可忽略集赋予质量，从而保证了二次代价下最优映射的存在性和唯一性。</strong></p>
<p>事实上，Monge的最优运输<span class="math inline">\(T\)</span>可能并不总是存在的。我们的工作主要集中在Monge问题上，因为我们感兴趣的是具有适当密度函数的测度。这里我们假设测度<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>具有密度函数<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>：<span class="math inline">\(d\mu = f (
x ) dx，d\nu = g ( y ) dy\)</span>，其中<span class="math inline">\(dx\)</span>和<span class="math inline">\(dy\)</span>是<span class="math inline">\(M\)</span>的体积元.在整个过程中，用<span class="math inline">\(W_p( f , g)\)</span>代替<span class="math inline">\(W_p( \mu , \nu)\)</span>来表示<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>之间的Wasserstein距离。</p>
<p>注2.2 ( C -周期单调性)  在<span class="math inline">\(\mathbb{R}^d\)</span>中，<span class="math inline">\(T ( x ) = x-\nabla \varphi =\nabla
\varphi^*\)</span>，其中<span class="math inline">\(\varphi\)</span>是我们前面讨论过的凸函数。这个结果常被称为Brenier定理[11]
。此外，由于<span class="math inline">\(T\)</span>是某个凸函数的梯度，所以<span class="math inline">\(T\)</span>是循环单调的[43]。即对集合<span class="math inline">\(\lbrace 1，· · ·，N
\rbrace\)</span>上的任意置换<span class="math inline">\(\sigma\)</span>，对任意的<span class="math inline">\(\lbrace x_i \rbrace ^N_{i =1} \subset \text{supp}
(\mu)\)</span>，有<span class="math inline">\(\sum^N_{i=1}x_i \cdot
T(x_i) \ge \sum^N_{i=1}x_i \cdot T(x_{\sigma(i)})\)</span>。不等式推出
<span class="math display">\[
\sum^N_{i=1}c(x_i,T(x_i)) \le \sum_{i=1}^N c(x_i,T(x_{\sigma(i)}))
\]</span> 式中<span class="math inline">\(c\)</span>为二次成本。事实上，式(2.12)可以推广到任意连续成本函数<span class="math inline">\(c\)</span>的Polish空间上的最优运输问题。OT的这一性质被称为"
c
-周期单调性"，为刻画最优运输方案提供了可供选择的论据。具体参见[3、24、48]。</p>
<p>注2.3(Monge–Ampère方程)   如注释2.2所述最优运输映射<span class="math inline">\(T ( x ) =\nabla \varphi^*\)</span>。考虑到( 2.2
)的保测性质，我们利用变量替换技巧得到如下Monge - Ampère方程. <span class="math display">\[
\det(D^2\varphi^*(x))=\frac{f(x)}{g(\nabla \varphi^*(x))}
\]</span> 此外，由Caffarelli正则性定理[12、49]可知，若<span class="math inline">\(f，g\in
C^{0,\alpha}\)</span>在它们的支撑上，上下界被正常数所约束，且<span class="math inline">\(\text{supp} \, g\)</span>是凸的，则<span class="math inline">\(\varphi^ *\in C^{2,α}\)</span>，即<span class="math inline">\(\varphi \in C^{2,\alpha}\)</span>和<span class="math inline">\(T \in C^{1,\alpha}\)</span>。</p>
<p>使用<span class="math inline">\(W_2\)</span>作为失配函数也需要我们获取其梯度信息。由定理2.2可知，Wasserstein距离的Fréchet梯度与对应的Kantorovich势有关</p>
<p><strong>定理2.2</strong> ( Wasserstein距离的Fréchet梯度)   泛函<span class="math inline">\(f\to W_p^p( f , g)\)</span>是凸函数，其在<span class="math inline">\(f\)</span>处的次微分与式( 2.3
)的Kantorovich势集合重合。如果存在唯一的Kantorovich势<span class="math inline">\(\varphi\)</span>直到加性常数(additive
constants)<strong>(?)</strong>，则Fréchet导数<span class="math inline">\(\frac{\delta W^p_p(f,g)}{\delta
f}=\varphi\)</span>
事实上，定理2.2对于具有一般连续消耗函数的最小运输成本是有效的，除了<span class="math inline">\(c( x , y) = d^p( x , y)\)</span>。<strong>当<span class="math inline">\(p = 2\)</span>时，由定理2. 1和定理2.2可得，若<span class="math inline">\(\text{supp}\,
f\)</span>是某个连通开集的闭包，则<span class="math inline">\(\frac{\delta W^2_2(f,g)}{\delta
f}=\varphi\)</span></strong></p>
<h2 id="mathbbs1上的最优运输"><span class="math inline">\(\mathbb{S}^1\)</span>上的最优运输</h2>
<p>令<span class="math inline">\(M =
\mathbb{R}\)</span>，假设概率测度支撑在区间[0,1]。众所周知[48]，对于<span class="math inline">\(f，g \in L^1，W_2\)</span>及其最优运输映射为 <span class="math display">\[
W^2_2(f,g)=\int^1_0|F^{-1}(t)-G^{-1}(t)|^2 \, dt,\quad T(t)=G^{-1}(F(t))
\quad (3.1)
\]</span> 其中，<span class="math inline">\(F\)</span>和<span class="math inline">\(G\)</span>分别为<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>的累积分布函数： <span class="math display">\[
F(t)=\int ^t_0 f(\tau) \, d\tau, \qquad G(t)=\int ^t_0 g(\tau)\, d\tau
\]</span> 式(3.1)中分布函数的伪逆定义为 <span class="math display">\[
F^{-1}(y)=\inf \lbrace t:y&lt;F(t) \rbrace, \quad G^{-1}(y)= \inf
\lbrace t:y&lt;G(t) \rbrace \qquad (3.3)
\]</span> 特别地，如果<span class="math inline">\(f,g\)</span>有正的下界，即<span class="math inline">\(f,g \in D\)</span>，其中 <span class="math display">\[
D:= \lbrace f \in L^1[0,1]: \int^1_0 f(\tau) \, d\tau=1,f \ge \eta \quad
\text{on}\, \mathbb{S}^1 \text{for some} \, \eta&gt;0 \rbrace
\]</span> 那么分布函数<span class="math inline">\(F\)</span>和<span class="math inline">\(G\)</span>是严格递增的，因此它们的伪逆(3.3)成为经典的逆函数.由注2.3中提到的Caffarelli定理，严格正性是建立OT正则性的重要条件。</p>
<p>式(3.1)可能导致算法复杂度为<span class="math inline">\(O ( N
)\)</span>。需要指出的是，该结果仅对实际线路情况成立。在一般情况下，<span class="math inline">\(W_2\)</span>没有明确的表达式。由于直接基于优化问题(2.1)和(2.3)的现有方法涉及到高达<span class="math inline">\(O ( N^ 3
)\)</span>的计算复杂度，因此需要有效的新方法来计算运输成本。然而，作为例外，我们在下文中表明，对于<span class="math inline">\(\mathbb{S}^1 \subset
\mathbb{R}^2\)</span>，最优运输问题可以归结为实线上的问题。</p>
<p>考虑<span class="math inline">\(M=\mathbb{S}^1 \cong
\mathbb{T}=\mathbb{R}/ \mathbb{Z}\)</span>，对于<span class="math inline">\(\mathbb{S}^1\)</span>上的密度函数<span class="math inline">\(f\)</span>，将其定义域从区间<span class="math inline">\([0,1)\)</span>扩展到<span class="math inline">\(\mathbb{R}\)</span>，通过使<span class="math inline">\(f ( t )\)</span>周期化：<span class="math inline">\(f(t+1)=f(t)\)</span>。由于密度函数<span class="math inline">\(f\)</span>在每个单位区间上具有单位质量，因此分布函数和逆分布函数可以通过推广得到:</p>
<p><span class="math display">\[
F(t+1)=F(t)+1, \qquad F^{-1}(t+1)=F^{-1}(t)+1
\]</span></p>
<p>相应地，我们将测地距离<span class="math inline">\(d\)</span>从<span class="math inline">\(\mathbb{S}^1\)</span>投影到<span class="math inline">\(\mathbb{R}\)</span>：</p>
<p><span class="math display">\[
d(x,y):=\underset{k \in \mathbb{Z}}{\min}|x-y+k|, \quad x,y \in
\mathbb{R}
\]</span></p>
<p><strong>定理3.1将<span class="math inline">\(\mathbb{S}^1\)</span>上的OT问题转化为一维优化问题</strong>：</p>
<p><strong>定理3.1</strong>   设<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>是<span class="math inline">\(\mathbb{S}^1\)</span>上的两个概率密度函数，其累积分布函数<span class="math inline">\(F\)</span>，<span class="math inline">\(G\)</span>和逆分布函数<span class="math inline">\(F^{ - 1}，G ^{-
1}\)</span>分别由(3.2)，(3.3)和(3.5)定义。令<span class="math inline">\(G^\alpha\)</span>表示函数<span class="math inline">\(G + \alpha\)</span>。则<span class="math inline">\(\mathbb{S}_1\)</span>上的二次Wasserstein距离为:</p>
<p><span class="math display">\[
W^2_2(f,g)=\underset{\alpha \in \mathbb{R}}{\inf}
\int^1_0|F^{-1}-(G^{\alpha})^{-1}|^2 \, dt \qquad (3.7)
\]</span></p>
<p>此外，最优映射由<span class="math inline">\(T(t)=( ( G^{\alpha^* })
^{-1} \circ F ) ( t )\)</span>给出，其中<span class="math inline">\(α^*\)</span>为(3.7)式中的下确界点。</p>
<p>我们注意到，在更一般的设定下，(3.7)式也是通过一种不同的方法得到的[19]，即Aubry
- Mather定理。受文献[42]的启发，我们在这里的思路是基于c
-周期单调性，证明在<span class="math inline">\(\mathbb{S}^1\)</span>上的OT可以通过在某一点处切割圆来化简为(3.1)，这样更简单、更直接。</p>
<p>引理3.1涉及公式(3.7)的有用性质，特别是<strong>提供了一种直观的方法来寻找(3.7)中的下确界点<span class="math inline">\(\alpha^*\)</span></strong></p>
<p><strong>引理3.1</strong>   设<span class="math inline">\(f，g \in L^1
( \mathbb{S}^1 )\)</span>为<span class="math inline">\(\mathbb{S}^1\)</span>上的概率密度函数，定义 <span class="math display">\[
I(\alpha,f,g):=\int^1_0|F^{-1}(t)-(G^{\alpha})^{-1}(t)|^2 \, dt
\]</span> 则<span class="math inline">\(I(\alpha,f,g)\)</span>满足以下性质：</p>
<p>  (i) 对于固定的<span class="math inline">\(f，g\)</span>，令<span class="math inline">\(I ( \alpha ):= I( \alpha; f , g)\)</span>     (a)
<span class="math inline">\(I (\alpha)\)</span>关于<span class="math inline">\(\alpha\)</span>是凸的。若<span class="math inline">\(f，g\in D\)</span>，则<span class="math inline">\(I (\alpha)\)</span>是严格凸的     (b) <span class="math inline">\(I(\alpha)\)</span>在区间<span class="math inline">\([ - 1,1]\)</span>上取得全局最小值，当<span class="math inline">\(f，g \in D\)</span>时，<span class="math inline">\(I (\alpha)\)</span>是唯一的。     (c) 若<span class="math inline">\(f，g\in D\)</span>，则<span class="math inline">\(I (\alpha)\)</span>关于<span class="math inline">\(\alpha\)</span>在<span class="math inline">\(\mathbb{R}\)</span>上二次可微且 <span class="math display">\[
I&#39;(\alpha)=2\int^1_0 F^{-1}(G(t)+\alpha) \, dt-1 \qquad (3.9)\\
I&#39;&#39;(\alpha)=\int^1_0 \frac{2}{f(F^{-1}(G(t)+\alpha))} \, dt =
\int^1_0 \frac{2}{g(G^{-1}(F(t)-\alpha))} \, dt \qquad (3.10)
\]</span>   (ii) 对于固定的<span class="math inline">\(\alpha \in
\mathbb{R}，I( \alpha, f , g)\)</span>关于<span class="math inline">\(f,g \in
L^1(\mathbb{S}^1)\)</span>是Lipschitz连续的，Lipschitz常数<span class="math inline">\(C_{\alpha}:= 4 + 2 \lceil | α |
\rceil\)</span>，其中<span class="math inline">\(\lceil \cdot
\rceil\)</span>表示取整函数。</p>
<p><strong>引理3.1表示关于<span class="math inline">\(\alpha\)</span>的优化问题是凸的，等价于求解非线性方程<span class="math inline">\(I&#39;(\alpha) = 0\)</span></strong>
。利用已知的最优映射，我们可以推导出梯度公式<span class="math inline">\(\frac{\delta W^2_2(f,g)}{\delta
f}\)</span>如下：</p>
<p><strong>注3.1</strong> (<span class="math inline">\(W_2\)</span>在<span class="math inline">\(\mathbb{S}^1\)</span>上的Fréchet梯度)   对于<span class="math inline">\(f，g \in D\)</span>，根据定理2.2，梯度<span class="math inline">\(\frac{\delta W^2_2(f,g)}{\delta
f}\)</span>恰好为Kantorovich势。基于定理2.1中最优映射与Kantorovich势之间的联系，<span class="math inline">\(W^2_2\)</span>在<span class="math inline">\(\mathbb{S}^1\)</span>上的Fréchet梯度由下面沿圆周的积分给出：
<span class="math display">\[
\frac{\delta W^2_2(f,g)}{\delta f}=2 \int^t_0(\tau-T(\tau)) \, d\tau + c
\qquad (3.11)
\]</span> 其中<span class="math inline">\(T(\tau)=G^{-1}(F(\tau)-\alpha^*)\)</span>是<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>之间的最优输运映射，且<span class="math inline">\(c\)</span>是任意常数.</p>
<h2 id="数值方法">数值方法</h2>
<p>为了计算的目的，定义在<span class="math inline">\([0,1)\)</span>上的密度函数<span class="math inline">\(f, g \in D\)</span>在节点<span class="math inline">\(\tau_i=i * h, i=0,1,2, \cdots,
N-1\)</span>上离散，其中<span class="math inline">\(h=\frac{1}{N}\)</span>。为方便起见，我们将区间<span class="math inline">\([0,1)\)</span>扩展到<span class="math inline">\([-1,2]\)</span>，将其离散化： <span class="math inline">\(-1=t_{-N}&lt;t_{-(N-1)}&lt;\cdots
t_0&lt;\cdots&lt;t_{2 N}&lt;t_{2 N+1}=2\)</span> , <span class="math inline">\(t_i=(2 i-1) * \frac{h}{2}\)</span>, <span class="math inline">\(i=-N&lt;i&lt;2 N+1\)</span>。</p>
<p>在区间<span class="math inline">\([-1，2)\)</span>上定义： <span class="math display">\[
f_h(t)=f_i:=\frac{1}{m}f(\tau_{j(i)}) \quad \text{for} \quad t\in I_i ,
\quad j(i)=i \, \text{mod} \, N , \quad i=-N,...,2N
\]</span></p>
<p>其中<span class="math inline">\(I_i：= [ t_i，t_{i + 1}
)\)</span>，重标度参数<span class="math inline">\(m =\sum ^{N - 1}_{i =
0}f ( \tau_i ) h\)</span>为周期上的质量。显然，<span class="math inline">\(f_h ( t
)\)</span>是一个周期的分片常数函数，且在每个周期内具有单位质量。根据(3.2)，分别给出了累积分布函数和逆累积分布函数
<span class="math display">\[
F_h(t)=f_i(t-t_i)+F_i, \, t\in I_i, \quad
F^{-1}_h(y)=\frac{y-F_i}{f_i}+t_i, \, y \in [F_i,F_{i+1}) \quad (4.1)
\]</span> 式中，<span class="math inline">\(F_{-N} = -1，F_{- ( N-1 )} =
-1 + \frac{h}{2}f_{-N}\)</span>，且对于<span class="math inline">\(- (
N-1 ) &lt; i\le 2N\)</span>，<span class="math inline">\(F_i = F_{i-1} +
h * f_{i - 1}\)</span>。密度函数<span class="math inline">\(g\)</span>可以按照同样的方式进行离散化，得到<span class="math inline">\(g_h\)</span>和<span class="math inline">\(G_h\)</span>。</p>
<p>我们现在准备计算( 3.9 )的离散版本，以解决最优运输问题。难点在于( 3.9
)涉及到<span class="math inline">\(F\)</span>与<span class="math inline">\(G\)</span>的复合形式的逆，这就需要对<span class="math inline">\(F^{ - 1}\)</span>和<span class="math inline">\(G\)</span>的节点进行集体排序。为了完整起见，我们提供下面的细节。</p>
<p>给定<span class="math inline">\(α \in [ -
1,1]\)</span>，存在整数<span class="math inline">\(i_\alpha\)</span>使得<span class="math inline">\(α∈[ F_{i_\alpha}，F_{i_\alpha + 1}
)\)</span>，因此<span class="math inline">\(α + 1 \in [ F_{i_\alpha +
N}，F_{i_\alpha + N + 1} )\)</span>。对于两个递增序列<span class="math inline">\(\lbrace H^0_i:= G_i \rbrace ^N_{i =
1}\)</span>和<span class="math inline">\(\lbrace H^1_i:= F_{i_\alpha +
i}-\alpha \rbrace^N_{i =
1}\)</span>，我们将它们的值排序为一个递增序列，记为<span class="math inline">\(\lbrace H_n \rbrace
^{2N}_{n=1}\)</span>。排序过程自动定义一个双射<span class="math inline">\(\sigma：( i , j)\to n\)</span>，使得<span class="math inline">\(H_i^j\)</span>在新的序列中被重新排序为<span class="math inline">\(H_n\)</span> 。现在定义索引序列<span class="math inline">\(\lbrace l^0_n \rbrace ^{2N}_{n=1}\)</span>和<span class="math inline">\(\lbrace l^1_n \rbrace^{2N}_{n=1}\)</span>为：
<span class="math display">\[
\begin{cases}
   l^j_n=i \quad \text{where}(i,j)=\sigma^{-1}(n) \\
   l^{1-j}_n=n-l^j_n
\end{cases}
\]</span></p>
<p>因此，节点序列<span class="math inline">\(\lbrace T_n:= G^{-1}_h (
H_n ) \rbrace ^{2N}_{n=1}\)</span>容易计算： <span class="math display">\[
T_n=\begin{cases}
   \frac{H_n-G_{l^0_n}}{g_{l^0_n}}+t_{l^0_n}  \, &amp;,\text{if} \, j=1
\\
   t_i \, &amp;, \text{if} \, j=0
\end{cases}
\text{where}(i,j)=\sigma^{-1}(n)
\]</span></p>
<p>在序列中加入<span class="math inline">\(T_0 = 0\)</span>和<span class="math inline">\(T_{2N + 1} = 1\)</span>，我们得到<span class="math inline">\(\lbrace T_n \rbrace ^{2N + 1}_{n =
0}\)</span>。重置<span class="math inline">\(\lbrace l^1_n :=l^1_n +
i_\alpha \rbrace^{2N}_{n=1}\)</span>。则对于<span class="math inline">\(t \in [ T_n,T_{n + 1} )\)</span>， <span class="math display">\[
F^{-1}_h(G_h(t)+\alpha)=K_nt + B_n
\]</span></p>
<p>它是<span class="math inline">\([0,1)\)</span>上的分段线性函数。参数<span class="math inline">\(K_n\)</span>和<span class="math inline">\(B_n\)</span>的计算公式为 <span class="math display">\[
K_n=\frac{g_{l^0_n}}{f_{l^1_n}}, \quad
B_n=\frac{\alpha+G_{l^0_n-g_{l^0_n}
t_{l^0_n}}-F_{l^1_n}}{f_{l^1_n}}+t_{l^1_n}
\]</span> <strong>最后，将积分式( 3.9 )离散为</strong> <span class="math display">\[
I&#39;_h:=2\int^1_0
F^{-1}_h(G_h(t)+\alpha)\,dt-1=2\sum^{2N}_{n=0}(\frac{1}{2}K_n(T^2_{n+1}-T^2_n)+B_n(T_{n+1}-T_n))-1
\quad (4.2)
\]</span> 二阶导数<span class="math inline">\(I&#39;&#39;_h\)</span>可以按照同样的方式计算。</p>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法1} \quad 牛顿法 \\
   \hline
   \qquad \textbf{给定}f_h，g_h和精度\epsilon \\
   \qquad \textbf{设定}初始值k = 0，\alpha_0 = 1。通过( 4.1
)计算F_h，G_h及其逆。\\
   \\
   \qquad \textbf{while}|\alpha_k-\alpha_{k-1}| \ge \epsilon
\,\text{or}\,  k=0 \,\textbf{do} \\
   \qquad \qquad  如式(4.2)计算I&#39;_h ( \alpha_k )和I&#39;&#39;_h(
\alpha_k ) \\
   \qquad \qquad \text{更新}\alpha_{k+1}:=\alpha_k-\frac{I&#39;_h (
\alpha_k )}{I&#39;&#39;_h( \alpha_k )}\\
   \qquad \qquad \qquad k:=k+1 \\
   \qquad \textbf{end while} \\
   \qquad 利用输出值\alpha_k，那么距离W^2_2( f_h ,
g_h)及其\text{Fréchet}梯度可以用(4.2)中的方法计算 \\
   \hline
\end{array}
\]</span></p>
<p><strong>注4.1</strong>  处理曲线或曲面上的OT问题最直接的方法是将对偶问题(2.3)离散化，并求解由此产生的线性规划问题。假设计算区域被<span class="math inline">\(N\)</span>个点离散：<span class="math inline">\(x_1，· · ·，x_N\)</span>。距离矩阵<span class="math inline">\(\mathbf{C} = ( c ( x_i , x_j) ) _{ij} \in
\mathbb{R}^{N × N}\)</span>，分布离散化<span class="math inline">\(\mathbf{f} = ( f_1 , · · · , f_N)，\mathbf{g} = (
g_1 , · · · , g_N) \in \mathbb{R}^N，\text{有} \,
\mathbf{f}^T\mathbf{1}_N = \mathbf{g}^T\mathbf{1}_N =
1\)</span>，其中<span class="math inline">\(\mathbf{1}_N = ( 1 , 1 , · ·
· , 1) \in \mathbf{R}^N\)</span>，<span class="math inline">\(\mathbf{I}_N \in \mathbf{R}^{N ×
N}\)</span>为单位矩阵。那么线性规划问题可以写成：</p>
<p><span class="math display">\[
\max _{\mathbf{h} \in \mathbb{R}^{2 N}, \mathbf{A}^{\mathrm{T}}
\mathbf{h} \leq \mathbf{C}}\left[\begin{array}{l}
\mathbf{f} \\
\mathbf{g}
\end{array}\right]^{\mathrm{T}} \mathbf{h}, \quad
\mathbf{A}=\left[\begin{array}{c}
\mathbf{1}_N^{\mathrm{T}} \otimes \mathbf{I}_N \\
\mathbf{I}_N \otimes \mathbf{1}_N^{\mathrm{T}}
\end{array}\right] \in \mathbb{R}^{(2 N) \times N^2} \qquad (4.3)
\]</span></p>
<p>式中<span class="math inline">\(\otimes\)</span>为Kronecker积。我们可以利用线性规划的流行算法[8]，如单纯形法，内点法，匈牙利法和拍卖算法来求解(4.3)
.然而，其中任何一种算法的计算复杂度至少为<span class="math inline">\(O(N^3)\)</span>。</p>
<p><strong>注4.2</strong>   采用算法1中的牛顿法求解非线性方程<span class="math inline">\(I&#39;_h = 0\)</span>。<span class="math inline">\(I ( \alpha
)\)</span>的严格凸性保证了算法收敛到全局极小值点，在精度<span class="math inline">\(\epsilon\)</span>范围内最多需要<span class="math inline">\(O( \log_2\log_2 ( \frac{1}{\epsilon} )
)\)</span>步[10]得到<span class="math inline">\(\alpha\)</span>。由于对两个递增的序列进行排序需要<span class="math inline">\(2N\)</span>次比较，因此计算<span class="math inline">\(I&#39;_h\)</span>和<span class="math inline">\(I&#39;&#39;_h\)</span>的每一步最多需要<span class="math inline">\(O ( N
)\)</span>次运算。因此，该算法的计算复杂度为<span class="math inline">\(O( N \, \log_2 \log_2 (\frac{1}{\epsilon})
)\)</span>。</p>
<p>由引理3.1，对每一对<span class="math inline">\(( f , g) \in
D\)</span>，存在唯一的<span class="math inline">\(\alpha \in [-
1,1]\)</span>，使得<span class="math inline">\(I&#39;( \alpha , f , g) =
0\)</span>。因此，<span class="math inline">\(\alpha\)</span>可以看成是一个函数<span class="math inline">\(\alpha(f,g)\)</span>。密度函数的离散化会导致<span class="math inline">\(\alpha\)</span>的误差，从而导致相关的<span class="math inline">\(W_2\)</span>距离。接下来，我们给出<span class="math inline">\(\alpha ( f , g)\)</span>的稳定性估计</p>
<p><strong>引理4.1</strong> (稳定性估计)  假设<span class="math inline">\(\alpha( f , g)\)</span>是由<span class="math inline">\(I&#39;( \alpha ; f , g) =
0\)</span>定义的隐函数，且<span class="math inline">\(f_i，g_i \in
D\)</span>连续可微，<span class="math inline">\(i =
1，2\)</span>。那么对于<span class="math inline">\(\alpha_i:= \alpha (
f_i , g_i)\)</span>，下面的估计成立： <span class="math display">\[
\left|\alpha_1-\alpha_2\right| \leq
\frac{1}{2}\left(\left\|f_1-f_2\right\|_{L^1[0,1]}+\left\|g_1-g_2\right\|_{L^1[0,1]}\right)
\]</span></p>
<blockquote>
<p>公式太多了，在这里总结一下上边的内容，不敲公式了太麻烦，直接手写看图片吧</p>
</blockquote>
<p><img src="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/1.jpg">
<img src="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/2.jpg">
<img src="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/3.jpg">
<img src="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/4.jpg">
<img src="/2025/02/13/OPTIMAL-TRANSPORTATION-FOR-ELECTRICAL-IMPEDANCE-TOMOGRAPHY/5.jpg"></p>
<h2 id="eit">EIT</h2>
<p>设<span class="math inline">\(\Omega\)</span>是<span class="math inline">\(\mathbb{R}^2\)</span>中具有光滑边界<span class="math inline">\(\partial \Omega\)</span>的开有界区域，<span class="math inline">\(\sigma \in L^\infty(\Omega)\)</span>在<span class="math inline">\(\Omega\)</span>上严格为正。在我们的问题中，<span class="math inline">\(\Omega\)</span>是单位圆盘.
EIT正问题采用椭圆型偏微分方程建模： <span class="math display">\[
\begin{aligned}
-\nabla \cdot(\sigma \nabla u) &amp; =0 &amp; &amp; \text { in } \Omega
\\
\sigma \frac{\partial u}{\partial n} &amp; =j &amp; &amp; \text { on }
\partial \Omega
\end{aligned} \qquad (5.1)
\]</span> 式中<span class="math inline">\(u\)</span>和<span class="math inline">\(j\)</span>分别为电势和电流。这里引入Sobolev空间<span class="math inline">\(\tilde{H}^k(\partial \Omega)\)</span>和<span class="math inline">\(\tilde{H}^k(\Omega)\)</span>，其中<span class="math inline">\(k \in \mathbb{R}\)</span>， <span class="math display">\[
\begin{aligned}
\tilde{H}^k(\partial \Omega) &amp; :=\left\{v \in H^k(\partial \Omega):
\int_{\partial \Omega} v \mathrm{~d} s=0\right\} \\
\tilde{H}^k(\Omega) &amp; :=\left\{v \in H^k(\Omega): \int_{\partial
\Omega} v \mathrm{~d} s=0\right\}
\end{aligned} \qquad (5.2)
\]</span> 对于每个<span class="math inline">\(j \in
\tilde{H}^{-\frac{1}{2}}(\partial \Omega)\)</span>，存在唯一的<span class="math inline">\(u \in
\tilde{H}^1(\Omega)\)</span>，使得式(5.1)成立。这里我们用<span class="math inline">\(u=F(\sigma) j\)</span>来表示(5.1)的正解。
因此，对于每个满足条件的<span class="math inline">\(\sigma\)</span>，可以定义一个Neumann to Dirichlet
(NtD)算子<span class="math inline">\(\Lambda(\sigma):
\tilde{H}^{-\frac{1}{2}}(\partial \Omega) \rightarrow
\tilde{H}^{\frac{1}{2}}(\partial \Omega)\)</span> ：</p>
<p><span class="math display">\[
\Lambda(\sigma) j=\phi, \quad \text { where } \phi=\gamma F(\sigma) j
\qquad (5.3)
\]</span></p>
<p>其中<span class="math inline">\(\gamma\)</span>是投影<span class="math inline">\(\tilde{H}^1(\Omega)\)</span>到<span class="math inline">\(\tilde   {H}^{\frac{1}{2}}(\partial
\Omega)\)</span>的迹算子. NtD映射<span class="math inline">\(\Lambda(\sigma)\)</span>是自伴且正定的。而正问题就是已知<span class="math inline">\(\sigma\)</span>计算<span class="math inline">\(\Lambda(\sigma)\)</span>。反问题(EIT)
[9、15、47]是已知<span class="math inline">\(\Lambda(\sigma)\)</span>，重构<span class="math inline">\(\sigma\)</span>，可以表述为一个优化问题:</p>
<p><span class="math display">\[
\sigma^*=\underset{\sigma \in \mathcal{A}}{\operatorname{argmin}}
\mathcal{J}(\sigma), \quad \mathcal{J}(\sigma):=\sum_{n=1}^N
\mathfrak{D}\left(\Lambda(\sigma) j_n, \tilde{\phi}_n\right) \qquad
(5.4)
\]</span></p>
<p>其中，<span class="math inline">\(\left(j_1, \tilde{\phi}_1\right),
\cdots,\left(j_N,
\tilde{\phi}_N\right)\)</span>为NtD图谱的测量值。定义在边界上的误匹配函数<span class="math inline">\(\mathfrak{D}(\phi,
\tilde{\phi})\)</span>衡量了<span class="math inline">\(\phi\)</span>和<span class="math inline">\(\tilde{\phi}\)</span>之间的差异。容许集<span class="math inline">\(\mathcal{A}\)</span>是:</p>
<p><span class="math display">\[
\mathcal{A}=\left\{\sigma \in L_{\infty}(\Omega): c_0 \leq \sigma \leq
c_1 \text { on } \Omega,\left.\sigma\right|_{\partial
\Omega}=\left.\sigma_0\right|_{\partial \Omega}\right\} \qquad (5.5)
\]</span><br>
</p>
<p>在现有方法[15-17、29、32、44]中，选择<span class="math inline">\(\mathfrak{D}\)</span>为<span class="math inline">\(\partial \Omega\)</span>上的<span class="math inline">\(L^2\)</span>范数，并在<span class="math inline">\(\mathcal{J}(\sigma)\)</span>中加入正则化项<span class="math inline">\(\mathscr{R}(\sigma)\)</span>，得到新的目标泛函。</p>
<p><span class="math display">\[
\Psi(\sigma):=\mathcal{J}(\sigma)+\beta \mathscr{R}(\sigma) \qquad (5.6)
\]</span></p>
<p>其中<span class="math inline">\(\beta\)</span>是正则化参数。由此产生的优化问题通常采用基于梯度的迭代优化方法进行求解。然而，由于EIT的不适定性[
47 ]，重建容易受到边界测量中噪声的干扰。需要一个适当的<span class="math inline">\(\beta\)</span>值来稳定重建过程。然而，当噪声达到一定的阈值水平时，很难选择一个合适的<span class="math inline">\(\beta\)</span>来平衡重建的平滑性和准确性。</p>
<p>在这里，我们应用二次Wasserstein距离来解决优化问题(5.4)，因为<span class="math inline">\(W_2\)</span>距离具有以下良好的性质[21]。事实上，在<span class="math inline">\(W_2\)</span>下，初始数据和扰动数据之间的差异很小，因为质量的局部消除使得最优映射接近恒等映射，从而对高频噪声具有鲁棒性[40]。例如[48]</p>
<p><span class="math display">\[
W_2\left(f_n, f\right)=O\left(\frac{1}{n}\right),
\quad\left\|f_n-f\right\|_{L^2}=O(1)
\]</span></p>
<p>其中<span class="math inline">\(f_n=1+\sin (2 \pi n
x)\)</span>是定义在<span class="math inline">\([0,1]\)</span>上的<span class="math inline">\(f=1\)</span>的扰动.同时，<span class="math inline">\(L^2\)</span>偏向于沿振幅轴方向的位移，而<span class="math inline">\(W_2\)</span>同时考虑了空间变化和振幅变化。这意味着<span class="math inline">\(W_2\)</span>比<span class="math inline">\(L^2\)</span>对数据的形状变化更敏感。此外，<span class="math inline">\(W_2\)</span>的梯度比注2.3中提到的输入数据<span class="math inline">\(f\)</span>更平滑，对反演产生了平滑作用。</p>
<p>为了在<span class="math inline">\(W_2\)</span>的基础上进行优化，需要计算<span class="math inline">\(\mathcal{J}(\sigma)\)</span>和<span class="math inline">\(\mathcal{J}^{\prime}(\sigma)\)</span>，这涉及到<span class="math inline">\(W_2\)</span>的值和梯度的计算。对于EIT问题，选择误匹配函数<span class="math inline">\(\mathfrak{D}\)</span>为</p>
<p><span class="math display">\[
\mathfrak{D}(\phi, \tilde{\phi})=W_2^2(\mathcal{L}(\phi),
\mathcal{L}(\tilde{\phi})) \qquad (5.7)
\]</span></p>
<p>其中<span class="math inline">\(\mathcal{L}\)</span>是将电势转化为具有单位质量的非负密度函数的正规化算子，即(3.4)中的<span class="math inline">\(\mathcal{L}(\phi) \in
D\)</span>。由于我们测量了电势<span class="math inline">\(\phi \in
\tilde{H}^{\frac{1}{2}}(\partial \Omega)\)</span>，所以<span class="math inline">\(\phi\)</span>在<span class="math inline">\(\partial
\Omega\)</span>上的质量为零，自动满足质量守恒.因此，我们只需要重新刻度<span class="math inline">\(\phi\)</span>使其为正.为了达到这个目的，一个简单的方法[53]是增加一个正的常数<span class="math inline">\(a\)</span>：</p>
<p><span class="math display">\[
\mathcal{L}(\phi)=\frac{\phi+a}{\int_{\partial \Omega}(\phi+a)
\mathrm{d} s}=\frac{1}{a} \phi+1, \quad \phi \in
\tilde{H}^{\frac{1}{2}}(\partial \Omega) \qquad (5.8)
\]</span></p>
<p>在[53]中，<span class="math inline">\(W_2\)</span>的度量结构由于质量的归一化而丢失。然而，从(5.8)中我们知道<span class="math inline">\(W_2(\mathcal{L}(\phi),
\mathcal{L}(\tilde{\phi}))\)</span>定义在<span class="math inline">\(\tilde{H}^{\frac{1}{2}}(\partial
\Omega)\)</span>中的性质是保持的，因为质量对<span class="math inline">\(\tilde{H}^{\frac{1}{2}}(\partial
\Omega)\)</span>中的函数是守恒的。 <span class="math inline">\(W_2(\mathcal{L}(\phi),
\mathcal{L}(\tilde{\phi}))\)</span>的计算沿用上一节的方法，因此误匹配函数<span class="math inline">\(\mathcal{J}(\sigma)\)</span>可以很容易地计算得到。</p>
<p>另一个重要问题是推导<span class="math inline">\(\mathcal{J}(\sigma)\)</span>的Fréchet梯度。为了简化记号，我们在(5.4)中讨论了<span class="math inline">\(N=1\)</span>的情形。一阶扰动给出[41]：</p>
<p><span class="math display">\[
\begin{aligned}
\delta \mathcal{J}=\left\langle\frac{\delta \mathcal{J}}{\delta \phi},
\delta \phi\right\rangle_{L^2(\partial \Omega)} &amp;
=\left\langle\frac{\delta \mathfrak{D}(\phi, \tilde{\phi})}{\delta
\phi}, \delta \phi\right\rangle_{L^2(\partial
\Omega)}=\left\langle\frac{\delta \mathfrak{D}(\phi,
\tilde{\phi})}{\delta \phi}, \frac{\delta \Lambda(\sigma) j}{\delta
\sigma} \delta \sigma\right\rangle_{L^2(\partial \Omega)} \\
= &amp; \left\langle\left(\frac{\delta \Lambda(\sigma) j}{\delta
\sigma}\right)^* \frac{\delta \mathfrak{D}(\phi, \tilde{\phi})}{\delta
\phi}, \delta \sigma\right\rangle_{L^2(\Omega)}
\end{aligned}
\]</span></p>
<p>式中<span class="math inline">\(\phi=\Lambda(\sigma)
j\)</span>为状态变量。因此泛函的梯度由下式给出</p>
<p><span class="math display">\[
\mathcal{J}^{\prime}(\sigma)=\left(\frac{\delta \Lambda(\sigma)
j}{\delta \sigma}\right)^* \frac{\delta \mathfrak{D}(\phi,
\tilde{\phi})}{\delta \phi} \qquad(5.9)
\]</span></p>
<p>式中<span class="math inline">\(\mathfrak{D}\)</span>的梯度为</p>
<p><span class="math display">\[
\frac{\delta \mathfrak{D}(\phi, \tilde{\phi})}{\delta \phi}=\frac{1}{a}
\frac{\delta W_2^2(\mathcal{L}(\phi), \mathcal{L}(\tilde{\phi}))}{\delta
\mathcal{L}(\phi)} \qquad (5.10)
\]</span></p>
<p>由注3.1，梯度可按第4节（数值方法）的方法计算。对于<span class="math inline">\(\frac{\delta \Lambda(\sigma) j}{\delta
\sigma}\)</span>的伴随算子，在[9]中给出：</p>
<p><span class="math display">\[
\begin{aligned}
\left(\frac{\delta \Lambda(\sigma) j}{\delta \sigma}\right)^*:
\tilde{H}^{-\frac{1}{2}}(\partial \Omega) &amp; \longrightarrow
L_1(\Omega) \\
h &amp; \longmapsto-\nabla u \cdot \nabla \tilde{u}
\end{aligned} \qquad (5.11)
\]</span></p>
<p>其中<span class="math inline">\(u=F(\sigma) j\)</span>，<span class="math inline">\(\tilde{u}=F(\sigma) h\)</span>。式(5.10)中<span class="math inline">\(W_2\)</span>的Fréchet梯度涉及超参数<span class="math inline">\(c\)</span>的选择，如式(3.11)所述。这里选择常数<span class="math inline">\(c\)</span>，使得</p>
<p><span class="math display">\[
\int_{\partial \Omega} \frac{\delta \mathfrak{D}(\phi,
\tilde{\phi})}{\delta \phi} \mathrm{d} s=\frac{1}{a} \int_{\partial
\Omega} \frac{\delta W_2^2(\mathcal{L}(\phi),
\mathcal{L}(\tilde{\phi}))}{\delta \mathcal{L}(\phi)} \mathrm{d} s=0
\]</span></p>
<p>因此，回顾定义(5.2)，<span class="math inline">\(\frac{\delta
\mathfrak{D}(\phi, \bar{\phi})}{\delta \phi} \in
\tilde{H}^{-\frac{1}{2}}(\partial
\Omega)\)</span>，在算子(5.11)的定义域内。</p>
<p>一般地，Fréchet梯度(5.9)被定义为<span class="math inline">\(\langle\cdot,
\cdot\rangle_{L^2(\Omega)}\)</span>下的积分算子。改变下标空间<span class="math inline">\(L^2\)</span>到<span class="math inline">\(H_0^1\)</span>，我们可以通过公式<span class="math inline">\(\left\langle\mathcal{J}^{\prime}(\sigma),
\eta\right\rangle_{L^2(\Omega)}=\left\langle\mathcal{J}_s^{\prime}(\sigma),
\eta\right\rangle_{H_0^1(\Omega)}\)</span>定义关于<span class="math inline">\(\langle\cdot,
\cdot\rangle_{H_0^1(\Omega)}\)</span> 的梯度<span class="math inline">\(\mathcal{J}_s^{\prime}\)</span>，这等价于求解方程[29、31]：</p>
<p><span class="math display">\[
\begin{aligned}
-\Delta \mathcal{J}_s^{\prime}(\sigma)+\mathcal{J}_s^{\prime}(\sigma)
&amp; =\mathcal{J}^{\prime}(\sigma) &amp; &amp; \text { in } \Omega \\
\mathcal{J}_s^{\prime}(\sigma) &amp; =0 &amp; &amp; \text { on }
\partial \Omega
\end{aligned}
\]</span></p>
<p>这个梯度<span class="math inline">\(\mathcal{J}_s^{\prime}\)</span>被称为Sobolev梯度[38]
。对于EIT问题，我们采用Sobolev梯度<span class="math inline">\(\mathcal{J}_s^{\prime}\)</span>代替标准的<span class="math inline">\(L^2\)</span>梯度<span class="math inline">\(\mathcal{J}^{\prime}\)</span>。事实上，<span class="math inline">\(\mathcal{J}^{\prime}\)</span>一般不能给出合理的重构，而<span class="math inline">\(\mathcal{J}_s^{\prime}\)</span>稳定了不适定问题(5.4)
。此外，<span class="math inline">\(\mathcal{J}_s^{\prime}\)</span>的应用自然地保留了式(5.5)中边界上的已知信息。梯度<span class="math inline">\(\mathcal{J}_s^{\prime}\)</span>也比振荡优化过程中优选的<span class="math inline">\(\mathcal{J}^{\prime}\)</span>更平滑。</p>
<p>利用梯度信息，<strong>我们提出了带有非单调线搜索策略的Barzilai-Borwein梯度算法来最小化(5.6)</strong>。算法2总结了该方法的具体实现。</p>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法2} \quad Barzilai-Borwein梯度算法 \\
   \hline \
   \qquad \textbf{给定}初始值\sigma_0，参数M \in \mathbb{N}^{+}, \tau
\in(0,1), 0&lt;\rho_1&lt;\rho_2&lt;1, 0&lt;s_{\min }&lt;s_{\max
}，最大迭代次数K_{\max } \\
   \qquad \textbf{设定}初始值k:=0, s_0:=s_{\max },
\sigma_{+}:=\sigma_0，计算\mathcal{J}_s^{\prime}\left(\sigma_0\right)\\
   \qquad \textbf{while} \, k&lt;K_{\max } \, \textbf{do} \\
      \qquad \quad \textbf{while}\\
         \qquad \qquad \qquad \qquad \Psi\left(\sigma_{+}\right) \geq
\underset{0 \leq j \leq M-1} {\max}
\Psi\left(\sigma_{k-j}\right)-\frac{\tau}{2
s_k}\left\|\sigma_{+}-\sigma_k\right\|_{H^1(\Omega)}^2 \\
      \qquad \quad \textbf{do}\\
         \qquad \qquad 选择\rho \in\left[\rho_1,
\rho_2\right]，更新s_k:=\rho * s_k \\
         \qquad \qquad 令\gamma_k=\sigma_k-s_k
\mathcal{J}_s^{\prime}\left(\sigma_k\right)。解决代理问题 \\
         (5.13) \qquad \qquad \qquad  \sigma_{+}:=\underset{\sigma \in
\mathcal{A}}{\operatorname{argmin}} \frac{1}{2
s_k}\left\|\sigma-\gamma_k\right\|_{H^1(\Omega)}^2+\beta
\mathscr{R}(\sigma)\\
      \qquad \quad \textbf{end while} \\
   \qquad \quad
令\sigma_{k+1}:=\sigma_{+}，计算\mathcal{J}_s^{\prime}\left(\sigma_{k+1}\right)。然后计算
\\
   \qquad \qquad \qquad x_k  :=\left\langle\sigma_{k+1}-\sigma_k,
\sigma_{k+1}-\sigma_k\right\rangle_{H^1(\Omega)}\\
   \qquad \qquad \qquad y_k:=\left\langle\sigma_{k+1}-\sigma_k,
\mathcal{J}_s^{\prime}\left(\sigma_{k+1}\right)-\mathcal{J}_s^{\prime}\left(\sigma_k\right)\right\rangle_{H^1(\Omega)}\\
   \qquad \quad \textbf{if} \, \,  y_k \leq 0 \, \, \textbf{then}\\
   \qquad \qquad s_{k+1}:=s_{\max }\\
   \qquad \quad \textbf{else}\\
   \qquad \qquad s_{k+1}:=\min \left\{s_{\max }, \max \left\{s_{\min },
\frac{x_k}{y_k}\right\}\right\} \\
   \qquad \quad \textbf{end if}\\
   \qquad \quad k:=k+1\\
   \qquad \textbf{end while}\\
   \qquad 输出\sigma_k为重构 \\
   \hline
\end{array}
\]</span></p>
<p>这里我们在<span class="math inline">\(H^1\)</span>范数中，首先在每一步中对<span class="math inline">\(\mathcal{J}(\sigma)\)</span>在<span class="math inline">\(\sigma_k\)</span>处进行局部线性化：</p>
<p><span class="math display">\[
\mathcal{J}(\sigma) \approx
\mathcal{J}\left(\sigma_k\right)+\left\langle\mathcal{J}_s^{\prime}\left(\sigma_k\right),
\sigma-\sigma_k\right\rangle_{H^1(\Omega)}+\frac{1}{2
s_k}\left\|\sigma-\sigma_k\right\|_{H^1(\Omega)}^2 \qquad (5.14)
\]</span></p>
<p>由Barzilai-Borwein规则初始化的步长<span class="math inline">\(s_k\)</span>意在提供Hessian的标量近似<span class="math inline">\(s_k I\)</span>。将式(5.14)代入式(5.6)，<span class="math inline">\(\Psi\)</span>的最小化问题归结为代理问题(5.13)</p>
<p>代理问题( 5.13 )对于一些正则化项如<span class="math inline">\(\mathscr{R}(\sigma)=\frac{1}{2}\lVert \nabla
\sigma\rVert_{L^2(\Omega)}^2\)</span>是凸的，这意味着它可以直接使用一阶最优性条件来求解。然而，对于我们实验中使用的全变差正则化<span class="math inline">\(\mathscr{R}(\sigma)=\lVert \nabla
\sigma\rVert_{L^1(\Omega)}\)</span>不再是凸的。我们取光滑逼近<span class="math inline">\(\mathscr{R}(\sigma)=\int_{\Omega} \sqrt{|\nabla
\sigma|^2+\epsilon} \mathrm{d} x\)</span>，然后用<span class="math inline">\(\mathscr{R}(\sigma)\)</span>在 <span class="math inline">\(\sigma_k\)</span>附近的二阶展开式代替<span class="math inline">\(\mathscr{R}(\sigma)\)</span>：</p>
<p><span class="math display">\[
\mathscr{R}(\sigma) \approx
\mathscr{R}\left(\sigma_k\right)+\left\langle\mathscr{R}^{\prime}\left(\sigma_k\right),
\sigma-\sigma_k\right\rangle_{L^2(\Omega)}+\frac{1}{2}\left\langle\mathscr{R}^{\prime
\prime}\left(\sigma_k\right)\left(\sigma-\sigma_k\right),
\sigma-\sigma_k\right\rangle_{L^2(\Omega)}
\]</span></p>
<p>因此，代理问题变成了一个二次型，可以通过求解关于<span class="math inline">\(\sigma\)</span>的线性变分方程来解决：</p>
<p><span class="math display">\[
\begin{aligned}
\langle\sigma, \eta\rangle_{H^1(\Omega)}+s_k \beta \cdot A_k(\sigma,
\eta) &amp; =\left\langle\gamma_k, \eta\right\rangle_{H^1(\Omega)}-s_k
\beta \cdot B_k(\eta), \quad \forall \eta \in H_0^1(\Omega) \\
\left.\sigma\right|_{\partial \Omega} &amp; =\sigma_0
\end{aligned}
\]</span></p>
<p>这里，双线性形式<span class="math inline">\(A_k(\sigma,
\eta)\)</span>和线性形式<span class="math inline">\(B_k(\eta)\)</span>可由下式计算：</p>
<p><span class="math display">\[
\begin{aligned}
A_k(\sigma, \eta) &amp;
:=\left\langle\mathscr{R}\left(\sigma_k\right)^{-1} \nabla \sigma,
\nabla
\eta\right\rangle_{L^2(\Omega)}-\left\langle\mathscr{R}\left(\sigma_k\right)^{-3}
\nabla \sigma_k \cdot \nabla \sigma, \nabla \sigma_k \cdot \nabla
\eta\right\rangle_{L^2(\Omega)} \\
B_k(\eta) &amp;
\left.:=\left.\left\langle\mathscr{R}\left(\sigma_k\right)^{-3}\right|
\nabla \sigma_k\right|^2 \nabla \sigma_k, \nabla
\eta\right\rangle_{L^2(\Omega)}
\end{aligned}
\]</span></p>
]]></content>
      <categories>
        <category>论文精读</category>
      </categories>
      <tags>
        <tag>最优化</tag>
        <tag>EIT</tag>
        <tag>最优运输</tag>
      </tags>
  </entry>
  <entry>
    <title>最优传输（二）</title>
    <url>/2025/02/10/OT2/</url>
    <content><![CDATA[<h1 id="kantorovich问题的对偶问题">Kantorovich问题的对偶问题</h1>
<p>这篇文章我们讲讲对偶理论，并且探究对偶问题和Kantorovich问题之间的关系
<span id="more"></span></p>
<h2 id="离散版本的kantorovich问题对偶">离散版本的Kantorovich问题对偶</h2>
<p>有两个离散分布<span class="math inline">\(a=\sum_i
a_i\delta_{x_i},b=\sum_j b_i\delta_{y_i}\)</span>，分别包含离散点<span class="math inline">\(\lbrace {x_i} \rbrace_{i=1}^n, \lbrace {y_j}
\rbrace_{j=1}^m\)</span></p>
<p>有一个成本矩阵<span class="math inline">\(C \in \mathbb R^{n\times
m}\)</span>,<span class="math inline">\(C_{ij}\)</span>就是从<span class="math inline">\(x_i\)</span>运到<span class="math inline">\(y_j\)</span>的成本</p>
<p>我们寻找一个联合分布的矩阵<span class="math inline">\(P\in \mathbb
R^{n \times m}\)</span>使得总成本最小化 <span class="math display">\[
\underset{p\in U(a,b)}{\min}\langle C,P \rangle = \underset{p\in
U(a,b)}{\min} \sum_{i,j}C_{ij}P_{ij}
\]</span> 其中，<span class="math inline">\(U(a,b)=\{\pi \in \mathbb
R^{n\times m}_+|\pi1_m=a\in\mathbb R^n,\pi^T1_m=b\in\mathbb
R^m\}\)</span></p>
<p>拉格朗日对偶形式： <span class="math display">\[
\underset{P\ge 0}{\min}\underset{(f,g)\in \mathbb R^n \times \mathbb
R^m}{\max}\langle C,P \rangle + \langle a-P1_m,f \rangle + \langle
b-P^T1_n,g \rangle
\]</span></p>
<p>交换min，max，有</p>
<p><span class="math display">\[
\underset{(f,g)\in \mathbb R^n\times \mathbb R^m}{\max}\langle a,f
\rangle + \langle b,g \rangle + \underset{P\ge 0}{\min}\langle
C-f1^T_m-1_ng^T,P \rangle
\]</span></p>
<p>可以将后面min的变为约束</p>
<p><span class="math display">\[
C-f1^T_m-1_ng^T=C-f \oplus g \ge 0
\]</span></p>
<p>综上，我们得到了对偶问题：</p>
<p><span class="math display">\[
\underset{(f,g)\in R(a,b)}{max}\langle f,a \rangle + \langle g,b \rangle
\]</span></p>
<p>其中，<span class="math inline">\(R(a,b)=\{(f,g)\in \mathbb R^n\times
R^m:f_i+g_i \le C_{ij}\}\)</span></p>
<h2 id="连续版本的kantorovich问题对偶">连续版本的Kantorovich问题对偶</h2>
<p>现在两个分布变为连续的<span class="math inline">\(\alpha(x),x\in
X,\beta(y),y\in Y\)</span>，成本函数连续化<span class="math inline">\(c(x_i,y_j)=C_{ij}\)</span>，我们将上面的乘子向量<span class="math inline">\(f,g\)</span>推广成<span class="math inline">\(f(x_i)=f_i,g(y_j)=g_j\)</span>。得到对偶形式为
<span class="math display">\[
\underset{(f,g)\in R(c)}{\sup}\int_X f(x) \, d\alpha(x)+\int_Y g(y) \,
d\beta(y)
\]</span> 其中<span class="math inline">\(R(c)=\{(f,g):\forall
(x,y),f(x_i)+g(y_j) \le c(x_i,y_j)\}\)</span></p>
<h3 id="通俗理解原问题和对偶问题">通俗理解原问题和对偶问题</h3>
<p>假设有一个运营商运营着<span class="math inline">\(n\)</span>个仓库和<span class="math inline">\(m\)</span>个工厂，每个仓库有<span class="math inline">\(a_i\)</span>质量的商品，每个工厂需要<span class="math inline">\(b_j\)</span>质量的商品，从<span class="math inline">\(i\)</span>仓库到<span class="math inline">\(j\)</span>工厂运输单位质量的商品需要成本<span class="math inline">\(C_{ij}\)</span>。</p>
<p><strong>原问题就是站在运营商的角度考虑</strong>：找出最优的传输方案<span class="math inline">\(P^*\)</span>使得传输总成本<span class="math inline">\(\sum_{i,j}C_{ij}P_{ij}\)</span>最小。</p>
<p>假设这个运营商外包给了一个供应商，供应商只需要给每个仓库和工厂定价：单位质量的商品第<span class="math inline">\(i\)</span>个仓库收取<span class="math inline">\(f_i\)</span>的费用，第<span class="math inline">\(j\)</span>个工厂收取<span class="math inline">\(g_j\)</span>的费用。</p>
<p>供应商不能随便定价，供应商在定价过程中需要保证所有<span class="math inline">\(f_i+g_j \le
C_{ij}\)</span>，一旦超过这个价格，那在某些路径上运营商就要付出额外的价格，还不如自己运呢。</p>
<p>所以对偶问题就是站在供应商的角度来考虑：在<span class="math inline">\(f_i+g_j \le C_{ij}\)</span>的情况下希望收费<span class="math inline">\(\langle f,a \rangle + \langle g,b
\rangle\)</span>最大。</p>
<h2 id="互补松弛条件">互补松弛条件</h2>
<p>考虑原问题的最优解<span class="math inline">\(P^*\)</span>和对偶问题最优对<span class="math inline">\((f^*,g^*)\)</span>的关系，并且证明他们满足互补松弛条件</p>
<p>设<span class="math inline">\(\bar z\)</span>为原问题的最优解，<span class="math inline">\(\underline
z\)</span>为对偶问题的最优解，先证明<strong>弱对偶性</strong>满足，即<span class="math inline">\(\bar z \ge \underline z\)</span></p>
<p>首先对原始问题进行一下改写，把决策变量<span class="math inline">\(P\)</span>“拉直”为<span class="math inline">\(p\)</span>： <span class="math display">\[
P\in \mathbb R^{n \times m}\in U(a,b) \iff p \in \mathbb R^{nm}_+,Ap=
\begin{bmatrix}
   a  \\
   b
\end{bmatrix}
\]</span> 其中 <span class="math display">\[
A=
\begin{bmatrix}
   1^T_n \otimes I_m  \\
   I_n \otimes1^T_m
\end{bmatrix},A \in \mathbb R^{(n+m)\times nm}
\]</span> 定义<span class="math inline">\(c\)</span>也为成本矩阵对应的展平形式，则原始问题的拉格朗日函数：
<span class="math display">\[
H(h)=\underset{p\in \mathbb R^{nm}_+}{\min}(c^Tp-h^T(Ap-q))
\]</span> 其中<span class="math inline">\(q=\begin{bmatrix}
   a  \\
   b
\end{bmatrix}\)</span>。这是一个松弛问题，因为约束<span class="math inline">\(Ap=q\)</span>被软化为罚函数形式。对于原问题的最优解<span class="math inline">\(p^*\)</span>，对于任何<span class="math inline">\(h\)</span>，有： <span class="math display">\[
H(h)\le c^T p^*-h^T(Ap^*-q)=c^T p^*=\bar z
\]</span> 当<span class="math inline">\(h\)</span>满足对偶可行性(<span class="math inline">\(A^Th\le c\)</span>)时，对偶问题的目标值<span class="math inline">\(h^Tq\)</span>是拉格朗日函数的下界。</p>
<p>因此结合上式： <span class="math display">\[
\underline z=\underset{h,A^Th \le c}{\max}h^Tq\le
\underset{h}{\max}h^Tq+\underset{p\in \mathbb
R^{mn}_+}{\min}(c^T-h^TA)p=\underset{h\in \mathbb R^{n+m}}{\max}H(h)\le
c^T p^*=\bar z
\]</span>
这表明对偶问题的最优值不超过原始问题的最优值，从而证明了弱对偶性。</p>
<p>可以证明强对偶性也是满足的，证明过程需要slater条件。</p>
<p>对于连续型，也是有如下定理保证了强对偶性：</p>
<blockquote>
<p>设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>是定义在完备可分度量空间<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>上的概率测度，<span class="math inline">\(c:X\times Y \to \mathbb
R_+\)</span>是一个可测函数，那么： <span class="math display">\[
\underset{\pi\in\Pi(\mu,\nu)}{\inf}\int_{X\times Y}c \, d\pi =
\underset{(\varphi,\psi)}{\sup}\lbrack \int_X \varphi \, d\mu + \int_Y
\psi \, d\nu \rbrack
\]</span></p>
</blockquote>
<p>最优化理论告诉我们，如果强对偶性满足，那么我们可以使用KKT中的经典的互补松弛条件得到：
<span class="math display">\[
P^*(x,y)(f^*(x)+g^*(y)-c(x,y))=0
\]</span></p>
<p>这就是原问题的最优解<span class="math inline">\(P^*\)</span>和对偶问题最优对<span class="math inline">\((f^*,g^*)\)</span>的关系。即在支撑集上<span class="math inline">\(P^*&gt;0\)</span>，对偶问题的最优函数永远满足<span class="math inline">\(f^*(x)+g^*(y)=c(x,y)\)</span></p>
<h2 id="c-变换">c-变换</h2>
<p>现在我们知道对偶问题两个函数的一个很重要的性质，就是在<span class="math inline">\(P^*\)</span>支撑集上有<span class="math inline">\(f^*(x)+g^*(y)=c(x,y)\)</span></p>
<p>下面假设这个函数对不是最优的，如果我们固定了<span class="math inline">\(f(x)\)</span>，能否得到一个“最好”的<span class="math inline">\(g(y)\)</span>使得对偶问题的目标函数尽可能大？c-变换就回答了这个问题</p>
<p>因为 <span class="math display">\[
f(x)+g(y)\le c(x,y)\\
\iff g(y)\le c(x,y)-f(x)\\
\iff g(y)\le \underset{x}{\inf} \{c(x,y)-f(x)\}
\]</span> 而<span class="math inline">\(f(x)\)</span>已经固定，我们希望<span class="math inline">\(\int g(y) \, d\beta(y)\)</span>越大越好，所以<span class="math inline">\(g(y)\)</span>越大越好，自然就得到了一个“最好的”函数：
<span class="math display">\[
\bar f(y)=\underset{x}{\inf}\{c(x,y)-f(x)\}
\]</span> 这个<span class="math inline">\(\bar f(y)\)</span>就叫做<span class="math inline">\(f\)</span>的c-变换</p>
<p>如果一个函数<span class="math inline">\(f\)</span>可以写成某个<span class="math inline">\(g\)</span>的c-变换，那么就称<span class="math inline">\(f\)</span>是c-凹的</p>
<p><strong>注</strong>：对偶问题的最优解<span class="math inline">\((f^*,g^*)\)</span>一定是c-凹的。否则令另一个是其c-变换，目标函数变大，矛盾。</p>
<p>下面定理还保证了最优对的存在性： &gt;设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>是<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>上的概率测度使得 <span class="math display">\[
\int_{X\times Y}c(x,y) \, d\mu(x) \, d\nu(y)&lt;\infty
\]</span> 则对偶 Kantorovich 问题存在一个最优对<span class="math inline">\((\varphi,\psi)\)</span>且<span class="math inline">\(\bar \varphi = \psi\)</span>，<span class="math inline">\(\bar \psi=\varphi\)</span>几乎处处成立。</p>
<h2 id="kantorovichrubinstein定理">Kantorovich–Rubinstein定理</h2>
<p>虽然对偶问题要求两个函数<span class="math inline">\(f,g\)</span>
，但通过c-transform可以将这两个函数“联系”起来，对偶问题从 <span class="math display">\[
\underset{(f,g)\in R(c)}{\sup}\int_X f \, d\mu + \int_Y g \, d\nu
\]</span> 变为 <span class="math display">\[
\underset{f}{\sup}\int_X f \, d\mu + \int_Y \bar f \, d\nu
\]</span> 我们可以证明，如果在度量空间<span class="math inline">\(X=Y\)</span>上，<span class="math inline">\(d(x,y)\)</span>是度量，且<span class="math inline">\(c(x,y)=d(x,y)\)</span>。如果<span class="math inline">\(f=\bar g\)</span>是一组c-变换，那么<span class="math inline">\(f\)</span>是d-Lipschitz的。</p>
<p>证明如下： <span class="math display">\[
f(z)=\underset{y}{\inf} \lbrace d(z,y)-g(y) \rbrace \le
\underset{y}{\inf} \lbrace d(z,x)+d(x,y)-g(y) \rbrace=f(x)+d(z,x)
\]</span> 互换<span class="math inline">\(z,x\)</span>即得到<span class="math inline">\(|f(x)-g(z)|\le d(x,z)\)</span></p>
<p>然后我们可以继续证明，如果<span class="math inline">\(f\)</span>是d-Lipschitz的，那么其c-变换<span class="math inline">\(\bar f = -f\)</span>： &gt;令<span class="math inline">\(x=y\)</span>，有<span class="math inline">\(\bar
f(x)=\underset{x}{\inf}\{d(x,x)-f(x)\}\le -f(x)\)</span>，又因为<span class="math inline">\(f\)</span>是d-Lipschitz的。所以<span class="math inline">\(\bar f(y)=\underset{x}{\inf}\{d(x,y)-f(x)\}\ge
-f(y)\)</span>。所以<span class="math inline">\(\bar f=-f\)</span></p>
<p>综上，结合强对偶性，我们有 <span class="math display">\[
\underset{\pi\in U(a,b)}{\inf}\int_{X^2}d(x,y)\,
d\pi=\underset{f}{\sup}\int_X f \, d\mu + \int_X \bar f \, d\nu =
\underset{\lVert f \rVert_{\text{Lip}}\le 1}{\sup}\int_X f \, d\mu -
\int_X  f \, d\nu
\]</span> 其中，<span class="math inline">\(\lVert f
\rVert_{\text{Lip}}=\underset{x \not = y}{\sup}\frac {\lVert
f(x)-f(y)\rVert}{d(x,y)}\)</span></p>
<p>我们现在得到了一个非常好非常好的结论：当成本函数是一个度量的时候，对偶问题可以变成上面的形式，我们只需要求解一个d-Lipschitz函数就可以了。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>最优化</tag>
        <tag>最优运输</tag>
      </tags>
  </entry>
  <entry>
    <title>Solving electrical impedance tomography with deep learning  Networks</title>
    <url>/2025/03/17/Solving-electrical-impedance-tomography-with-deep-learning/</url>
    <content><![CDATA[<h1 id="solving-electrical-impedance-tomography-with-deep-learning-networks">Solving
electrical impedance tomography with deep learning Networks</h1>
<p>期刊：Journal of Computational Physics</p>
<p>时间：November 2019</p>
<h2 id="摘要">摘要</h2>
<p>本文介绍了一种利用深度神经网络解决电阻抗层析成像(EIT)问题的新方法。EIT的数学问题是从Dirichlet-to-Neumann(DtN)映射反演电导率。电导率到DtN映射的正向映射和反向映射都是高维非线性的。受正向映射的线性扰动分析的启发，基于数值低秩特性，我们提出了2D和3D问题的正向和反向映射的紧致神经网络结构。数值结果表明了所提出的神经网络的有效性。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>电阻抗层析成像(Electrical Impedance
Tomography，EIT)是通过在物体边界处进行电压和电流测量来确定未知介质电导率分布的问题。作为一种无辐射成像技术，EIT可以重复、非侵入地测量物体的区域变化；因此，它已被用作重症医学中多种应用的监测工具，如监测通气分布[52]、评估肺过度扩张[41]和检测气胸[18]，以及许多工业应用[53]。</p>
<h3 id="背景">背景</h3>
<p>EIT数学公式的核心是Dirichlet-to-Neumann（DtN）映射，它是椭圆偏微分方程分析中的关键对象，在经典
Calderón 问题中起着重要作用 [14,50,8]</p>
<p>EIT的控制方程，或等效的逆电导率问题，是 <span class="math display">\[
\begin{array}{cl}
-\operatorname{div}(\gamma(x) \nabla \phi(x))=0, &amp; \text { in }
\Omega \subset \mathbb{R}^d, \\
\phi(x)=\psi(x), &amp; \text { on } \partial \Omega,  \qquad(1.1)
\end{array}
\]</span></p>
<p>其中 是<span class="math inline">\(\Omega\)</span>有界Lipschitz域，<span class="math inline">\(\phi(x)\)</span>是电压，<span class="math inline">\(\gamma(x)&gt;0\)</span>是电导率分布，<span class="math inline">\(\psi(x)\)</span>是施加在边界上的电压。相应的DtN映射由
<span class="math display">\[
\Lambda_\gamma: H^{\frac{1}{2}}(\partial \Omega) \rightarrow
H^{-\frac{1}{2}}(\partial \Omega),\left.\left.\quad
\psi(x)\right|_{\partial \Omega} \rightarrow \gamma(x) \frac{\partial
\phi(x)}{\partial n(x)}\right|_{\partial \Omega}
\]</span></p>
<p>其中<span class="math inline">\(n(x)\)</span>是外法向量。这里<span class="math inline">\(H^{\frac{1}{2}}(\partial \Omega)\)</span>是<span class="math inline">\(L^2(\partial \Omega)\)</span>函数的空间，它是<span class="math inline">\(H^1(\Omega)\)</span>中函数的轨迹，<span class="math inline">\(H^{-\frac{1}{2}}(\partial
\Omega)\)</span>是它的对偶。我们建议读者阅读 [48]
以了解有关DtN映射的更多详细信息</p>
<p>一个密切相关的逆电导率问题涉及零能量时Schrödinger方程的DtN映射
[48]，其形式如下 <span class="math display">\[
\begin{aligned}
(-\Delta+\eta(x)) u(x)=0, &amp; \text { in } \Omega \\
u(x)=f(x), &amp; \text { on } \partial \Omega
\end{aligned}    \qquad(1.2)
\]</span></p>
<p>然后，Schrödinger方程的DtN映射由下式定义</p>
<p><span class="math display">\[
\Lambda_\eta: H^{\frac{1}{2}}(\partial \Omega) \rightarrow
H^{-\frac{1}{2}}(\partial \Omega),\left.\left.\quad
f(x)\right|_{\partial \Omega} \rightarrow \frac{\partial u(x)}{\partial
n(x)}\right|_{\partial \Omega} \qquad(1.3)
\]</span></p>
<p><span class="math inline">\(\Lambda_\eta\)</span>和<span class="math inline">\(\Lambda_\gamma\)</span>这两个DtN映射密切相关。如果<span class="math inline">\(\phi\)</span>是(1.1)的解，那么<span class="math inline">\(u=\sqrt{\gamma} \phi\)</span>是(1.2)
的解，其中<span class="math inline">\(\eta=\frac{\Delta
\sqrt{\gamma}}{\sqrt{\gamma}}\)</span>且 <span class="math inline">\(f=\sqrt{\gamma} \psi\)</span>。此外，<span class="math inline">\(\Lambda_\eta=\gamma^{-1 / 2} \Lambda_\gamma
\gamma^{-1 / 2}+\frac{1}{2 \gamma} \frac{\partial \gamma}{\partial
n}\)</span>。实际上，<span class="math inline">\(\Lambda_\eta\)</span>和<span class="math inline">\(\Lambda_\gamma\)</span>这两个映射携带相同的信息，并且可以相互确定[48]。本文将重点介绍Schrödinger方程的DtN映射<span class="math inline">\(\Lambda_\eta\)</span>。所有结果都可以毫无困难地扩展到
DtN 映射<span class="math inline">\(\Lambda_\gamma\)</span>。</p>
<p>由于DtN映射<span class="math inline">\(\Lambda_\eta\)</span>对于固定<span class="math inline">\(\eta\)</span>是线性的 [8]，因此对 <span class="math inline">\(r, s \in \partial
\Omega\)</span>，存在一个分布核<span class="math inline">\(\lambda_\eta(r, s)\)</span>使得 <span class="math display">\[
\left(\Lambda_\eta f\right)(r)=\frac{\partial u}{\partial
n}(r)=\int_{\partial \Omega} \lambda_\eta(r, s) f(s) \mathrm{d} S(s)
\qquad(1.4)
\]</span></p>
<p>DtN 映射的正向问题是，给定<span class="math inline">\(\eta(x)\)</span>，求解核<span class="math inline">\(\lambda_\eta(r, s)\)</span>，即<span class="math inline">\(\eta \rightarrow \lambda_\eta\)</span>。</p>
<p>逆问题的任务是在<span class="math inline">\(\Omega\)</span>中根据观测数据恢复<span class="math inline">\(\eta(x)\)</span>，观测数据通常是Dirichlet边界条件<span class="math inline">\(f\)</span> 的对<span class="math inline">\(\left(f, \Lambda_\eta
f\right)\)</span>和相应的Neumann数据<span class="math inline">\(\Lambda_\eta
f\)</span>的集合。在假设Dirichlet边界条件被充分采样的情况下，可以假设核<span class="math inline">\(\lambda_\eta\)</span>是已知的，因此，逆问题是从<span class="math inline">\(\lambda_\eta\)</span>恢复<span class="math inline">\(\eta\)</span>，即<span class="math inline">\(\lambda_\eta \rightarrow \eta\)</span>。由于 <span class="math inline">\(\left.\lambda_\eta(r, s)\right|_{r, s \in \partial
\Omega}\)</span>是<span class="math inline">\(2(d-1)\)</span>个变量的函数，而<span class="math inline">\(\eta(x)\)</span>是<span class="math inline">\(d\)</span>个变量的函数，因此如果<span class="math inline">\(d=1\)</span>，则由于简单的维度计数，逆问题无法解决。对于
<span class="math inline">\(d \geq
2\)</span>，原则上，逆问题的解存在并且在某些条件下是唯一的
[51]。然而，由于EIT的椭圆性质，即使对于<span class="math inline">\(d
\geq 2\)</span>，逆问题也是严重的病态[2-4,12]</p>
<p>正向和逆向问题的数值解可能具有挑战性。前向问题是从<span class="math inline">\(d\)</span>维函数到<span class="math inline">\(2(d-1)\)</span>维函数的映射。对于3D问题，计算和表示固定<span class="math inline">\(\eta\)</span>的整个DtN映射<span class="math inline">\(\Lambda_\eta\)</span>可能非常昂贵。对于逆问题，由于条件不良，逆映射<span class="math inline">\(\Lambda_\eta \rightarrow
\eta\)</span>在数值上不稳定[2-4,12]。为了避免不稳定，通常需要一个依赖于正则化项来稳定逆问题，例如，参见
[30,16,33]。从算法上讲，逆问题通常使用迭代方法 [30,27,11,12]
来解决，这通常需要大量的迭代。</p>
<p>在过去的几年里，深度神经网络（DNN）在计算机视觉、图像处理、语音识别和许多其他人工智能应用中取得了巨大成功
[31,37,26,43,39,47,38,46]。最近，基于DNN的方法也被应用于求解偏微分方程
[34,9,28,23,22,6,44,21,36]。这些尝试可分为两类。第一类[45,15,28,35,20]旨在表示具有DNN的高维偏微分方程的解（而不是有限元和有限差分法等经典方法）。第二类[42,29,34,23,22,21,36,40,7]处理参数化的PDE问题，并使用DNN来表示从PDE的高维参数到PDE解的映射。</p>
<h3 id="贡献">贡献</h3>
<p>深度神经网络在用于解决正向和逆向问题时具有多个优势。对于前向问题，由于新颖的软件和硬件架构可以快速将神经网络应用于输入数据，因此当用
DNN
表示前向映射时，可以显著加速前向问题。对于逆问题，解算法和正则化项的选择是两个关键问题。幸运的是，深度神经网络可以在这两个方面提供帮助。首先，关于求解算法，由于其在表示高维函数方面的灵活性，DNN
可能用于近似完整的逆映射，从而避免迭代求解过程。其次，关于正则化术语，机器学习的最新研究表明，DNN
通常可以自动从数据中提取特征，并提供数据驱动的正则化。</p>
<p>本文通过使用一种新颖的神经网络架构表示从<span class="math inline">\(\Lambda_\eta\)</span>到<span class="math inline">\(\eta\)</span>的逆映射，将深度学习方法应用于EIT问题。<strong>新架构的动机来自对EIT问题的正向映射和逆映射的线性近似的扰动分析</strong>。分析表明，在重参数化DtN映射<span class="math inline">\(\Lambda_\eta\)</span>后，<span class="math inline">\(\eta\)</span>和<span class="math inline">\(\Lambda_\eta\)</span>之间的映射在局部数值上是低秩的。这个观察使我们能够将<span class="math inline">\(d\)</span>维<span class="math inline">\(\eta\)</span>和<span class="math inline">\(2（d −
1）\)</span>维<span class="math inline">\(\Lambda_\eta\)</span>之间的映射简化为两个（准）<span class="math inline">\(（d −
1）\)</span>维函数之间的映射。作为平移不变性和全局性的，这个新映射用最近提出的
BCR-Net[10]表示，它是一个基于小波分解的非标准形式的多尺度神经网络。这种神经网络架构用于近似正向映射和反向映射。对于正在考虑的测试问题，由于BCR-Net的降维和紧凑的结构，最后的神经网络在2D情况下只有<span class="math inline">\(10^4 ∼ 10^5\)</span>个参数，在3D情况下只有<span class="math inline">\(10^5 ∼
10^6\)</span>个参数。相当少量的参数允许在相当有限的数据集上进行训练，这通常是
EIT 问题的情况。</p>
<h3 id="文章架构">文章架构</h3>
<p>本文的其余部分概述如下。第2节研究了DtN映射的数学背景。第 3
节讨论了2D情况下正向映射和逆向映射的DNN的设计和架构，以及数值测试。结果扩展到第4节中的3D情况。</p>
<h2 id="dtn映射的数学分析">DtN映射的数学分析</h2>
<p>本节总结了DtN映射的必要数学背景。让我们用<span class="math inline">\(\mathcal{G} f(x)=\int_{\Omega} G(x, y) f(y)
\mathrm{d} y\)</span>表示 <span class="math inline">\(\mathcal{L}=-\Delta+\eta\)</span>和<span class="math inline">\(\mathcal{G}=\mathcal{L}^{-1}\)</span>，其中<span class="math inline">\(G\)</span>是运算符<span class="math inline">\(\mathcal{L}\)</span>的Green函数，具有Dirichlet边界条件。散度定理的应用表明</p>
<p><span class="math display">\[
0=\int_{\partial \Omega} \frac{\partial u}{\partial n(y)}(y) G(x, y)
\mathrm{d} S(y)=\int_{\Omega} \operatorname{div}_y\left(\nabla_y u(y)
\cdot G(x, y)\right) \mathrm{d} y=\int_{\Omega}\left(\Delta_y u \cdot
G+\nabla_y G \nabla_y u\right) \mathrm{d} y \qquad(2.1)
\]</span></p>
<p>类似地，将散度定理第二次应用于上述结果会导致</p>
<p><span class="math display">\[
\begin{aligned}
\int_{\partial \Omega} \frac{\partial G}{\partial n(y)}(x, y) f(y)
\mathrm{d} S(y) &amp; =\int_{\Omega} \operatorname{div}_y\left(\nabla_y
G(x, y) \cdot u(y)\right) \mathrm{d} y \\
&amp; =\int_{\Omega}\left(\Delta_y G(x, y) \cdot u(y)+\nabla_y G(x, y)
\nabla_y u(y)\right) \mathrm{d} y \\
&amp; =\int_{\Omega}\left(\Delta_y G(x, y) \cdot u(y)-\Delta_y u(y)
\cdot G(x, y)\right) \mathrm{d} y \\
&amp; =\int_{\Omega}\left(-\left(-\Delta_y+\eta(y)\right) G(x, y) \cdot
u(y)+\left(-\Delta_y+\eta(y)\right) u(y) \cdot G(x, y)\right) \mathrm{d}
y \\
&amp; =-u(x)
\end{aligned} \qquad(2.2)
\]</span></p>
<p>这里最后一个相等利用的事实是<span class="math inline">\(G\)</span>是<span class="math inline">\(\mathcal{L}=-\Delta+\eta\)</span>的Green函数，<span class="math inline">\(u\)</span>是(1.2)的解。对 <span class="math inline">\(x \in \partial \Omega\)</span>取(2.2)的两条边关于
<span class="math inline">\(n(x)\)</span>的法向导数，得到 <span class="math display">\[
\frac{\partial u}{\partial n}(x)=-\int_{\partial \Omega}
\frac{\partial^2 G}{\partial n(x) n(y)}(x, y) f(y) \mathrm{d} S(y),
\quad x \in \partial \Omega \qquad(2.3)
\]</span> 它用格林函数G描述了DtN映射<span class="math inline">\(\Lambda_\eta(\psi)\)</span>的核:</p>
<p><span class="math display">\[
\lambda_\eta(r, s)=-\frac{\partial^2 G}{\partial n(r) n(s)}(r, s), \quad
r, s \in \partial \Omega  \qquad(2.4)
\]</span></p>
<p>为避免混淆，我们用<span class="math inline">\(r,
s\)</span>来表示边界上的点，用<span class="math inline">\(p,
q\)</span>来表示域中的点。</p>
<p>为了理解 DtN 映射如何取决于电位<span class="math inline">\(\eta\)</span>，我们对<span class="math inline">\(\eta&gt;0\)</span>接近固定<span class="math inline">\(\eta_0\)</span>的<span class="math inline">\(\eta\)</span>到<span class="math inline">\(\lambda_\eta\)</span>的映射进行了扰动分析。为简单起见，假设<span class="math inline">\(\eta_0=0\)</span>。让我们引入<span class="math inline">\(\mathcal{E}=-\eta \mathcal{I}\)</span>，其中<span class="math inline">\(\mathcal{I}\)</span>是恒等运算符，<span class="math inline">\(\mathcal{L}_0=-\Delta\)</span>，<span class="math inline">\(\mathcal{G}_0=\mathcal{L}_0^{-1}\)</span>（核用<span class="math inline">\(G_0\)</span>表示）作为<span class="math inline">\(\mathcal{L}_0\)</span>的格林函数，具有Dirichlet边界条件。当<span class="math inline">\(\eta&gt;0\)</span>足够小时，<span class="math inline">\(\mathcal{G}\)</span>可以通过Neumann级数展开</p>
<p><span class="math display">\[
\mathcal{G}=\left(\mathcal{L}_0-\mathcal{E}\right)^{-1}=\mathcal{G}_0+\mathcal{G}_0
\mathcal{E} \mathcal{G}_0+\mathcal{G}_0 \mathcal{E} \mathcal{G}_0
\mathcal{E} \mathcal{G}_0+\ldots \qquad(2.5)
\]</span></p>
<p>通过引入<span class="math inline">\(\lambda_0(r,
s)=\left.\lambda_\eta(r,
s)\right|_{\eta=\eta_0}\)</span>，可以通过了解背景情况<span class="math inline">\(\eta=\eta_0\)</span>来计算，相当于关注差值<span class="math inline">\(\lambda_\eta-\lambda_0\)</span>（通常称为差值成像，详见
[13]），这也是<span class="math inline">\(\mathcal{G}-\mathcal{G}_0\)</span>的核。对于足够小的<span class="math inline">\(\eta\)</span>，运算符<span class="math inline">\(\mathcal{G}-\mathcal{G}_0\)</span>可以用它的第一项<span class="math inline">\(\mathcal{G}_0 \mathcal{E}
\mathcal{G}_0\)</span>来近似，它在<span class="math inline">\(\mathcal{E}\)</span>中是线性的。利用<span class="math inline">\(\mathcal{E}=-\eta
\mathcal{I}\)</span>的事实，得出以下差值 DtN 映射的近似值<span class="math inline">\(\mu\)</span>，</p>
<p><span class="math display">\[
\mu(r, s):=\left(\lambda_\eta-\lambda_0\right)(r,
s)=-\frac{\partial^2\left(G-G_0\right)}{\partial n(r) \partial n(s)}(r,
s) \approx \int_{\Omega}\left(\frac{\partial G_0}{\partial n(r)}(r, p)
\frac{\partial G_0}{\partial n(s)}(p, s)\right) \eta(p) \mathrm{d} p
\qquad(2.6)
\]</span> <strong>它是NN体系结构设计的动机</strong>。</p>
<h2 id="用于2d情况的神经网络">用于2D情况的神经网络</h2>
<p>考虑域<span class="math inline">\(\Omega=[0,1] \times[-Z,
Z]\)</span>，其中<span class="math inline">\(Z\)</span>是一个固定常数。为简单起见，在左边界和右边界处指定了周期性边界条件。如图1.所示，电极只能放置在顶部边界（单侧检测）或顶部和底部边界（双面检测）上。对于单边检测，为简单起见，假设底部为零Dirichlet边界条件，但其他边界条件也相关。在下文中，我们将首先考虑用于单边检测的正向映射和反向映射。然后，该架构扩展到双边检测案例。</p>
<p>在大多数EIT问题中，电导率在域边界附近是已知的。这意味着存在一个常数<span class="math inline">\(\delta&gt;0\)</span>，使得 <span class="math inline">\(\eta(p)\)</span>支撑在<span class="math inline">\([0,1] \times[-(Z-\delta), Z-\delta]\)</span>。</p>
<figure>
<img src="/2025/03/17/Solving-electrical-impedance-tomography-with-deep-learning/1.jpg" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<h3 id="单边检测的前向映射">单边检测的前向映射</h3>
<p>对于单边检测，DtN映射限制在顶部边界。设<span class="math inline">\(r=\left(r_1, Z\right), s=\left(s_1,
Z\right)\)</span>和<span class="math inline">\(p=(x,
z)\)</span>，其中<span class="math inline">\(x\)</span>是水平坐标，<span class="math inline">\(z\)</span>是深度坐标。映射（2.6）可以重写为 <span class="math display">\[
\mu\left(\left(r_1, Z\right),\left(s_1, Z\right)\right) \approx
\int_{\Omega} \frac{\partial G_0}{\partial n(r)}\left(\left(r_1,
Z\right),(x, z)\right) \frac{\partial G_0}{\partial
n(s)}\left(\left(s_1, Z\right),(x, z)\right) \eta(x, z) \mathrm{d} x
\mathrm{~d} z \qquad(3.1)
\]</span></p>
<p>注意到<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>用于域<span class="math inline">\(\Omega\)</span>中的点，<span class="math inline">\(r\)</span>和<span class="math inline">\(s\)</span>用于边界上的点，<span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span>用于水平和深度坐标。</p>
<p>分析和架构设计的一个关键步骤是引入新的水平变量<span class="math inline">\(m\)</span>和<span class="math inline">\(h\)</span>，使<span class="math inline">\(r_1=m+h\)</span>和<span class="math inline">\(s_1=m-h\)</span>。用新的变量得到重参数化差值DtN映射<span class="math inline">\(\mu\)</span> <span class="math display">\[
\mu(m, h):=\mu((m+h, Z),(m-h, Z)) \approx \int_{\Omega} K(m, h, x, z)
\eta(x, z) \mathrm{d} x \mathrm{~d} z \qquad(3.2)
\]</span></p>
<p>核<span class="math inline">\(k\)</span>如下给出 <span class="math display">\[
K(m, h, x, z):=\frac{\partial G_0}{\partial n}((m+h, Z),(x, z))
\frac{\partial G_0}{\partial n}((m-h, Z),(x, z))  \qquad(3.3)
\]</span></p>
<p>其中，<span class="math inline">\(n=(0,1)\)</span> 且 <span class="math inline">\(\frac{\partial G_0}{\partial n}(\cdot,
\cdot)\)</span>为<span class="math inline">\(G_0\)</span>在第1个变量上的方向导数。注意到<span class="math inline">\(G_0\)</span>是左边和右边带有周期边界条件，上边和下边带有Dirichlet边界条件的区域上算子<span class="math inline">\(-\Delta\)</span>的Green函数，我们可以将<span class="math inline">\(G_0\)</span>显式地写成[25] <span class="math display">\[
G_0(p, q)=\sum_{\ell \in
\mathbb{Z}^2}\left(\Gamma\left(p-q+\left(\ell_1, 2 \ell_2
Z\right)\right)-\Gamma\left(p-q^*+\left(\ell_1, 2 \ell_2
Z\right)\right)\right) \qquad(3.4)
\]</span></p>
<p>式中 <span class="math inline">\(q^*=\left(q_1, 2
Z+q_2\right)\)</span>为算子<span class="math inline">\(-\Delta\)</span>在整个空间<span class="math inline">\(\mathbb{R}^2\)</span>上的格林函数。由于当<span class="math inline">\(\eta=\eta_0\)</span>时，Green函数<span class="math inline">\(G_0\)</span>在水平方向上是平移不变的， <span class="math display">\[
\frac{\partial G_0}{\partial n}((m \pm h, Z),(x, z))=\frac{\partial
G_0}{\partial n}(( \pm h, Z),(x-m, z)) \qquad(3.5)
\]</span></p>
<p>对于其余的讨论，可以方便地将<span class="math inline">\(h\)</span>和<span class="math inline">\(z\)</span>作为参数进行处理和引入 <span class="math display">\[
\begin{array}{ll}
\frac{\partial G_{0, \pm h, z}}{\partial n}(x-m):=\frac{\partial
G_0}{\partial n}(( \pm h, Z),(x-m, z), &amp; \eta_z(x):=\eta(x, z) \\
k_{h, z}(m):=\frac{\partial G_{0,+h, z}}{\partial n}(m) \frac{\partial
G_{0,-h, z}}{\partial n}(m), &amp; \mu_h(m):=\mu(m, h)
\end{array}
\]</span></p>
<p>利用新的记号，(3.2)可重新表述为 <span class="math display">\[
\mu_h(m) \approx \int_{-Z}^Z\left(k_{h, z} * \eta_z\right)(m) \mathrm{d}
z=\int_{-(Z-\delta)}^{Z-\delta}\left(k_{h, z} * \eta_z\right)(m)
\mathrm{d} z \qquad(3.6)
\]</span></p>
<p>式中卷积作用在<span class="math inline">\(m\)</span>。最后一个等式成立的原因是考虑到<span class="math inline">\(\eta\)</span>在深度方向上被支撑于<span class="math inline">\(-(Z-\delta)\)</span>和<span class="math inline">\(Z-\delta\)</span>之间</p>
<p><em>低秩逼近和降维</em> : 一个关键的观察是这样</p>
<p><span class="math display">\[
\text{the kernel} \, k_{h, z}(m) \, \text{is smooth in} \, h \,
\text{for} \, h \in[0,1] \, \text{and} \, z \in(-(Z-\delta), Z-\delta).
\]</span></p>
<p>考察式(3.3)中<span class="math inline">\(K\)</span>的定义可知，<span class="math inline">\(k_{h, z}(m)\)</span>仅在<span class="math inline">\(z=Z\)</span>时奇异，因此，对于<span class="math inline">\(h \in[0,1], m \in[0,1]\)</span>，和z<span class="math inline">\(z \in(-(Z-\delta), Z-\delta)\)</span>，核<span class="math inline">\(k_{h, z}(m)\)</span>是一致光滑的。<span class="math inline">\(h\)</span>和<span class="math inline">\(z\)</span>变量中的光滑性表明，<span class="math inline">\(k_{h, z}(m)\)</span>在<span class="math inline">\(h\)</span>和<span class="math inline">\(z\)</span>中可以通过少量项的近似方案得到很好的近似。为简化讨论，不失一般性地假设采用稳定的插值方案(如切比雪夫插值)。通过将<span class="math inline">\(h\)</span>变量和<span class="math inline">\(z\)</span>变量的插值点集分别记为<span class="math inline">\(\{\hat{h}\}\)</span>和<span class="math inline">\(\{\hat{z}\}\)</span>，这样的插值为 <span class="math display">\[
k_{h, z}(m) \approx \sum_{\hat{h}} \sum_{\hat{z}} R_{h, \hat{h}}
k_{\hat{h}, \hat{z}}(m) R_{z, \hat{z}} \qquad(3.7)
\]</span></p>
<p>其中<span class="math inline">\(R_{h, \hat{h}}\)</span>和<span class="math inline">\(R_{z, \hat{z}}\)</span>分别是<span class="math inline">\(h\)</span>和<span class="math inline">\(z\)</span>变量的插值算子.</p>
<p>这种对<span class="math inline">\(k_{h,
z}\)</span>的近似自然隐含了对(3.6)式的近似。</p>
<p><span class="math display">\[
\mu_h(m) \approx \sum_{\hat{h}} R_{h, \hat{h}}\left(\sum_{\hat{z}}
k_{\hat{h}, \hat{z}} *\left(\int_{-(Z-\delta)}^{Z-\delta} R_{z, \hat{z}}
\eta_z \mathrm{~d} z\right)\right)(m) \qquad(3.8)
\]</span></p>
<p>从算法上讲，<strong>这种近似允许将前向映射分解为三个步骤</strong>：
1. 将二维函数<span class="math inline">\(\eta_z=\eta(x,
z)\)</span>压缩为一维函数的集合</p>
<p><span class="math display">\[
\tilde{\eta}_{\hat{z}}(x):=\int_{-(z-\delta)}^{z-\delta} R_{z, \hat{z}}
\eta_z(x) \mathrm{d} z
\]</span></p>
<ol start="2" type="1">
<li>与k<span class="math inline">\(k_{\hat{h},
z}\)</span>在一维空间中卷积得到</li>
</ol>
<p><span class="math display">\[
\tilde{\mu}_{\hat{h}}(m):=\left(\sum_{\hat{z}} k_{\hat{h}, \hat{z}} *
\tilde{\eta}_{\hat{z}}\right)(m)
\]</span></p>
<ol start="3" type="1">
<li>将一维函数集合<span class="math inline">\(\tilde{\mu}_{\hat{h}}(m)\)</span>插值为二维函数</li>
</ol>
<p><span class="math display">\[
\mu_h(m)=\sum_{\hat{h}} R_{h, \hat{h}} \tilde{\mu}_{\hat{h}}(m)
\]</span></p>
<p>这有效地将前向映射减少到若干个1D卷积。在(3.8)中的这种降维是神经网络构造的基础。</p>
<p><strong>备注1.</strong> 可以去掉<span class="math inline">\(\eta(p)\)</span>支撑在在<span class="math inline">\([0,1] \times[-(Z-\delta),
Z-\delta]\)</span>的假设.实际上，我们可以用<span class="math inline">\(\delta \ll Z\)</span>将<span class="math inline">\([-Z, Z]\)</span>分成三个区间<span class="math inline">\([-Z,-(Z-\delta)],[-(Z-\delta),
Z-\delta]\)</span>和<span class="math inline">\([Z-\delta,
Z]\)</span>，然后逐个研究限制在每个区间上的核<span class="math inline">\(k_{h, z}(m)\)</span>的性质.由于<span class="math inline">\(\delta \ll Z\)</span>，低秩逼近(3.8)仍然有效</p>
<p><em>离散化</em>
：到目前为止的分析都是在连续设定下进行的。一个简单的数值方法是用均匀的笛卡尔网格对区域<span class="math inline">\(\Omega\)</span>进行离散，拉普拉斯算子用5点中心差分格式逼近，边界上的方向导数用单边一阶差分代替。数值格林函数定义为零边界条件离散拉普拉斯算子的逆。令<span class="math inline">\(N_r\)</span>为电极的个数。通过求解(1.2) <span class="math inline">\(N_r\)</span>次来评估DtN图，每次将<span class="math inline">\(f ( x
)\)</span>设置为一个电极处的delta函数。在稍微滥用表示法的情况下，用相同的字母表示连续核及其离散化。(3.8)的离散版本:
<span class="math display">\[
\mu_h(m) \approx \sum_{\hat{h}} R_{h, \hat{h}}\left(\sum_{\hat{z}}
k_{\hat{h}, \hat{z}} *\left(\sum_z R_{z, \hat{z}}
\eta_z\right)\right)(m) \qquad(3.9)
\]</span></p>
<p>神经网络架构。微扰分析表明，当<span class="math inline">\(\eta&gt;0\)</span>充分小时，前向映射<span class="math inline">\(\eta \rightarrow
\mu\)</span>可以用(3.9)式近似。具体来说，计算(3.9)的三个步骤可以自然地表示为一个具有三个模块的神经网络：
- 编码模块将二维数据<span class="math inline">\(\eta\)</span>压缩为一组一维数据<span class="math inline">\(\tilde{\eta}_{\hat{z}}\)</span> - 将<span class="math inline">\(k_{\hat{h}, \hat{z}}\)</span>与一维数据<span class="math inline">\(\tilde{\eta}_{\hat{z}}\)</span>卷积得到<span class="math inline">\(\tilde{\mu}_{\hat{h}}\)</span>的中间模块 -
一个将一维数据集合<span class="math inline">\(\tilde{\mu}_{\hat{h}}\)</span>扩展到二维数据<span class="math inline">\(\mu\)</span>的解码模块</p>
<p>当<span class="math inline">\(\eta\)</span>不能充分小时，前向映射<span class="math inline">\(\eta \rightarrow
\mu\)</span>的线性近似是不准确的。为了将式(3.9)的神经网络扩展到非线性情况，一个直接的解决方法是包含非线性激活函数并增加层数，例如在[23、21]中。为了简单起见，我们假设集合
<span class="math inline">\(\{\hat{z}\}\)</span>的大小<span class="math inline">\(N_{\hat{z}}\)</span>和<span class="math inline">\(\{\hat{h}\}\)</span>的大小<span class="math inline">\(N_{\hat{h}}\)</span>都等于一个常数参数<span class="math inline">\(c\)</span></p>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法1} \quad 单边检测前向映射\eta \rightarrow \mu 的神经网络
\\
   \hline
   \qquad \textbf{需要：}c=N_{\hat{z}}=N_{\hat{h}}, n_{\text {cnn }} \in
\mathbb{N}, \eta \in \mathbb{R}^{N_x \times N_z} \\
   \qquad \textbf{确保：}\mu \in \mathbb{R}^{N_m \times N_h}\\
   \qquad 1: \tilde{\eta} \leftarrow \text{Encoding} [c](\eta) \\
   \qquad 2: \tilde{\mu} \leftarrow \text{BCR-Net1d} \left[c, n_{\text
{cnn }}\right](\tilde{\eta})\\
   \qquad 3: \mu \leftarrow
\text{Decoding}\left[N_h\right](\tilde{\mu})\\
   \qquad 4: \text{return} \, \mu \\
   \hline
\end{array}
\]</span></p>
<p><img src="/2025/03/17/Solving-electrical-impedance-tomography-with-deep-learning/2.png" alt="image">
在算法1中总结了用于单边检测的前向映射的神经网络结构，并在图2中进行了说明。让我们对神经网络的这三个组成部分一一进行解释</p>
<ul>
<li><p>编码模块：<span class="math inline">\(\tilde{\eta}=\)</span>
Encoding <span class="math inline">\([c](\eta)\)</span>仅在<span class="math inline">\(z\)</span>维压缩<span class="math inline">\(\eta
\in \mathbb{R}^{N_x \times N_z}\)</span>到<span class="math inline">\(\tilde{\eta} \in \mathbb{R}^{N_x \times
c}\)</span>。它可以用一维卷积层Conv1d实现，窗口大小为1，通道数为<span class="math inline">\(c\)</span>，取<span class="math inline">\(\eta\)</span>的第二个维度作为通道。线性激活函数对于这里使用的Conv1d层是足够的</p></li>
<li><p>中间模块：由于(3.9)中的线性情况下的核<span class="math inline">\(k_{\hat{h},
\mathcal{z}}\)</span>是一个卷积，它可以由一个一维卷积层Conv1d实现，其中窗口大小<span class="math inline">\(N_x\)</span>，通道数<span class="math inline">\(c\)</span>和线性激活函数。对于非线性情况，一个自然的扩展是使用多个卷积层，并在每一层后添加非线性激活函数，如整流线性单元(ReLU)函数。对于精细离散化的问题，一个窗口大小为<span class="math inline">\(N_x\)</span>的卷积层可能有很多参数。最近，一些具有更少参数的多尺度神经网络被提出作为全宽卷积层的有效替代者。实例包括[23、22]中基于层次矩阵的方法和BCR-Net
[21]中基于层次矩阵的方法。这里，使用BCR-Net来表示中间模块。BCR-Net是基于拟微分算子的数据稀疏非标准小波表示[10]提出的。它将不同尺度下的信息分开处理，每个尺度可以理解为一个局部卷积神经网络。一维<span class="math inline">\(\tilde{\mu}=\text{ BCR-Net1d}\left[c, n_{\text
{cnn }}\right](\tilde{\eta})\)</span>将<span class="math inline">\(\tilde{\eta} \in \mathbb{R}^{N_x \times
c}\)</span>映射到<span class="math inline">\(\tilde{\mu} \in
\mathbb{R}^{N_x \times
c}\)</span>，其中每个尺度下局部卷积神经网络的通道数和层数分别为<span class="math inline">\(c\)</span>和<span class="math inline">\(n_{\mathrm{cnn}}\)</span>。读者可参见[21]，以了解BCR-Net的更多细节。</p></li>
<li><p>解码模块：<span class="math inline">\(\mu=
\text{Decoding}\left[N_h\right](\tilde{\mu})\)</span>将一维数据集<span class="math inline">\(\tilde{\mu} \in \mathbb{R}^{N_m \times
c}\)</span>解码为二维数据集<span class="math inline">\(\mu \in
\mathbb{R}^{N_m \times
N_h}\)</span>。在实现中，该解码模块由窗口大小为1、通道数为<span class="math inline">\(N_h\)</span>、线性激活函数的一维卷积层Conv1d实现。</p></li>
</ul>
<h3 id="用于单边检测的逆映射">用于单边检测的逆映射</h3>
<p>扰动分析表明，如果<span class="math inline">\(\eta\)</span>充分小，前向映射可以被很好地逼近</p>
<p><span class="math display">\[
\mu \approx K \eta \qquad(3.10)
\]</span></p>
<p>这是离散化（3.2）的运算符表示法。这里，<span class="math inline">\(\eta\)</span>是由( <span class="math inline">\(x,
z\)</span> )索引的向量，<span class="math inline">\(\mu\)</span>是由(
<span class="math inline">\(m, h\)</span> )索引的向量，<span class="math inline">\(K\)</span>是由( <span class="math inline">\(m,
h\)</span> )索引的行和( <span class="math inline">\(x, z\)</span>
)索引的列组成的矩阵.通常的滤波反投影算法[32]采用这种形式</p>
<p><span class="math display">\[
\eta \approx\left(K^{\top} K+\varepsilon I\right)^{-1} K^{\top} \mu
\]</span> 其中<span class="math inline">\(\varepsilon\)</span>是一个正则化参数。</p>
<p>在上述讨论之后，应用于<span class="math inline">\(K\)</span>的降维近似对<span class="math inline">\(K^{\top}\)</span>也是有效的 <span class="math display">\[
\left(K^{\top} \mu\right)_z(x) \approx \sum_{\hat{z}} R_{z,
\hat{z}}\left(\sum_{\hat{z}} k_{\hat{h}, \hat{z}} *\left(\sum_h R_{h,
\hat{h}} \mu_h\right)\right)(x)
\]</span></p>
<p>因此，我们得到了一个类似的将<span class="math inline">\(K^{\top}\)</span>应用于<span class="math inline">\(\mu\)</span>的三步算法，该算法也可以表述为一个具有三个模块的神经网络：
- 从<span class="math inline">\(\mu\)</span>编码到<span class="math inline">\(\tilde{\mu}_{\hat{h}}=\sum_h R_{h, \hat{h}}
\mu_h\)</span></p>
<ul>
<li><p>卷积形成<span class="math inline">\(\tilde{\eta}_{\hat{z}}=\sum_{\hat{h}} k_{\hat{h},
2} * \tilde{\mu}_{\hat{h}}\)</span></p></li>
<li><p>从<span class="math inline">\(\tilde{\eta}_{\hat{z}}\)</span>解码到<span class="math inline">\(\left(K^{\top} \mu\right)_z=\sum_{\hat{z}} R_{z,
\hat{z}} \tilde{\eta}_{\hat{z}}\)</span></p></li>
</ul>
<p><span class="math inline">\(\left(K^{\top} K+\varepsilon
I\right)^{-1}\)</span>部分可以看作是对<span class="math inline">\(K^{\top} \mu\)</span>的后处理。<span class="math inline">\(K\)</span>(3.3)的定义意味着算子( <span class="math inline">\(K^{\top} K+\varepsilon I\)</span>
)是卷积算子。作为反卷积算子，<span class="math inline">\(\left(K^{\top}
K+\varepsilon I\right)^{-1}\)</span>也可以用卷积神经网络来实现。</p>
<p>结合这两个组件表明，对于逆映射，合适的架构是前向映射的NN架构，然后是2d卷积神经网络。逆映射的神经网络架构在算法2中进行了概述，如图3所示。算法2中的层与算法1中的层共享相同的定义，除了CNN2d层，其定义如下。</p>
<ul>
<li>后处理模块：<span class="math inline">\(\eta=\mathrm{CNN} 2
\mathrm{~d}\left[w, n_{\mathrm{cnn2}}\right](\bar{\eta})\)</span>将<span class="math inline">\(\bar{\eta} \in \mathbb{R}^{N_x \times
N_z}\)</span>映射为<span class="math inline">\(\eta \in \mathbb{R}^{N_x
\times N_z}\)</span>，是一个具有<span class="math inline">\(n_{\text
{cnn2 }}\)</span>个卷积层的二维卷积神经网络，<span class="math inline">\(w\)</span>为窗口大小。ReLU被用作所有中间层的激活函数。然而，由于<span class="math inline">\(\eta\)</span>可以取任意实数，最后一层采用线性激活函数。</li>
</ul>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法2} \quad 单边检测逆映射\eta \rightarrow \mu 的神经网络 \\
   \hline
   \qquad \textbf{需要：}c, w, n_{\mathrm{cnn}}, n_{\mathrm{cnn} 2} \in
\mathbb{N}, \mu \in \mathbb{R}^{N_m \times N_h}\\
   \qquad \textbf{确保：}\eta \in \mathbb{R}^{N_x \times N_z}\\
   \qquad 1: \tilde{\mu} \leftarrow \text { Encoding }[c](\mu) \\
   \qquad 2: \tilde{\eta} \leftarrow \text { BCR-Net1d }\left[c,
n_{\text {cnn }}\right](\tilde{\mu})\\
   \qquad 3: \bar{\eta} \leftarrow
\operatorname{Decoding}\left[N_z\right](\tilde{\eta})\\
   \qquad 4: \eta \leftarrow \operatorname{CNN} 2 \mathrm{~d}\left[w,
n_{\text {cnn2 }}\right](\bar{\eta}) \\
   \qquad 5: \text{return} \,\eta \\
   \hline
\end{array}
\]</span></p>
<figure>
<img src="/2025/03/17/Solving-electrical-impedance-tomography-with-deep-learning/3.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>EIT</tag>
      </tags>
  </entry>
  <entry>
    <title>最优传输(Optimal Transportation)</title>
    <url>/2025/02/09/OT1/</url>
    <content><![CDATA[<h2 id="什么是最优传输">什么是最优传输？</h2>
<p>最优传输最开始由Monge于1781年提出。一个典型的Monge问题是考虑将一堆具有一定形状的沙子搬运到指定的另外一个形状所需要的具有最小代价的搬运方法。
如下图所示，我们想将左边红色区域的沙堆搬运到右边，形成右边绿色的沙堆的形状。我们想要找到消耗最少的搬运方式。
<img src="/2025/02/09/OT1/1.jpg" alt="image">
一句话来概括，就是<strong>如何用最少的代价将一个质量分布转为另一个质量分布。</strong>
<span id="more"></span></p>
<h2 id="质量分布">质量分布</h2>
<p>质量分布其实就是两个测度空间<span class="math inline">\((X,\mu),(Y,\nu)\)</span>。一般情况下，质量不会凭空产生，所以我们会要求这两个分布的“总质量”是一样的，即<span class="math inline">\(\int_x d\mu=\int_Yd\nu\)</span>。
问题有了考虑的对象，我们还可以定义成本函数<span class="math inline">\(c(x,y):X \times Y \to \mathbb
R^+\)</span>，一般是有界的，来衡量将质量从点<span class="math inline">\(x\)</span>运到点<span class="math inline">\(y\)</span>的成本。那么如何去进行移动？主要有两个角度去考虑，分别是Monge问题和Kantorovich问题。</p>
<h2 id="monge问题">Monge问题</h2>
<p>Monge问题就是寻找一个保测度的映射<span class="math inline">\(T:X \to
Y\)</span> <span class="math display">\[
\underset{T} {\min} \int_X c(x,T(x))d \mu(x),T_{ \# } \mu= \nu
\]</span> <span class="math inline">\(T_{
\#  }\)</span>是前推算子（<span class="math inline">\(T_{ \#
}\)</span>的作用对象是<span class="math inline">\(\mu\)</span>，表示把测度<span class="math inline">\(\mu\)</span>推到<span class="math inline">\(\nu\)</span>），这个“推”的过程就是一个保测度的过程，即
<span class="math display">\[
T_{ \#  }\mu = \nu \iff \forall B \subset Y,\nu (B)=\mu (T^{-1}(B))
\]</span> <img src="/2025/02/09/OT1/2.jpg" alt="Monge Map">
但是映射的定义就限制了我们不能实现“一对多”的操作，这就导致了一个很严重的问题，Monge问题不一定有解。比如一个狄拉克分布（在包含某个点的集合测度是1，其余是0）就不可能保测度地映射到高斯分布。
我们可以让质量“可分”，即以概率的形式去进行“移动”。这就是Kantorovich问题。</p>
<h2 id="kantorovich问题">Kantorovich问题</h2>
<p>在Kantorovich问题中，Kantorovich问题中，我们对Monge问题进行松弛，不再寻找一个映射，而是寻找一个联合分布（耦合coupling），其中它的边界分布分别是<span class="math inline">\(\mu,\nu\)</span>。从而最小化总成本 <span class="math display">\[
\underset{\pi}{\min}\int_{X \times Y}c(x,y)d\pi(x,y),P_{x
\#  }\pi=\mu,P_{y \# }\pi=\nu
\]</span> <img src="/2025/02/09/OT1/3.jpg" alt="Kantorovich Relaxation"> <span class="math inline">\(\pi(x,y)\)</span>就是从<span class="math inline">\(x\)</span>移动到<span class="math inline">\(y\)</span>的概率，上式是总成本的期望。
这样的松弛之后，Kantorovich本质上变成了一个无限维的线性规划问题（如果分布是离散的，比如一堆点，我们要做的就是在两个点云之间做matching，那么<span class="math inline">\(\pi\)</span>就变成了一个矩阵，就变成了有限维的线性规划问题）
线性规划理论告诉我们，如果耦合集合非空且紧，目标函数是下半连续的，那么线性规划一定可以取到最小值。也就保证了Kantorovich一定有解。</p>
<h2 id="分布之间的度量-wasserstein距离">分布之间的度量-Wasserstein距离</h2>
<p>比较两种分布的一些方法：</p>
<p><strong>交叉熵</strong>：对应分布为<span class="math inline">\(p(x)\)</span>的随机变量，熵<span class="math inline">\(H(p)\)</span>表示其最优编码长度。交叉熵是按照概率分布<span class="math inline">\(q\)</span>的最优编码对真实分布为<span class="math inline">\(p\)</span>的信息进行编码的长度 交叉熵定义为 <span class="math display">\[
H(p,q)=E_q[-logq(x)]=-\displaystyle\sum_{x}p(x)logq(x)
\]</span> 在给定<span class="math inline">\(p\)</span>的情况下，如果<span class="math inline">\(q\)</span>和<span class="math inline">\(p\)</span>越接近，交叉熵越小；如果<span class="math inline">\(q\)</span>和<span class="math inline">\(p\)</span>越远，交叉熵越大</p>
<p><strong>KL散度</strong>:是用概率分布<span class="math inline">\(q\)</span>来近似<span class="math inline">\(p\)</span>时所造成的信息损失量。KL散度是按照概率分布<span class="math inline">\(q\)</span>的最优编码对真实分布为<span class="math inline">\(p\)</span>的信息进行编码，其平均编码长度<span class="math inline">\(H(p,q)\)</span>和<span class="math inline">\(p\)</span>的最优平均编码长度<span class="math inline">\(H(p)\)</span>之间的差异。对于离散概率分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>，从<span class="math inline">\(q\)</span>到<span class="math inline">\(p\)</span>的KL散度定义为: <span class="math display">\[
D_{KL}(p\parallel q)=H(p,q)-H(p)=\sum_x p(x)log{\frac {p(x)} {q(x)} }
\]</span>
KL散度可以是衡量两个概率分布之间的距离。KL散度总是非负的，只有当<span class="math inline">\(p=q\)</span>时，<span class="math inline">\(D_{KL}(p\parallel
q)=0\)</span>。两个分布越接近，KL散度越小；两个分布越远，KL散度越大。但KL散度并不是一个真正的度量或距离，一是KL散度不满足距离的对称性，二是KL散度不满足距离的三角不等式性质。</p>
<p><strong>JS散度</strong>：
JS散度是一种对称的衡量两个分布相似度的度量方式，定义为 <span class="math display">\[
D_{JS}(p\parallel q)={\frac 1 2}D_{KL}(p \parallel m)+{\frac 1
2}D_{KL}(q \parallel m)
\]</span> 其中，<span class="math inline">\(m={\frac 1 2}(p+q)\)</span>
JS 散度是KL散度一种改进。但两种散度都存在一个问题，即如果两个分布p, q
没有重叠或者重叠非常少时，KL散度和JS 散度都很难衡量两个分布的距离</p>
<p><strong>Wasserstein距离</strong>： Wasserstein 距离（Wasserstein
Distance）也用于衡量两个分布之间的距离。对于两个分布<span class="math inline">\(q_1,q_2,p-Wasserstein\)</span>距离定义为 <span class="math display">\[
W_p(q_1,q_2)=(\underset{\pi(x,y) \in U(x,y)} {\inf}E_{(x,y)\sim \pi
(x,y)}[d(x,y)^p]) ^{1/p}
\]</span> 其中，<span class="math inline">\(U(x,y)\)</span>是边际分布为<span class="math inline">\(q_1\)</span>和<span class="math inline">\(q_2\)</span>的所有可能的联合分布集合，<span class="math inline">\(d(x,y)\)</span>为<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>的距离 Wasserstein距离相比KL散度和JS
散度的优势在于：即使两个分布没有重叠或者重叠非常少，Wasserstein
距离仍然能反映两个分布的远近。</p>
<p>我们可以发现，如果令<span class="math inline">\(d(x,y)^p=c(x,y)\)</span>，Wasserstein距离实际上就是从一个分布转换为另一个分布所要付出的代价。</p>
<p>Wasserstein
GAN就是将W-1距离作为损失函数，解决了GAN的许多问题，比如训练不稳定，判别器不能训练的“太好”等。究其原因主要是因为W-1度量比KL度量更“弱”：也就是说在K-L散度下收敛的序列在W-1距离下也一定收敛。这样的性质就保证了W-1可以捕捉到序列更多的几何信息（比如不重叠的分布的KL散度永远是0，但W-1距离不然。），训练会更鲁棒。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>最优化</tag>
        <tag>最优运输</tag>
      </tags>
  </entry>
  <entry>
    <title>The_Perception-Distortion_Tradeoff</title>
    <url>/2025/03/02/The-Perception-Distortion-Tradeoff/</url>
    <content><![CDATA[<h1 id="the-perception-distortion-tradeoff感知-失真权衡">The
Perception-Distortion Tradeoff（感知-失真权衡）</h1>
<p>会议： CVPR</p>
<p>时间： 2018</p>
<h2 id="摘要">摘要</h2>
<p>图像复原算法通常是通过某种失真测度(例如：PSNR、SSIM、IFC、VIF)或通过量化感知质量的人类主观评分来评估的。在本文中，我们从数学上证明了失真和感知质量是相互矛盾的。具体来说，<strong>我们研究了从真实图像中正确判别图像复原算法输出的最佳概率</strong>。<strong>我们证明，随着平均扭曲的减小，这个概率必定增加(说明感知质量较差)</strong>。与通常的信念相反，这个结果对任何失真测度都是正确的，而不仅仅是PSNR或SSIM标准的问题。然而，正如我们在实验中所显示的那样，对于某些措施来说，(例如：VGG特征之间的距离)并不那么严重。我们还表明，生成对抗网络(Generative-adversarial-nets，GANs)为接近感知-失真界提供了一种原则性的方法。
这构成了他们在低级视觉任务中观察到的成功的理论支持。基于我们的分析，我们提出了一种新的评估图像复原方法的方法，并使用它对最近的超分辨率算法进行了广泛的比较。</p>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<p>在过去的几十年中，图像复原算法(例如去噪,去模糊,超分辨率等)在视觉质量和失真指标如峰值信噪比(PSNR)和结构相似性指数(SSIM)方面都取得了不断的进步[45]。然而，近年来，重建精度的提高似乎并不总是伴随着视觉质量的提高。事实上，也许是反直觉的，在感知质量方面占优势的算法，往往在例如PSNR和SSIM等方面处于劣势[22、16、6、38、51、49]。这种现象通常被解释为现有失真测度的一个缺点[44]，它推动了对替代的"更感性"标准的不断搜索。
在本文中，我们对感知质量和失真测度之间的表观权衡提供了一个补充的解释。<strong>具体来说，我们证明了感知-失真平面中存在一个区域，无论算法方案(见图1)如何，这个区域都是无法达到的。此外，该区域的边界是单调的。因此，在其附近，只有可能改善感知质量或失真，其中一个是以牺牲另一个为代价的</strong>。对于所有的失真测度，感知-失真权衡是存在的，而不仅仅是均方误差(
MSE
)或SSIM标准的问题。<strong>然而，对于某些措施而言，这种权衡要弱于其他措施。例如，我们通过实验发现最近提出的深度网络特征之间的距离[16、22]与感知质量之间的权衡弱于MSE。这与该度量比MSE更"感性"的观察一致</strong>。</p>
<figure>
<img src="/2025/03/02/The-Perception-Distortion-Tradeoff/The-Perception-Distortion-Tradeoff\1.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>让我们来厘清失真与感知质量的区别。图像复原的目标是从退化图像<span class="math inline">\(y\)</span> (例如,噪声、模糊等)中估计出图像<span class="math inline">\(x\)</span>。失真是指重建图像<span class="math inline">\(x\)</span>与原始图像<span class="math inline">\(\hat{x}\)</span>之间的差异。感知质量，另一方面，仅指<span class="math inline">\(\hat{x}\)</span>的视觉质量，而不考虑它与<span class="math inline">\(x\)</span>的相似性。也就是说，它是<span class="math inline">\(\hat{x}\)</span>看起来像一个有效的自然图像的程度。一种越来越流行的测量感知质量的方法是使用real-vs.-fake用户研究，该研究考察了人类观察者判断<span class="math inline">\(\hat{x}\)</span>是真实的还是算法[15、53、39、8、6、14、54、11]
(类似于生成对抗网络的思想[10])。因此，感知质量可以被定义为在这种鉴别实验中的最佳成功概率，正如我们所显示的那样，它<strong>正比于<span class="math inline">\(\hat{x}\)</span>的分布与自然图像的分布之间的距离</strong>。
基于这些关于感知和失真的定义，我们遵循率失真理论的逻辑[4]。也就是说，我们寻求将最佳可达到的感知质量(对自然图像统计的最小偏差)的行为描述为最大允许平均失真的函数，对于任何估计量。该感知-失真函数(图1中的宽曲线)分离了感知-失真平面中的可达区域和不可达区域，从而描述了感知和失真之间的基本权衡。我们的分析表明，算法不可能同时非常准确，并产生愚弄观察者相信它们是真实的图像，不管用什么方法来量化精度。
这种权衡意味着优化失真测度不仅是无效的，而且在视觉质量方面可能是有害的。这已经在[
22、16、38、51、6]中得到了经验观察，但从未在理论上得到证实。
从算法设计的角度，我们证明了生成对抗网络(GANs)提供了一种接近感知-失真边界的原则性方法。这为GANs在图像复原[22、38、35、51、36、15、55]上的优势提供了越来越多的实证支持。
感知-失真权衡对低层视觉有重要影响。在某些应用中，重建精度是非常重要的。在另一些情况下，感知质量可能更受欢迎。同时实现这两个目标的不可能性呼吁了一种新的评估算法的方式：将它们放置在感知-失真平面上。我们使用这种新的方法对最近的超分辨率(
SR )方法进行了广泛的比较，揭示了哪种SR方法最接近感知-失真界。</p>
<h2 id="问题描述">问题描述</h2>
<p>从统计学意义上讲，一幅自然图像<span class="math inline">\(x\)</span>可以看作是由自然图像<span class="math inline">\(p_X\)</span>的分布实现的。在图像复原中，我们通过某种条件分布<span class="math inline">\(p_{Y|X}\)</span>
(对应于噪声、模糊、下采样等。)来观察一个关于<span class="math inline">\(x\)</span>的退化版本<span class="math inline">\(y\)</span>。给定<span class="math inline">\(y\)</span>，我们根据某种分布<span class="math inline">\(p_{\hat{X}|Y}\)</span>产生一个估计量<span class="math inline">\(\hat{x}\)</span>
。这种描述很一般，因为它不限制估计量<span class="math inline">\(\hat{x}\)</span>是<span class="math inline">\(y\)</span>的确定性函数。该问题设置如图2所示。</p>
<figure>
<img src="/2025/03/02/The-Perception-Distortion-Tradeoff/The-Perception-Distortion-Tradeoff\2.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>给定一个全参考相异性准则<span class="math inline">\(\Delta(x,\hat{x})\)</span>，估计量<span class="math inline">\(\hat{X}\)</span>的平均偏差由下式给出 <span class="math display">\[
\mathbb{E}[\Delta(X, \hat{X})] \qquad (2)
\]</span> 其中，期望关于联合分布<span class="math inline">\(p_{X,\hat{X}}\)</span>。该定义与在退化自然图像数据库中评估平均性能的通常做法一致。注意到一些失真措施，例如.
SSIM，实际上是(越高越好)的相似性度量，但总是可以反转成为相异性度量。
估计<span class="math inline">\(\hat{X}\)</span>的感知质量(如通过真实和虚假的人类观点研究进行量化)与它的重建图像的分布<span class="math inline">\(p_{\hat{X}}\)</span>和自然图像的分布<span class="math inline">\(p_X\)</span>之间的距离直接相关。因此，我们定义估计<span class="math inline">\(p_{\hat{X}}\)</span>的感知质量指标(越低越好)为
<span class="math display">\[
d\left(p_X, p_{\hat{X}}\right) \qquad (3)
\]</span> 其中<span class="math inline">\(d(·,·)\)</span>是分布之间的一些散度，例如KL散度，TV距离，Wasserstein距离等。
注意到，当算法的输出遵循自然图像的分布时,即<span class="math inline">\(p_{\hat{X}}=p_X\)</span>，可以获得尽可能好的感知质量。在这种情况下，通过查看重建图像，无法判断它们是由算法生成的。然而，并不是每一个具有这种性质的估计量都一定是准确的。事实上，我们可以通过随机绘制与原始"真实"图像无关的自然图像来达到完美的感知质量。在这种情况下，畸变将相当大。
我们的目标是刻画(2)和(3)之间的权衡。</p>
<h2 id="感知-失真平衡">感知-失真平衡</h2>
<p>我们看到，低失真通常并不意味着良好的感知质量。那么，一个有趣的问题是：给定失真水平的估计器所能达到的最佳感知质量是什么？</p>
<p><strong>定义1</strong> 信号恢复任务的感知-失真函数由下式给出</p>
<p><span class="math display">\[
P(D)=\min _{p_{\hat{X} \mid Y}} d\left(p_X, p_{\hat{X}}\right) \quad
\text { s.t. } \quad \mathbb{E}[\Delta(X, \hat{X})] \leq D \qquad(9)
\]</span></p>
<p>其中<span class="math inline">\(\Delta(\cdot,\cdot)\)</span>是失真测度，<span class="math inline">\(d(\cdot,\cdot)\)</span>是分布之间的散度。
简言之，<span class="math inline">\(P ( D )\)</span>是分布<span class="math inline">\(p_X\)</span>和<span class="math inline">\(p_{\hat{X}}\)</span>之间的最小偏差，它可以通过一个带有偏差<span class="math inline">\(D\)</span>的估计量来获得。为了直观地了解该函数的典型行为，考虑如下例子。</p>
<p><strong>例1</strong> 假设<span class="math inline">\(Y=X+N\)</span>，其中<span class="math inline">\(X
\sim \mathcal{N}(0,1)\)</span>和<span class="math inline">\(N \sim
\mathcal{N}\left(0, \sigma_N\right)\)</span>是相互独立的.取"<span class="math inline">\(\Delta(\cdot,
\cdot)\)</span>"为误差平方失真，<span class="math inline">\(d(\cdot,
\cdot)\)</span>为KL散度。为了简单起见，我们把注意力限制在形如<span class="math inline">\(\hat{X}=a
Y\)</span>的估计量上。在这种情况下，我们可以导出方程（9）的一个封闭形式的解。对图5中的若干噪声水平<span class="math inline">\(\sigma_N\)</span>作图。可以看出，最小可达<span class="math inline">\(d_{\mathrm{KL}}\left(p_X,
p_{\hat{X}}\right)\)</span>随着最大允许失真(MSE)的增大而减小。此外，这种权衡是凸的，并且在更高的噪声水平<span class="math inline">\(\sigma_N\)</span>变得更加严重。</p>
<figure>
<img src="/2025/03/02/The-Perception-Distortion-Tradeoff/The-Perception-Distortion-Tradeoff\3.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>在一般情况下，解析地求解(9)是不可能的。然而，事实证明，如图5所示的行为是典型的，正如我们接下来的(见附录中的证明)所示。</p>
<p><strong>定理1</strong>
(感知-失真权衡)。假设第3节的问题设定。如果式(3)的<span class="math inline">\(d(p,q)\)</span>在其第二个参数中为凸函数（<span class="math inline">\(d\left(p, \lambda q_1+(1-\lambda) q_2\right) \leq
\lambda d\left(p, q_1\right)+(1-\lambda) d\left(p, q_2\right), \forall
\lambda \in[0,1]\)</span>），则式(9)的感知-失真函数<span class="math inline">\(P(D)\)</span>为 1.单调不增 2.凸的</p>
<p>注意到定理1对失真测度<span class="math inline">\(\Delta(·,·)\)</span>不做任何假设。这意味着对于任何失真测度，包括例如MSE、SSIM、VGG特征间的平方误差[
16、22
]等，都存在感知质量和失真之间的权衡。然而，这并不意味着所有的失真测度都具有相同的感知-失真函数。事实上，正如我们在Sec.6，对于捕获图像之间语义相似性的失真度量，这种权衡往往更不严重。</p>
<p><span class="math inline">\(P(D)\)</span>的凸性意味着在低失真和高感知质量的极端情况下，这种权衡更加严重。这一点在考虑与真假图像区分能力相关的TV散度时尤为重要。(见Sec
. 2.2)。由于<span class="math inline">\(P ( D
)\)</span>在低失真的情况下更陡峭，因此对于失真已经很低的算法，任何小的失真改善都必然伴随着对鉴别器的欺骗能力的大幅度降低。同样，对于感知指数已经较低的算法，感知质量的任何微小提升，必然伴随着失真的大幅增加。让我们评论一下，<span class="math inline">\(d( p , q)\)</span>是凸的这一假设，并不是很有限制。
例如，任意的<span class="math inline">\(f\)</span> -散度( e.g. KL , TV
,Hellinger, <span class="math inline">\(\mathcal{X}^2\)</span>)以及Renyi散度，都满足这个假设[5、43]
。在任何情况下，即使没有这个假设，函数<span class="math inline">\(P ( D
)\)</span>也是单调非增的。</p>
<h2 id="通过gan进行折衷">通过GAN进行折衷</h2>
<p>存在一种系统的方法来设计逼近感知-失真曲线的估计器：使用GANs。具体来说，受[22、35、51、38、36、15]的启发，恢复问题可以通过修改GAN的生成器的损失来解决</p>
<p><span class="math display">\[
\ell_{\mathrm{gen}}=\ell_{\text {distortion }}+\lambda
\ell_{\mathrm{adv}} \qquad(11)
\]</span></p>
<p>式中，<span class="math inline">\(\ell_{\text {distortion
}}\)</span>原始图像与重建图像之间的失真，<span class="math inline">\(\ell_{\text {adv
}}\)</span>为标准GAN对抗损失。众所周知，<span class="math inline">\(\ell_{\text {adv
}}\)</span>正比于生成器和数据分布[10、1、34]
(散度的类型取决于损失)之间的某种散度<span class="math inline">\(d\left(p_X,
p_{\hat{X}}\right)\)</span>。因此，(11)实际上是对目标的近似。</p>
<p><span class="math display">\[
\ell_{\text {gen }} \approx \mathbb{E}[\Delta(x, \hat{x})]+\lambda
d\left(p_X, p_{\hat{X}}\right)
\]</span></p>
<p>将<span class="math inline">\(\lambda\)</span>视为拉格朗日乘子，显然，对某个<span class="math inline">\(D\)</span>，最小化<span class="math inline">\(\ell_{\text {gen
}}\)</span>等价于最小化(9)。改变<span class="math inline">\(\lambda\)</span>对应于改变<span class="math inline">\(D\)</span>，从而产生沿感知-失真函数的估计器。</p>
<p>我们用这种方法来探索图4中<span class="math inline">\(\sigma=3\)</span>的数字去噪例子的感知失真权衡。我们训练了一个基于Wasserstein生成式对抗网络(WGAN)的去噪器[1、12]，其MSE失真损失为<span class="math inline">\(\ell_{\text {distortion }}\)</span>。这里，<span class="math inline">\(\ell_{\text {adv
}}\)</span>正比于生成器和数据分布之间的Wasserstein距离<span class="math inline">\(d_W\left(p_X,
p_{\hat{X}}\right)\)</span>。<strong>WGAN的一个有价值的性质是它的鉴别器(评论家)损失是<span class="math inline">\(d_W\left(p_X,
p_{\hat{X}}\right)\)</span>的一个精确估计(直到一个常数因子)
[1]。这使得我们可以很容易地计算训练好的去噪器的感知质量指标</strong>。我们得到具有几个<span class="math inline">\(\lambda
\in[0,0.3]\)</span>值的估计量的集合。对于每个去噪器，我们通过最终的判别器损失来评估感知质量。如图6所示，连接感知-失真平面上的估计量的曲线是单调递减的。此外，它与逐渐从模糊和准确过渡到尖锐和不准确的估计有关。这条曲线显然与解析界
(9)
(用虚线表示)不重合。然而，它似乎与之相邻。<strong>这表现在WGAN曲线的最左端点非常接近理论界的最左端点，这与MMSE估计器相对应</strong>。WGAN训练细节和架构参见附录。</p>
<p>除了MMSE估计器，图6还包括MAP估计器和一个从数据集(记为"随机抽签")中随机抽取图像的估计器。如上所述，通过WGAN判别器[1]的最终损失来评估这3个估计器的感知质量，训练来区分估计器的输出和数据集中的图像。<strong>值得注意的是，去噪WGAN估计器(D)实现了与MAP估计器相同的失真，但具有更好的感知质量。此外，它达到了与随机绘制估计器几乎相同的感知质量，但具有显著更低的失真</strong>。</p>
<figure>
<img src="/2025/03/02/The-Perception-Distortion-Tradeoff/The-Perception-Distortion-Tradeoff\4.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<h2 id="结论">结论</h2>
<p>我们证明并证明了失真和感知质量相互矛盾的反直觉现象。也就是说，算法的失真越小，它的分布就越必须偏离自然场景的统计数据。我们实证表明，许多流行的扭曲测量都存在这种权衡，包括那些被认为与人类感知密切相关的测量。因此，任何单独的失真测量都不适合评估图像恢复方法。我们的新方法利用一对
NR 和 FR
指标将每种算法置于感知失真平面上，从而有助于对图像恢复方法进行更翔实的比较。</p>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>图像恢复</tag>
      </tags>
  </entry>
  <entry>
    <title>Unsupervised Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal Transport</title>
    <url>/2025/02/24/Unsupervised-Noise-Adaptive-Speech-Enhancement-by-Discriminator-Constrained-Optimal-Transport/</url>
    <content><![CDATA[<h1 id="unsupervised-noise-adaptive-speech-enhancement-by-discriminator-constrained-optimal-transport判别器约束最优传输的无监督噪声自适应语音增强">Unsupervised
Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal
Transport（判别器约束最优传输的无监督噪声自适应语音增强）</h1>
<p>会议：NeurIPS</p>
<p>时间：2021</p>
<h2 id="摘要">摘要</h2>
<p>本文提出了一种新的判别器约束的最优传输网络(DOTN)，该网络执行无监督的域自适应语音增强(SE)，这是语音处理中必不可少的回归任务。<strong>DOTN旨在利用从源域获得的知识，在目标域中估计带噪语音的干净参考</strong>。训练和测试数据之间的领域转换已被报道是不同领域学习问题的障碍。尽管有丰富的文献研究无监督域适应分类，但所提出的方法，特别是在回归中，仍然是稀缺的，并且往往依赖于关于输入数据的额外信息。<strong>提出的DOTN方法将数学分析中的最优传输(OT)理论与生成对抗框架进行策略性融合，以帮助评估目标域中的连续标签</strong>。在两个SE任务上的实验结果表明，通过扩展经典的OT公式，我们提出的DOTN以一种纯无监督的方式优于先前的对抗域适应框架。
<span id="more"></span></p>
<h2 id="介绍">介绍</h2>
<p>语音增强(SE)的目标是将低质量的语音信号转换为质量和可懂度提高的语音信号。SE作为语音处理领域的一个重要的回归任务，已被广泛用作语音相关应用的预处理器，如语音编码[1]、自动语音识别(
Automatic Speech Recognition，ASR )
[2]、说话人识别[3]和辅助听觉设备[4、5]。最近机器学习的进展使SE技术取得了重大进展。一般来说，基于学习的SE方法在训练阶段估计从带噪语音信号到干净语音信号的映射函数的变换[6]。估计变换在测试阶段将带噪语音信号转换为类干净信号。各种神经网络模型已被用于表征噪声到干净变换。这类模型的著名例子包括全连接神经网络[7]，深度去噪自编码器[8]，卷积神经网络[9]，长短期记忆网络[10]和Transformer
[11]。为了有效地处理各种噪声情况，我们通常准备相当数量的涵盖各种噪声类型的训练数据来训练SE模型。<strong>然而在实际应用场景中，测试数据中的噪声类型可能并不总是包含在训练集中。因此，从训练数据中学习到的噪声到干净的转换不能很好地应用于处理测试噪声，从而导致有限的增强性能</strong>。这种训练-测试不匹配通常被称为<strong>SE的域不匹配问题</strong>。需要一种有效的解决方案，<strong>通过制定与测试条件相匹配的精确的噪声到干净的转换来执行域适应来调整SE模型</strong>。大多数现有的领域自适应方法至少依赖于以下一种自适应机制：对齐领域不变特征[12、13、14]和对抗训练，其中在训练过程中引入判别器作为领域分类器[15、16、17]。</p>
<p><strong>本研究旨在通过引入最优传输(OT)来解决SE的无监督域适应问题。特别地，我们考虑在目标域上用完全无标签的数据进行SE测试，且只有源域的有标签数据可供参考</strong>。一般而言，OT理论通过比较两个(概率)分布，并考虑介于两者之间的所有可能的运输方案，从而找到一个具有最小位移成本的方案。OT的概念可以用来最小化域失配，从而实现无监督的域自适应。即使使用OT提供的数学特性，由于人类语音具有的复杂结构，获得优异SE性能的障碍仍然存在。为了进一步克服这些障碍，生成对抗网络(Generative
Adversarial
Network，GAN)的另一个概念被整合来帮助实现复杂的SE域自适应。虽然现有的域转换技术"域对抗训练"和我们的提案在名称上有相似之处，但其基本结构有本质的不同。该方法的一个关键要素在于<strong>一个用于检测语音输出特征的判别器，而不是一个域分类器</strong>。更准确地说，我们的方法中的<strong>判别器通过学习源标签的概率分布来控制输出语音质量</strong>。这种新颖的方法专为无监督的SE域自适应而设计，表现出优异的性能，并在VoiceBank和TIMIT数据集上进行了验证。</p>
<h3 id="贡献">贡献</h3>
<p>我们<strong>提出了一种新的方法，专门用于回归环境下的无监督域适应</strong>。这一领域的研究成果仍然十分有限；此外，现有的方法往往需要对源域进行额外的分类，或者可能尚未得到强回归应用的支持。相反，我们的方法不需要源样本、源标签和目标样本以外的任何额外的输入信息。我们的方法被应用于两个标准化SE任务，即VoiceBank
-
DEMAND和TIMIT，并在语音质量感知评价(PESQ)和短时客观可懂度(STOI)评分方面取得了优异的自适应性能。
此外，由于输入要求简单，我们可以通过增加目标域中允许的噪声类型的数量来很容易地研究目标样本复杂度对我们方法的影响，这在我们所知之前的文献中没有被报道过。</p>
<h2 id="相关工作">相关工作</h2>
<h3 id="对抗域适应adversarial-domain-adaptation">对抗域适应（Adversarial
domain adaptation）</h3>
<p>领域对抗训练( Domain Adversarial Training，DAT
)的主要目标是<strong>通过利用来自目标领域的大量未标记数据，训练出能够适应其他相似领域的深度模型(从源域)</strong>[15、18]。传统的DAT系统由<strong>深度特征提取器、标签预测器和领域分类器</strong>三部分组成。通过使用梯度反转层，提取的深度特征对主要学习任务具有判别性，并且在源域和目标域之间的转换具有不变性。DAT方法已被应用和证实可以有效地补偿许多任务中源(训练时间)和目标(测试时间)条件的不匹配，例如语音信号处理[19、20]、图像处理[15、21]和可穿戴传感器信号处理[22]。DAT方法已被应用和证实可以有效地<strong>补偿许多任务中源(训练时间)和目标(测试时间)条件的不匹配</strong>，例如语音信号处理[19、20]、图像处理[15、21]和可穿戴传感器信号处理[22]。后来发展的多源域对抗网络(MDAN)
[23]扩展了原有的DAT，解除了单域迁移的约束，利用多个域分类器为主要学习任务提取有判别力的深度特征，同时对多个域迁移[3、24]具有不变性。</p>
<h3 id="域适应的最优传输optimal-transport-for-domain-adaptation">域适应的最优传输（Optimal
transport for domain adaptation）</h3>
<p>迄今为止，OT
[25、26]已被用于域适应[27、28]，并取得了相关的分析结果[29]。不仅如此，在[30、31]的联合分配框架下，OT的概念被证明是更有用的。最近，为了提高OT对异常值的敏感性，Robust
OT被提出[32]。此外，还提出了一种将对抗域适应的概念与OT和边界分离相结合的方法[33]。然而，在这些研究中进行的几乎所有实验都是分类问题，不像SE任务是我们研究的重点。</p>
<h3 id="语音增强中的域适应domain-adaptation-in-speech-enhancement">语音增强中的域适应（Domain
adaptation in speech enhancement）</h3>
<p>现有的SE方法中的域适应可以分为两类：有监督和无监督。对于有监督域自适应，测试条件中的成对噪声和干净语音信号可用来调整SE模型中的参数。在[34、35]中，已经提出了基于迁移学习的方法来适应SE模型，以缓解语料不匹配。为了克服灾难性遗忘问题，Lee等人在对SE模型进行预形成域适应时，提出了一种结合曲率正则化和路径优化增强策略的SERIL算法[36]。相反，对于无监督域适应，只提供带噪语音信号，而相应的干净对应物是无法获得的。一般而言，无监督域适应对现实场景具有较好的适用性。在[37]中，SE的无监督域适应是通过最小化教师分类器和学生分类器产生的后验概率之间的库尔贝克-莱布勒散度来实现的，而没有成对的噪声-干净的适应数据。在[19、38]中，DAT方法用于使SE模型适应新的噪声条件。
<strong>尽管现有的无监督领域自适应方法取得了很好的性能，但需要额外的信息</strong>，例如单词标签、语言模型和噪声类型标签。在本文中，我们提出了一种新的方法：判别器约束的OT网络(
DOTN
)来对SE进行无监督域适应。与相关工作不同，<strong>DOTN在适应原始SE模型以匹配新的噪声条件时不需要额外的标签信息</strong>。我们的实验表明，DOTN能够有效地将SE模型适应于新的测试条件，并且取得了比之前的对抗域适应方法更好的适应性能，这些对抗域适应方法需要额外的噪声类型信息。</p>
<h2 id="方法">方法</h2>
<h3 id="问题设置与注释">问题设置与注释</h3>
<p>考虑具有配对数据的源域<span class="math inline">\(\left(\mathbf{X}^s,
\mathbf{Y}^s\right)=\left\{\left(\mathbf{x}_i^s,
\mathbf{y}_i^s\right)\right\}_{i=1}^{N_s}\)</span>，其中<span class="math inline">\(\mathbf{x}_i^s \in \mathbb{R}^n, \mathbf{y}_i^s
\in \mathbb{R}^m\)</span>表示样本<span class="math inline">\(i\)</span>的输入和对应的标签。无监督域适应假设存在另一个目标域，只包含未标记的数据，<span class="math inline">\(\mathbf{X}^t=\left\{\mathbf{x}_i^t \in
\mathbb{R}^n\right\}_{i=1}^{N_s}\)</span>。目标是根据源域提供的知识，为目标标签<span class="math inline">\(\mathbf{Y}^t=\left\{\mathbf{y}_i^t\right\}_{i=1}^{N_t}\)</span>(存在但不为人知)寻求一个真值估计器(或统计假设)<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span>。</p>
<p>数据集<span class="math inline">\(\mathcal{D}\)</span>的概率分布用<span class="math inline">\(\mathbb{P}_{\mathcal{D}}\)</span>表示，这里<span class="math inline">\(\mathcal{D}\)</span>要么是<span class="math inline">\(\mathbf{X}^s, \mathbf{Y}^s,
\mathbf{X}^t\)</span>或<span class="math inline">\(\mathbf{Y}^t\)</span>在我们的讨论中。我们的问题是寻找一个函数<span class="math inline">\(f\)</span>，使得<span class="math inline">\(f\)</span>在<span class="math inline">\(\mathbf{Y}^t\)</span>中诱导一个概率分布<span class="math inline">\(\mathbb{P}_{f\left(\mathbf{X}^t\right)}\)</span>，在一定测度下有<span class="math inline">\(\mathbb{P}_{f\left(\mathbf{X}^t\right)}
\rightarrow
\mathbb{P}_{\mathbf{Y}^t}\)</span>。我们提出使用OT的概念来解决这个问题。</p>
<h3 id="提出模型判别器约束的最优运输网络-discriminator-constrained-optimal-transport-networkdotn">提出模型：判别器约束的最优运输网络
Discriminator-Constrained Optimal Transport Network(DOTN)</h3>
<p>给定一对分布<span class="math inline">\(\mathbb{P}_{\mathcal{D}_1}\)</span>和<span class="math inline">\(\mathbb{P}_{\mathcal{D}_2}\)</span>和一个置换成本矩阵<span class="math inline">\(C \geq 0\)</span>，OT求解运输计划<span class="math inline">\(\gamma \in \prod\left(\mathbb{P}_ {\mathcal{D}_
1},
\mathbb{P}_{\mathcal{D}_2}\right)\)</span>使得总成本(在离散的设定下)最小</p>
<p><span class="math display">\[
\min _{\gamma \in \Pi\left(\mathbb{P}_{\mathcal{D}_1},
\mathbb{P}_{\mathcal{D}_2}\right)}\langle C, \gamma\rangle_F,
\]</span></p>
<p>其中<span class="math inline">\(\prod\left(\mathbb{P}_{\mathcal{D}_1},
\mathbb{P}_{\mathcal{D}_2}\right)\)</span>表示边际为<span class="math inline">\(\mathbb{P}_{\mathcal{D}_1}\)</span>和<span class="math inline">\(\mathbb{P}_{\mathcal{D}_1}\)</span>的联合分布空间，<span class="math inline">\(\langle\cdot,
\cdot\rangle_F\)</span>为Frobenius积，并且<span class="math inline">\(C\)</span>的项<span class="math inline">\(C_{i
j}\)</span>表示第i个和第j个样本的置换成本。可以证明，这个问题的最小值是一个距离，并且当相应的代价是范数[25、26]时称为Wasserstein距离</p>
<p><strong>我们提出的方法包括两部分：OT对齐和Wasserstein生成对抗网络（WGAN）训练
[39，40]。这两个步骤都基于OT，但是，它们被考虑为两对不同的分布，并采用不同的算法。</strong></p>
<p><strong>通过联合分配最佳运输进行调整</strong>
 我们的适应机制依赖于源域和目标域的联合分布之间的对齐（对于目标域，标签是估计标签）。特别是，我们通过最小化联合分布<span class="math inline">\(\mathbb{P}_{\mathbf{X}^s} \times
\mathbb{P}_{\mathbf{Y}^s}\)</span>和<span class="math inline">\(\mathbb{P}_{\mathbf{X}^t} \times
\mathbb{P}_{f\left(\mathbf{X}^t\right)}\)</span>之间的OT损失来近似<span class="math inline">\(f\)</span>，并选择一个成本矩阵</p>
<p><span class="math display">\[
C_{i
j}=\alpha\left\|\mathbf{x}_i^s-\mathbf{x}_j^t\right\|^2+\beta\left\|\mathbf{y}_i^s-f\left(\mathbf{x}_j^t\right)\right\|^2,
\quad(\alpha, \beta&gt;0) \qquad (2)
\]</span></p>
<p>通过对齐源域和目标域的联合分布，当 OT
寻找每个目标样本最“相似”的源样本时，自然而然地实现了噪声适应。</p>
<p>尽管 OT
为每个样本提供了准确的估计值，但每个估计误差的影响可能会在训练过程中累积，并在不保留语音数据结构的情况下将
f 误导到方便的局部最小值。为了避免这种情况，我们采用了 Wasserstein
生成对抗网络（WGAN）训练来补充和增强我们的适应系统。</p>
<p><strong>输出和源标签的判别训练</strong>
 与我们考虑输入和标签的联合分布的适应不同，我们专注于源标签分布<span class="math inline">\(\mathbb{P}_{\mathbf{Y}^s}\)</span>和输出分布<span class="math inline">\(\mathbb{P}_{f\left(\mathbf{X}^t\right)}\)</span>的WGAN训练。在生成对抗训练的术语中，我们将<span class="math inline">\(f\)</span>视为生成器，并引入基于卷积神经网络的判别器<span class="math inline">\(h\)</span>作为
“评判者”。一般来说，我们使用判别器来决定<span class="math inline">\(f\)</span>的输出是否与源标签
“相似”。从形式上讲，WGAN 算法求解</p>
<p><span class="math display">\[
\min _f \max _{h \in \mathcal{L}}\left\{\mathbb{E}_{y \sim
\mathbb{P}_{\mathbf{Y}^s}}(h(y))-\mathbb{E}_{x \sim
\mathbb{P}_{\mathbf{X}^t}}(h(f(x)))\right\}
\]</span></p>
<p>由Kantorovich-Rubinstein 对偶性 [25]，其中<span class="math inline">\(\mathcal{L}\)</span>是 1-Lipschitz
函数的集合。在这种情况下，在最优判别器下，最小化相对于生成器参数的值函数，使分布<span class="math inline">\(\mathbb{P}_{\mathbf{Y}^s}\)</span>和<span class="math inline">\(\mathbb{P}_{f\left(\mathbf{X}^t\right)}\)</span>之间的
Wasserstein 距离最小。</p>
<p>这种判别性训练补充了我们的对齐方式，并从源标签和目标标签估计之间的明确关系中提供了额外的约束。这些约束支持联合分布对齐，并在梯度下降训练过程中提供进一步的指导。当我们的判别性训练支持联合分布对齐时，实验的性能会大大提高。</p>
<h3 id="损失函数和提出的算法">损失函数和提出的算法</h3>
<p>我们的域对齐可以通过解决下面的优化问题来实现：</p>
<p><span class="math display">\[
\min _{\gamma, f} \mathcal{L}_1+\mathcal{L}_2=\min _{\gamma, f}
\frac{1}{N^s}
\sum_i\left\|\mathbf{y}_i^s-f\left(\mathbf{x}_i^s\right)\right\|^2+\sum_{i,
j} \gamma_{i
j}\left(\alpha\left\|\mathbf{x}_i^s-\mathbf{x}_j^t\right\|^2+\beta\left\|\mathbf{y}_i^s-f\left(\mathbf{x}_j^t\right)\right\|^2\right)
\qquad (4)
\]</span></p>
<p>其中<span class="math inline">\(\alpha,
\beta&gt;0\)</span>是平衡所选择的参数。值得注意的是，第一项强调了源域的知识在训练过程中不会被遗忘，这在[31、41、42]中都有所体现。没有这种重视，就不能很好地保持源领域知识，从而整体性能可能会下降。在SE实验中也观察到了这一现象。第二项是用于域对齐的。为了说明一些直觉，考虑理想情况其中Eq.
(4)完全极小化为零，从而导致</p>
<p><span class="math display">\[
\left\|\mathbf{x}_i^s-\mathbf{x}_j^t\right\|^2 \equiv 0 \text { and
}\left\|\mathbf{y}_i^s-f\left(\mathbf{x}_j^t\right)\right\|^2 \equiv 0
\quad \Rightarrow \quad \mathbf{x}_i^s=\mathbf{x}_j^t \text { and }
\mathbf{y}_i^s=f\left(\mathbf{x}_j^t\right)
\]</span></p>
<p>对于所有<span class="math inline">\(i,
j\)</span>。这表明对于每个给定的目标域样本<span class="math inline">\(\mathbf{x}_j^t\)</span>，都会找到来自源域的相同样本<span class="math inline">\(\mathbf{x}_
i^s\)</span>,然后由相应的源标签构建未知目标标签。尽管实际上不太可能发生零损失的理想情况，但OT损失旨在寻找最“相似”的对应关系，这需要Eq.（4）的域对齐直觉。从这个角度来看，虽然<span class="math inline">\(C_{i j}\)</span>中的项<span class="math inline">\(\left\|\mathbf{x}_ i^s-\mathbf{x}_
j^t\right\|\)</span>（在方程 （2） 中）与<span class="math inline">\(f\)</span>和<span class="math inline">\(h\)</span>的网络反向传播没有直接关系，但它不能被忽视，因为丢弃该项将导致错误的运输计划<span class="math inline">\(\gamma_{i j}\)</span>并最终导致不希望的对齐。</p>
<p>对于判别式训练，判别器<span class="math inline">\(h\)</span>由判别器损失函数<span class="math inline">\(\mathcal{L}_h=\frac{1}{m} \sum_{i=1}^m
h\left(\mathbf{y}_i^s\right)-h\left(f\left(\mathbf{x}_i^t\right)\right)\)</span>训练，<span class="math inline">\(f\)</span>遵循生成器损失函数<span class="math inline">\(\mathcal{L}_f=-\frac{1}{m} \sum_{i=1}^m
h\left(f\left(\mathbf{x}_i^t\right)\right)\)</span>，其中<span class="math inline">\(m\)</span>是批量大小。由于我们的框架中有多组参数<span class="math inline">\(\gamma, h\)</span>和<span class="math inline">\(f\)</span>，因此每次都会更新一组参数，而其他一组参数是固定的。</p>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法1} \quad \text{DOTN，提出的算法} \\
   \hline \
    \textbf{需要：}
\mathbf{x}^s，源域输入。\mathbf{y}^s，源域标签。\mathbf{x}^t，目标域输入。c，裁剪参数。m，批量大小。\\
    \qquad n_f, n_h, n_s：
分别是每次生成器训练、判别器训练和源域训练的\text{OT}迭代次数。n，迭代次数。\\
    \textbf{需要：}
\theta_f，估计器f的初始参数。\theta_h，判别器h的初始参数 \\
    \quad \textbf{for} \, \, 每批源样本 \left(\mathbf{x}^s,
\mathbf{y}^s\right)和目标样本\left(\mathbf{y}^t\right) \, \,
\textbf{do}\\
    \qquad \quad 固定\theta_f，用\text{OT}求解方程（4）中的\gamma\\
    \qquad \quad 固定 \gamma，\theta_f \leftarrow
\operatorname{Adam}\left(\nabla_{\theta_f} \mathcal{L}_2, \theta_f,
\theta_h\right) \text {. } \\
    \qquad \quad \textbf{if} \, \, n \bmod n_f==0 \textbf { then }\\
    \qquad \qquad \quad \theta_f \leftarrow
\operatorname{Adam}\left(\nabla_{\theta_f} \mathcal{L}_1, \theta_f,
\theta_h\right) .\\
    \qquad \quad \textbf{end if} \\
    \qquad \quad \textbf{if} \, \, n \bmod n_f==0 \textbf { then }\\
    \qquad \qquad \quad \theta_f \leftarrow
\operatorname{Adam}\left(\nabla_{\theta_f} \mathcal{L}_f, \theta_f,
\theta_h\right) .\\
    \qquad \quad \textbf{end if} \\
    \qquad \quad \textbf{if} \, \, n \bmod n_h==0 \textbf { then }\\
    \qquad \qquad \quad \theta_h \leftarrow
\operatorname{Adam}\left(\nabla_{\theta_h} \mathcal{L}_h, \theta_f,
\theta_h\right) .\\
    \qquad \qquad \quad \theta_h \leftarrow
\operatorname{clip}\left(\theta_h,-c, c\right) \\
    \qquad \quad \textbf{end if} \\  
    \quad \textbf{end for}\\  
   \hline
\end{array}
\]</span></p>
<h2 id="结论">结论</h2>
<p>在这项研究中，我们提出了一种新的DOTN方法，该方法专为回归设置中的无监督域适应而设计。我们的方法巧妙地融合了OT和生成对抗框架，根据源域提供的信息在目标域中实现无监督学习，不需要额外的结构或输入，例如多源域和噪声类型标签。我们的实验表明，所提出的方法能够在SE中实现卓越的适应性能，在VoiceBank-DEMAND和TIMIT数据集的PESQ和STOI分数上优于其他对抗域适应方法。此外，我们表明，当适度增加目标样本的复杂度时（通过增加目标域中的噪声类型数量），只观察到小程度的退化。这表明我们的方法对目标域中的样本复杂性是稳健的。</p>
]]></content>
      <categories>
        <category>论文泛读</category>
      </categories>
      <tags>
        <tag>最优化</tag>
        <tag>最优运输</tag>
        <tag>SE（语音增强）</tag>
      </tags>
  </entry>
  <entry>
    <title>两个曲线之间的距离</title>
    <url>/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<h2 id="两个曲线之间的距离">两个曲线之间的距离</h2>
<p>搜Fréchet怎么打的时候看到了Fréchet距离，感觉这两天接触的距离比较多，做下整理（说不定以后换个距离就是个创新点）</p>
<p>本篇主要介绍两个距离：<strong>Fréchet距离</strong>和<strong>Hausdorff距离</strong>。以后看到了比较两个曲线的方法再做补充（之前介绍过的两个分布之间的距离也会写篇博客整理出来）
<span id="more"></span></p>
<h2 id="fréchet距离">Fréchet距离</h2>
<p>Fréchet distance(弗雷歇距离)是法国数学家Maurice René
Fréchet在1906年提出的一种路径空间相似形描述，这种描述同时还考虑进路径空间距离的因素，对于空间路径的相似性比较适用。</p>
<p><strong>直观的理解，Fréchet distance就是最短的狗绳长度</strong>： ·
主人走路径A，狗走路径B，他们有不同的配速方案。 ·
主人和狗各自走完这两条路径过程中所需要的最短狗绳长度。（在某一种配速下需要的狗绳长度，但其他配速下需要的狗绳长度更长）
<img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/1.jpg" alt="image"></p>
<p><strong>严格的数学定义：</strong> 设<span class="math inline">\((\mathbb S ,d)\)</span>是一个度量空间，<span class="math inline">\(d\)</span>是<span class="math inline">\(\mathbb{S}\)</span>上的度量。 1. <span class="math inline">\(A\)</span>是<span class="math inline">\(\mathbb{S}\)</span>上的一个将单位区间映射到<span class="math inline">\(\mathbb{S}\)</span>的连续映射，例如：<span class="math inline">\(A:[0,1] \to \mathbb S\)</span>. 2.
从单位区间到其自身的重参数化映射<span class="math inline">\(\alpha
:[0,1]\to [0,1]\)</span>满足如下三个条件：1）<span class="math inline">\(\alpha\)</span>是连续的。 2）<span class="math inline">\(\alpha\)</span>是非降的，即对于任意的<span class="math inline">\(x,y \in [0,1],x\le y\)</span>，都有<span class="math inline">\(\alpha(x) \le \alpha(y)\)</span>。 3）<span class="math inline">\(\alpha\)</span>是满射。此时有<span class="math inline">\(\alpha(0)=0,\alpha(1)=1\)</span> 3. 设<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是<span class="math inline">\(\mathbb{S}\)</span>上的两条曲线，即<span class="math inline">\(A:[0,1]\to \mathbb{S},B:[0,1]\to
\mathbb{S}\)</span>。又<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是单位区间的两个重参数化映射，则曲线<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>之间的Fréchet距离定义为： <span class="math display">\[
F(A,B)=\underset{\alpha ,\beta}{\inf}\underset{t \in [0,1]}{\max}\lbrace
d(A(\alpha(t)),B(\beta(t)))
\]</span></p>
<p>解释：</p>
<ul>
<li>两条曲线<span class="math inline">\(A(\alpha(t))\)</span>和<span class="math inline">\(B(\beta(t))\)</span>之间距离最大值的下确界
<ul>
<li>t理解为时间</li>
<li><span class="math inline">\(\alpha(t)\)</span>和<span class="math inline">\(\beta(t)\)</span>理解为人和狗随时间变化的速度</li>
<li><span class="math inline">\(A(\alpha(t))\)</span>和<span class="math inline">\(B(\beta(t))\)</span>代表t时刻人和狗的位置</li>
<li>最大值的下确界意思为,每一种人狗速度方案下，都有对应的距离最大值；那么对于所有的速度方案，这些距离最大值中最小的是哪个？</li>
</ul></li>
</ul>
<p><strong>离散化</strong></p>
<p>设定<span class="math inline">\(t\)</span>是时间点，该时刻，曲线<span class="math inline">\(A\)</span> 上的采样点为<span class="math inline">\(A(\alpha(t))\)</span>, 曲线<span class="math inline">\(B\)</span>上采样点为<span class="math inline">\(B(\alpha(t))\)</span>.
如果使用欧氏距离，则容易定义<span class="math inline">\(d(A(\alpha(t)),B(\beta(t)))\)</span>.
在每次采样中<span class="math inline">\(t\)</span>离散的遍历区间<span class="math inline">\([0,1]\)</span>, 得到该种采样下的最大距离<span class="math inline">\(\underset{t\in [0,1]}{\max} \lbrace
d(A(\alpha(t)),B(\beta(t))) \rbrace\)</span>.
Fréchet距离就是使该最大距离最小化的采样方式下的值。
易于理解的，在离散方式下，我们不可能得到真实的Fréchet距离，而可以无限的趋近。但是越精确的值需要越大的计算量。</p>
<p><img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/2.jpg"></p>
<h2 id="hausdorff距离">Hausdorff距离</h2>
<p>衡量两个集合之间的距离</p>
<p><strong>定义</strong>: 给定欧氏空间中的两点集<span class="math inline">\(A=\lbrace a_1,a_2,... \rbrace , B=\lbrace
b_1,b_2,...
\rbrace\)</span>，豪斯多夫（Hausdorff）距离就是用来衡量这两个点集间的距离。定义公式如下:
<span class="math display">\[
H(A,B)=\max[h(A,B),h(B,A)]
\]</span> 其中， <span class="math display">\[
h(A,B)=\underset{a \in A}{\max}\underset{b\in B}{\min} \lVert a-b \rVert
\\
h(B,A)=\underset{b \in B}{\max}\underset{a\in A}{\min} \lVert b-a \rVert
\]</span> <span class="math inline">\(H(A,B)\)</span>称为双向Hausdorff
距离，<span class="math inline">\(h(A,B)\)</span>称为从点集A到点集B的单向 Hausdorff
距离。相应地<span class="math inline">\(h(B,A)\)</span>称为从点集B到点集A的单向Hausdorff距离</p>
<p><strong>一些图例</strong></p>
<p><img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/3.jpg"> <img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/4.jpg"></p>
<p><strong>性质</strong></p>
<ul>
<li><p>双向Hausdorff距离 <span class="math inline">\(H(A,B)\)</span>是单向 Hausdorff 距离<span class="math inline">\(h(A,B)\)</span>和<span class="math inline">\(h(B,A)\)</span>两者中较大者，显然它度量了两个点集间的最大不匹配程度。</p></li>
<li><p>当<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>都是闭集的时候,Hausdorff距离满足度量的三个定理：</p></li>
</ul>
<ol type="1">
<li><span class="math inline">\(H(A,B)\ge 0\)</span>，当且仅当<span class="math inline">\(A=B\)</span>时，<span class="math inline">\(H(A,B)= 0\)</span></li>
<li><span class="math inline">\(H(A,B)=H(B,A)\)</span></li>
<li><span class="math inline">\(H(A,B)+H(B,C)\ge H(A,C)\)</span></li>
</ol>
<ul>
<li><p>若凸集<span class="math inline">\(A,B\)</span>满足<span class="math inline">\(A \nsubseteq B,B \nsubseteq A\)</span>并记<span class="math inline">\(\partial A,\partial B\)</span>分别为<span class="math inline">\(A,B\)</span>边界的集合，则<span class="math inline">\(A,B\)</span>的Hausdorff距离等于<span class="math inline">\(\partial A,\partial
B\)</span>的Hausdorff距离</p></li>
<li><p>Hausdorff距离易受到突发噪声的影响。</p></li>
</ul>
<p><img src="/2025/02/13/%E4%B8%A4%E4%B8%AA%E6%9B%B2%E7%BA%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB/5.jpg"></p>
<p>当图像受到噪声污染或存在遮挡等情况时，原始的Haudorff距离容易造成误匹配。所以，在1933年，Huttenlocher提出了部分
Hausdorff距离的概念。</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>各种距离</tag>
      </tags>
  </entry>
  <entry>
    <title>WGAN</title>
    <url>/2025/03/10/WGAN/</url>
    <content><![CDATA[<h1 id="wasserstein-gan">Wasserstein GAN</h1>
<span id="more"></span>
<h2 id="介绍">介绍</h2>
<p>本文关心的问题是无监督学习问题。主要是，学习一个概率分布意味着什么？对此，经典的答案是学习一个概率密度。这通常是通过定义一个参数密度族<span class="math inline">\(\left(P_\theta\right)_{\theta \in
\mathbb{R}^d}\)</span>，并在数据上找到使似然函数最大化的密度族：如果有真实的数据样本<span class="math inline">\(\left\{x^{(i)}\right\}_{i-1}^m\)</span>，我们就可以解决这个问题。</p>
<p><span class="math display">\[
\max _{\theta \in \mathbb{R}^d} \frac{1}{m} \sum_{i=1}^m \log
P_\theta\left(x^{(i)}\right)
\]</span></p>
<p>如果真实数据分布<span class="math inline">\(\mathbb{P}_r\)</span>具有密度，<span class="math inline">\(\mathbb{P}_\theta\)</span>是参数化密度<span class="math inline">\(P_\theta\)</span>的分布，那么渐近地，这相当于最小化Kullback-Leibler散度<span class="math inline">\(KL\left(\mathbb{P}_r \|
\mathbb{P}_\theta\right)\)</span></p>
<p>为此，需要模型密度<span class="math inline">\(P_\theta\)</span>存在。在我们处理由低维流形支撑的分布时，这种情况并不常见。那么模型流形和真实分布的支撑集不太可能有不可忽略（测度大于0）的交集[1]，这意味着KL距离没有定义(或者简单地无限)。</p>
<p>典型的补救方法是在模型分布中加入噪声项。这就是为什么在经典的机器学习文献中描述的几乎所有的生成模型都包含了噪声成分。在最简单的情况下，为了覆盖所有的例子，假设高斯噪声具有相对较大的带宽。然而，在图像生成模型中，这种噪声会降低样本的质量，使其变得模糊。例如，在文献[23]中，我们可以看到当生成图像的像素已经归一化到<span class="math inline">\([0,1]\)</span>范围内时，当做最大似然估计时，添加到模型中的噪声的最佳标准偏差是0.1。
这是一个非常高的噪声量，以至于当论文报告他们的模型的样本时，他们没有添加他们报告似然数的噪声项。换句话说，添加的噪声项对于问题显然是不正确的，但需要使最大似然方法发挥作用。</p>
<p>我们可以定义一个固定分布<span class="math inline">\(p(z)\)</span>的随机变量<span class="math inline">\(Z\)</span>，并通过一个参数函数<span class="math inline">\(g_\theta: \mathcal{Z} \rightarrow
\mathcal{X}\)</span>
(通常是某种类型的神经网络)，直接生成服从某一分布<span class="math inline">\(\mathbb{P}_\theta\)</span>的样本，而不是估计可能不存在的<span class="math inline">\(\mathbb{P}_r\)</span>的密度。通过改变<span class="math inline">\(\theta\)</span>，可以改变这种分布，使其接近真实的数据分布<span class="math inline">\(\mathbb{P}_r\)</span>。这在两个方面是有用的。首先，与密度不同，这种方法可以表示限制在低维流形上的分布。其次，容易生成样本的能力往往比知道密度的数值更有用。
一般而言，给定任意高维密度生成样本在计算上是困难的[16]。</p>
<p>变分自编码器(Variational Auto-Encoders，VAEs)
[9]和生成式对抗网络(Generative Adversarial Networks，GANs)
[4]就是这种方法的典型代表。由于VAEs关注的是样本的近似似然，因此它们具有标准模型的局限性，需要处理额外的噪声项。GANs在目标函数的定义上提供了更多的灵活性，包括Jensen-Shannon
[4]，所有的f-散度[17]以及一些奇异组合[6]。另一方面，训练GANs是众所周知的微妙和不稳定的，原因在理论上研究[1]。</p>
<p>在这篇文章中，我们将注意力集中在用不同的方法来度量模型分布和真实分布的接近程度，或者等价地，用不同的方法来定义一个距离或散度<span class="math inline">\(\rho\left(\mathbb{P}_\theta,
\mathbb{P}_r\right)\)</span>。这种距离之间最根本的区别在于它们对概率分布序列收敛性的影响。一个分布列<span class="math inline">\(\left(\mathbb{P}_t\right)_{t \in
\mathbb{N}}\)</span>收敛当且仅当存在一个分布<span class="math inline">\(\mathbb{P}_{\infty}\)</span>，使得<span class="math inline">\(\rho\left(\mathbb{P}_t,
\mathbb{P}_{\infty}\right)\)</span>趋向于零，这取决于距离<span class="math inline">\(\rho\)</span>定义的精确程度。不严格的说，当距离<span class="math inline">\(\rho\)</span>使得一个分布序列更容易收敛时，就会产生一个较弱的拓扑结构。（当<span class="math inline">\(\rho&#39;\)</span>下的收敛序列集是<span class="math inline">\(\rho\)</span>下收敛序列集的子集时，<span class="math inline">\(\rho\)</span>诱导的拓扑比<span class="math inline">\(\rho&#39;\)</span>诱导的拓扑弱）第2节阐明了常用概率距离在这方面的差异。</p>
<p>为了优化参数<span class="math inline">\(\theta\)</span>，需要使映射<span class="math inline">\(\theta \mapsto
\mathbb{P}_\theta\)</span>以连续的方式定义我们的模型分布<span class="math inline">\(\mathbb{P}_\theta\)</span>。连续性是指当参数序列<span class="math inline">\(\theta_t\)</span>收敛于<span class="math inline">\(\theta\)</span>时，分布<span class="math inline">\(\mathbb{P}_{\theta_t}\)</span>也收敛于<span class="math inline">\(\mathbb{P}_\theta\)</span>。然而，需要注意的是，分布<span class="math inline">\(\mathbb{P}_{\theta_t}\)</span>收敛的概念取决于我们计算分布之间距离的方式。这个距离越弱，就越容易定义一个从<span class="math inline">\(\theta\)</span>空间到<span class="math inline">\(\mathbb{P}_\theta\)</span>空间的连续映射，因为它的分布就越容易收敛。我们关心映射<span class="math inline">\(\theta \mapsto
\mathbb{P}_\theta\)</span>连续的主要原因如下. 如果<span class="math inline">\(\rho\)</span>是我们关于两个分布之间的距离的概念，我们希望有一个连续的损失函数<span class="math inline">\(\theta \mapsto \rho\left(\mathbb{P}_\theta,
\mathbb{P}_r\right)\)</span>，这相当于在使用分布之间的距离<span class="math inline">\(\rho\)</span>时，映射<span class="math inline">\(\theta \mapsto
\mathbb{P}_\theta\)</span>是连续的。</p>
<p>本文的贡献在于：
1.我们提供了一个全面的理论分析，与学习分布中使用的流行概率距离和散度相比，Earth
Mover（EM）距离是如何表现的</p>
<p>2.在第3节中，我们定义了一种称为Wasserstein
GAN的GAN形式，该GAN使EM距离的合理有效近似最小化，我们从理论上证明了相应的优化问题是合理的</p>
<p>3.在第4节中，我们实证表明，WGAN解决了GAN的主要训练问题。特别是，训练WGAN不需要在训练鉴别器和生成器时保持谨慎的平衡，也不需要仔细设计网络架构。GAN中典型的模式下降现象也大大减少。WGAN最引人注目的实际好处之一是能够通过将鉴别器训练到最优性来连续估计EM距离。绘制这些学习曲线不仅有助于调试和超参数搜索，而且与观察到的样本质量有很好的相关性。</p>
<h3 id="towards-principled-methods-for-training-generative-adversarial-networks">[1]《Towards
principled methods for training generative adversarial networks》</h3>
<h4 id="介绍-1">介绍</h4>
<p>传统的生成式建模方法依赖于最大化似然，或者等价地最小化未知数据分布<span class="math inline">\(\mathbb{P}_r\)</span>和生成器分布<span class="math inline">\(\mathbb{P}_g\)</span>之间的KL散度。如果假设两个分布都是密度为<span class="math inline">\(P_r\)</span>和<span class="math inline">\(P_g\)</span>的连续分布，那么这些方法都试图最小化
<span class="math display">\[
K L\left(\mathbb{P}_r \| \mathbb{P}_g\right)=\int_{\mathcal{X}} P_r(x)
\log \frac{P_r(x)}{P_g(x)} \mathrm{d} x
\]</span></p>
<p>该代价函数具有在<span class="math inline">\(\mathbb{P}_g=\mathbb{P}_r\)</span>处有唯一最小值的良好性质，并且不需要知道未知的<span class="math inline">\(P_r(x)\)</span>来优化它(仅需要样本)。然而，它的发散在<span class="math inline">\(\mathbb{P}_r\)</span>r和<span class="math inline">\(\mathbb{P}_g\)</span>之间不对称（<span class="math inline">\(K L\left(\mathbb{P}_r \| \mathbb{P}_g\right) \not
= K L\left(\mathbb{P}_g \| \mathbb{P}_r\right)\)</span>）：</p>
<p>1.若<span class="math inline">\(P_r(x)&gt;P_g(x)\)</span>，则<span class="math inline">\(x\)</span>是来自数据的概率高于生成样本的点。这就是通常所说的'mode
dropping'现象的核心：存在大片区域满足<span class="math inline">\(P_r\)</span>值较大的，而<span class="math inline">\(P_g\)</span>值较小或为零。需要注意的是，当<span class="math inline">\(P_r(x)&gt;0\)</span>但<span class="math inline">\(P_g(x) \rightarrow
0\)</span>时，KL内的被积函数迅速增长到无穷大，这意味着该成本函数为没有覆盖部分数据的生成器分布分配了极高的成本（如果生成器不能完全生成全部真实样本数据，那么cost将很高）</p>
<p>2.如果<span class="math inline">\(P_r(x)&lt;P_g(x)\)</span>，则<span class="math inline">\(x\)</span>作为数据点的概率较低，但由我们的模型生成的概率很高。当我们看到生成器输出的图像看起来不真实时，就是这种情况。在这种情况下，当<span class="math inline">\(P_r(x) \rightarrow 0\)</span>并且<span class="math inline">\(P_g(x)&gt;0\)</span>时，看到KL内的值变为0，这意味着这个成本函数将为生成看起来很假的样本支付极低的成本。</p>
<p>相反，如果我们将<span class="math inline">\(K L\left(\mathbb{P}_g \|
\mathbb{P}_r\right)\)</span>最小化，这些误差的权重将被逆转，这意味着这个代价函数将付出很高的成本来生成看上去不真实的图片。生成对抗网络已经被证明可以优化这两个代价函数的对称中间点，即Jensen-shannon散度
<span class="math display">\[
J S D\left(\mathbb{P}_r \| \mathbb{P}_g\right)=\frac{1}{2} K
L\left(\mathbb{P}_r \| \mathbb{P}_A\right)+\frac{1}{2} K
L\left(\mathbb{P}_g \| \mathbb{P}_A\right)
\]</span></p>
<p>其中，<span class="math inline">\(\mathbb{P}_A\)</span>为"平均"分布，密度为<span class="math inline">\(\frac{P_r+P_g}{2}\)</span>
。可以推测，GANs成功地生成真实图像的原因是由于从传统的最大似然方法转换而来的。然而，还有问题</p>
<p>生成对抗网络的构建分为两个步骤。我们首先训练一个判别器<span class="math inline">\(D\)</span>使其最大化 <span class="math display">\[
L\left(D, g_\theta\right)=\mathbb{E}_{x \sim \mathbf{P}_r}[\log
D(x)]+\mathbb{E}_{x \sim \mathbf{P}_g}[\log (1-D(x))] \qquad(1)
\]</span></p>
<p>很容易地证明最优判别器具有形状（L对D（x）求导等于0） <span class="math display">\[
D^*(x)=\frac{P_r(x)}{P_r(x)+P_g(x)}
\]</span></p>
<p>且 <span class="math inline">\(L\left(D^*, g_\theta\right)=2 J S
D\left(\mathbb{P}_r \| \mathbb{P}_g\right)-2 \log 2\)</span>,
因此，当判别器是最优的时，最小化作为<span class="math inline">\(\theta\)</span>的函数的方程(1)等价于最小化Jensen-Shannon散度。因此，在理论上，人们期望我们首先训练尽可能接近最优的判别器(因此,
<span class="math inline">\(\theta\)</span>上的代价函数更好地逼近JSD
)，然后在<span class="math inline">\(\theta\)</span>上做梯度步骤，交替这两种情况
，然而，这并不奏效。在实际应用中，随着判别器的变好，生成器的更新也不断变差。</p>
<h4 id="不稳定的来源">不稳定的来源</h4>
<p>该理论告诉我们，训练好的判别器最好也就是<span class="math inline">\(2
\log 2-2 J S D\left(\mathbb{P}_r \|
\mathbb{P}_g\right)\)</span>。然而，在实际中，如果我们仅训练<span class="math inline">\(D\)</span>直到收敛，它的误差将趋于0，但是如图一所示，这表明它们之间的JSD是最大的。唯一可能的情况是，两个分布不是连续的，或者它们有不相交的支撑。
<img src="/2025/03/10/WGAN/WGAN\1.png" alt="image"></p>
<p>原因总结：真实数据分布的支撑集往往形成流形，而生成器的输出分布也往往是流形，而两个流形相交部分测度为0。</p>
<p>定理2.3 设<span class="math inline">\(\mathbb{P}_r\)</span>和<span class="math inline">\(\mathbb{P}_g\)</span>是两个分布，其支撑点位于两个流形<span class="math inline">\(\mathcal{M}\)</span>和<span class="math inline">\(\mathcal{P}\)</span>中，这两个流形不具有全维且不完全对齐。我们进一步假设<span class="math inline">\(\mathbb{P}_r\)</span>和<span class="math inline">\(\mathbb{P}_g\)</span>在各自的流形上是连续的，则有
<span class="math display">\[
\begin{aligned}
J S D\left(\mathbb{P}_r \| \mathbb{P}_g\right) &amp; =\log 2 \\
K L\left(\mathbb{P}_r \| \mathbb{P}_g\right) &amp; =+\infty \\
K L\left(\mathbb{P}_g \| \mathbb{P}_r\right) &amp; =+\infty
\end{aligned}
\]</span></p>
<p>值得注意的是，即使两个流形任意靠近，这些分歧也会被放大。我们的生成器的样本可能看起来很好，但两个KL散度都是无穷大，JSD为常数log2，这在训练的时候会发生梯度消失。</p>
<p>为什么log2？ <img src="/2025/03/10/WGAN/WGAN\2.png" alt="image"></p>
<p>回到WGAN</p>
<h2 id="不同距离">不同距离</h2>
<p>设<span class="math inline">\(\mathcal{X}\)</span>是紧致度量集(如图像的空间<span class="math inline">\([0,1]^d\)</span>)，<span class="math inline">\(\Sigma\)</span>表示<span class="math inline">\(\mathcal{X}\)</span>的所有Borel子集构成的集合，<span class="math inline">\(\operatorname{Prob}(\mathcal{X})\)</span>表示定义在<span class="math inline">\(\mathcal{X}\)</span>上的概率测度空间。定义两个分布<span class="math inline">\(\mathbb{P}_r, \mathbb{P}_g \in
\operatorname{Prob}(\mathcal{X})\)</span>之间的距离： - 全变分 (TV)
距离</p>
<p><span class="math display">\[
\delta\left(\mathbb{P}_r, \mathbb{P}_g\right)=\sup _{A \in
\Sigma}\left|\mathbb{P}_r(A)-\mathbb{P}_g(A)\right|
\]</span></p>
<ul>
<li><p>The Kullback-Leibler (KL) 散度 <span class="math display">\[
K L\left(\mathbb{P}_r \| \mathbb{P}_g\right)=\int \log
\left(\frac{P_r(x)}{P_g(x)}\right) P_r(x) d \mu(x)
\]</span></p></li>
<li><p>The Jensen-Shannon (JS) 散度 <span class="math display">\[
J S\left(\mathbb{P}_r, \mathbb{P}_g\right)=K L\left(\mathbb{P}_r \|
\mathbb{P}_m\right)+K L\left(\mathbb{P}_g \| \mathbb{P}_m\right)
\]</span></p></li>
</ul>
<p>其中 <span class="math inline">\(\mathbb{P}_m\)</span>是<span class="math inline">\(\left(\mathbb{P}_r+\mathbb{P}_g\right) /
2\)</span>的混合</p>
<ul>
<li>The Earth-Mover (EM) 距离 （Wasserstein-1 距离） <span class="math display">\[
W\left(\mathbb{P}_r, \mathbb{P}_g\right)=\inf _{\gamma \in
\Pi\left(\mathbb{P}_r, \mathbb{P}_g\right)} \mathbb{E}_{(x, y) \sim
\gamma}[\|x-y\|] \qquad(1)
\]</span></li>
</ul>
<p>其中，<span class="math inline">\(\Pi\left(\mathbb{P}_r,
\mathbb{P}_g\right)\)</span>表示所有边际分布分别为<span class="math inline">\(\mathbb{P}_r\)</span>和<span class="math inline">\(\mathbb{P}_g\)</span>的联合分布<span class="math inline">\(\gamma(x, y)\)</span>的集合。直观上，<span class="math inline">\(\gamma(x, y)\)</span>表示为了将分布<span class="math inline">\(\mathbb{P}_r\)</span>转化为分布<span class="math inline">\(\mathbb{P}_g\)</span>需要从<span class="math inline">\(x\)</span>输送到<span class="math inline">\(y\)</span>的"质量"。那么EM距离就是最优运输方案的"成本"。</p>
<p>下面的例子说明了简单的概率分布序列在EM距离下是收敛的，而在上面定义的其他距离和散度下是不收敛的。</p>
<p>例1 令<span class="math inline">\(Z \sim
U[0,1]\)</span>为单位区间上的均匀分布。设<span class="math inline">\(\mathbb{P}_0\)</span>为<span class="math inline">\((0, Z) \in
\mathbb{R}^2\)</span>上的分布(0在x轴上,随机变量<span class="math inline">\(Z\)</span>在y轴上)，在过原点的竖直线上均匀分布。现令<span class="math inline">\(g_\theta(z)=(\theta, z)\)</span>，<span class="math inline">\(\theta\)</span>为单一实参数。不难看出，在这种情况下
- <span class="math inline">\(W\left(\mathbb{P}_0,
\mathbb{P}_\theta\right)=|\theta|\)</span>, - <span class="math inline">\(J S\left(\mathbb{P}_0, \mathbb{P}_\theta\right)=
\begin{cases}\log 2 &amp; \text { if } \theta \neq 0, \\ 0 &amp; \text {
if } \theta=0,\end{cases}\)</span> - <span class="math inline">\(K
L\left(\mathbb{P}_\theta \| \mathbb{P}_0\right)=K L\left(\mathbb{P}_0 \|
\mathbb{P}_\theta\right)= \begin{cases}+\infty &amp; \text { if } \theta
\neq 0, \\ 0 &amp; \text { if } \theta=0,\end{cases}\)</span> - and
<span class="math inline">\(\delta\left(\mathbb{P}_0,
\mathbb{P}_\theta\right)= \begin{cases}1 &amp; \text { if } \theta \neq
0, \\ 0 &amp; \text { if } \theta=0 .\end{cases}\)</span></p>
<p>当<span class="math inline">\(\theta_t \rightarrow
0\)</span>时，序列<span class="math inline">\(\left(\mathbb{P}_{\theta_t}\right)_{t \in
\mathbb{N}}\)</span>在EM距离下收敛于<span class="math inline">\(\mathbb{P}_0\)</span>，但在JS、KL、反向KL或TV散度下均不收敛。图1说明了EM和JS距离的情况。
<img src="/2025/03/10/WGAN/WGAN\3.png" alt="image"></p>
<p>示例1提供了一个案例，可以通过在EM距离上进行梯度下降来学习低维流形上的概率分布。这无法通过其他距离和散度来实现，因为由此产生的损失函数甚至不是连续的。尽管这个简单的例子以具有不相交支撑的分布为特征，但当支撑在一组零测集中包含非空交集时，同样的结论成立。</p>
<p>由于Wasserstein距离比JS距离更弱，我们现在可以问，在适当的假设下，<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>是否是<span class="math inline">\(\theta\)</span>上的连续损失函数</p>
<p><strong>定理1</strong> 设<span class="math inline">\(\mathbb{P}_r\)</span>是<span class="math inline">\(\mathcal{X}\)</span>上的一个固定分布，<span class="math inline">\(Z\)</span>是另一空间<span class="math inline">\(\mathcal{Z}\)</span>上的随机变量(例如 Gaussian)
。设<span class="math inline">\(g: \mathcal{Z} \times \mathbb{R}^d
\rightarrow \mathcal{X}\)</span>是一个函数，记为<span class="math inline">\(g_\theta(z)\)</span>，其中<span class="math inline">\(z\)</span>是第一坐标，<span class="math inline">\(\theta\)</span>是第二坐标。令<span class="math inline">\(\mathbb{P}_\theta\)</span>表示<span class="math inline">\(g_\theta(Z)\)</span>的分布。那么</p>
<ol type="1">
<li><p>如果<span class="math inline">\(g\)</span>在关于<span class="math inline">\(\theta\)</span>是连续的，则<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>也是连续的</p></li>
<li><p>如果<span class="math inline">\(g\)</span>是局部Lipschitz并满足假设1，那么<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>处处连续，并且几乎处处可微</p></li>
<li><p>1-2对于Jensen-Shannon散度<span class="math inline">\(J
S\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>和所有KL都不成立</p></li>
</ol>
<p>以下推论告诉我们，通过最小化
EM距离进行学习对于神经网络来说是有意义的（至少在理论上是这样）</p>
<p><strong>推论1</strong> 设<span class="math inline">\(g_\theta\)</span>为由<span class="math inline">\(\theta\)</span>参数化的任何前馈神经网络，<span class="math inline">\(p(z)\)</span>为<span class="math inline">\(z\)</span>上的先验分布，使得 <span class="math inline">\(\mathbb{E}_{z \sim
p(z)}[\|z\|]&lt;\infty\)</span>（例如高斯、均匀等）。然后假设 1
得到满足，因此<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>处处连续，几乎处处可微</p>
<p>所有这些都表明，对于研究的问题来说，EM是一个至少比
Jensen-Shannon散度更合理的成本函数。以下定理描述了由这些距离和散度诱导的拓扑的相对强度，其中KL最强，其次是JS和TV，EM最弱</p>
<p><strong>定理2</strong> 设<span class="math inline">\(\mathbb{P}\)</span>是紧空间<span class="math inline">\(\mathcal{X}\)</span>上的分布，<span class="math inline">\(\left(\mathbb{P}_n\right)_{n \in
\mathbb{N}}\)</span>是<span class="math inline">\(\mathcal{X}\)</span>上的分布序列。然后，<span class="math inline">\(n \rightarrow \infty\)</span>，</p>
<ol type="1">
<li>以下语句是等价的</li>
</ol>
<ul>
<li><span class="math inline">\(\delta\left(\mathbb{P}_n,
\mathbb{P}\right) \rightarrow 0\)</span></li>
<li>JS <span class="math inline">\(\left(\mathbb{P}_n, \mathbb{P}\right)
\rightarrow 0\)</span></li>
</ul>
<ol start="2" type="1">
<li>以下语句是等价的</li>
</ol>
<ul>
<li><span class="math inline">\(W\left(\mathbb{P}_n, \mathbb{P}\right)
\rightarrow 0\)</span></li>
<li><span class="math inline">\(\mathbb{P}_n \xrightarrow{\mathcal{D}}
\mathbb{P}\)</span> 其中 <span class="math inline">\(\xrightarrow{\mathcal{D}}\)</span>
表示随机变量的分布收敛</li>
</ul>
<ol start="3" type="1">
<li><p><span class="math inline">\(K L\left(\mathbb{P}_n \|
\mathbb{P}\right) \rightarrow 0\)</span> 或 <span class="math inline">\(K L\left(\mathbb{P} \| \mathbb{P}_n\right)
\rightarrow 0\)</span>能推出（1）</p></li>
<li><p>（1）能推出（2）</p></li>
</ol>
<p>这突出了这样一个事实，即在学习低维流形支持的分布时，KL、JS 和 TV
距离不是合理的代价函数。但是，EM
距离是合理的。下面将介绍优化EM距离的实际近似值</p>
<h2 id="wasserstein-gan-1">Wasserstein GAN</h2>
<p>定理2指出<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>在优化时可能比<span class="math inline">\(J S\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>具有更好的性质。然而，(1)中的下确界是非常难解的。另一方面，由Kantorovich-Rubinstein对偶[22]</p>
<p><span class="math display">\[
W\left(\mathbb{P}_r, \mathbb{P}_\theta\right)=\sup _{\|f\|_L \leq 1}
\mathbb{E}_{x \sim \mathbb{P}_r}[f(x)]-\mathbb{E}_{x \sim
\mathbb{P}_\theta}[f(x)] \qquad(2)
\]</span></p>
<p>其中，上确界在所有1-Lipschitz函数<span class="math inline">\(f:
\mathcal{X} \rightarrow \mathbb{R}\)</span>中寻找。请注意，如果我们将
<span class="math inline">\(\|f\|_L \leq 1\)</span>替换为<span class="math inline">\(\|f\|_L \leq
K\)</span>（某个常数K的K-Lipschitz），那么我们最终得到<span class="math inline">\(K \cdot W\left(\mathbb{P}_r,
\mathbb{P}_g\right)\)</span>。因此，如果有一个参数化的函数族<span class="math inline">\(\left\{f_w\right\}_{w \in
\mathcal{W}}\)</span>都是K-Lipschitz，那么可以考虑解决这个问题</p>
<p><span class="math display">\[
\max _{w \in \mathcal{W}} \mathbb{E}_{x \sim
\mathbb{P}_r}\left[f_w(x)\right]-\mathbb{E}_{z \sim
p(z)}\left[f_w\left(g_\theta(z)\right]\right.  \qquad(3)
\]</span></p>
<p>如果（2）中的上确界是由某个 <span class="math inline">\(w \in
\mathcal{W}\)</span>（这是一个非常强的假设，类似于证明估计器一致性时的假设）而得到的，那么这个过程将会使<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>的计算到达一个常数。此外，我们可以考虑通过估计<span class="math inline">\(\mathbb{E}_{z \sim p(z)}\left[\nabla_\theta
f_w\left(g_\theta(z)\right)\right]\)</span>通过方程（2）进行反推来对<span class="math inline">\(W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)\)</span>求导数（同样，直到一个常数）。虽然这都是直觉，但现在证明，在最优性假设下，这个过程是有原则的。</p>
<p><strong>定理3</strong> 设<span class="math inline">\(\mathbb{P}_r\)</span>为任意分布。设<span class="math inline">\(\mathbb{P}_\theta\)</span>为<span class="math inline">\(g_\theta(Z)\)</span>的分布，<span class="math inline">\(Z\)</span>为密度为<span class="math inline">\(p\)</span>的随机变量，<span class="math inline">\(g_\theta\)</span>为满足假设1的函数。那么，下面问题有一个解<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[
\max _{\|f\|_L \leq 1} \mathbb{E}_{x \sim
\mathbb{P}_r}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_\theta}[f(x)]
\]</span> 并且有 <span class="math display">\[
\nabla_\theta W\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)=-\mathbb{E}_{z \sim p(z)}\left[\nabla_\theta
f\left(g_\theta(z)\right)\right]
\]</span></p>
<p>现在的问题是找到解决方程（2）中最大化问题的函数<span class="math inline">\(f\)</span>。为了大致近似这一点，我们可以做的是训练一个参数化的神经网络，其中权重<span class="math inline">\(w\)</span>位于紧空间<span class="math inline">\(\mathcal{W}\)</span>中，然后通过<span class="math inline">\(\mathbb{E}_{z \sim p(z)}\left[\nabla_\theta
f_w\left(g_\theta(z)\right)\right]\)</span>进行反向传播，就像我们使用典型的GAN一样。注意，<span class="math inline">\(\mathcal{W}\)</span>是紧的这一事实意味着所有函数<span class="math inline">\(f_w\)</span>都是K-Lipschitz，它只取决于<span class="math inline">\(\mathcal{W}\)</span>，而不取决于单个权重，因此近似（2）到一个不相关的比例因子和“critic”<span class="math inline">\(f_w\)</span>的容量。为了使参数<span class="math inline">\(w\)</span>位于紧空间中，我们可以做的一件简单的事情是在每次梯度更新后将权重clamp到一个固定的范围中（比如<span class="math inline">\(\mathcal{W}=[-0.01,0.01]^l\)</span>）。Wasserstein生成对抗网络（WGAN）过程在算法1中进行了描述</p>
<p><span class="math display">\[
    \begin{array}{l}
   \hline \
   \textbf{算法1} \quad \text{WGAN} \\
   \hline
   \textbf{给定：}\alpha:学习率。c:裁剪参数。m:批量大小。 \\
   \qquad n_\text{critic}:每次生成器迭代，迭代\text{critic}的次数\\
   \textbf{给定：}w_0：初始 critic 参数。\theta_0：初始生成器的参数。
   \\1:
   \quad \textbf{while} \,\theta 尚未收敛 \, \textbf{do} \\2:
   \qquad \textbf{for} \, t=0,...,n_\text{critic} \, \textbf{do} \\3:
   \qquad \quad
从真实数据的批次采样一个批次\left\{x^{(i)}\right\}_{i=1}^m \sim
\mathbb{P}_r \\4:
   \qquad \quad 采样一批先验样本 \left\{z^{(i)}\right\}_{i=1}^m \sim
p(z) \\5:
   \qquad \quad g_w \leftarrow \nabla_w\left[\frac{1}{m} \sum_{i=1}^m
f_w\left(x^{(i)}\right)-\frac{1}{m} \sum_{i=1}^m
f_w\left(g_\theta\left(z^{(i)}\right)\right)\right]\\6:
   \qquad \quad w \leftarrow w+\alpha \cdot
\operatorname{RMSProp}\left(w, g_w\right)\\7:
   \qquad \quad w \leftarrow \operatorname{clip}(w,-c, c) \\8:
   \qquad \textbf{end for}\\9:
   \qquad 采样一批先验样本 \left\{z^{(i)}\right\}_{i=1}^m \sim p(z)
\\10:
   \qquad g_\theta \leftarrow-\nabla_\theta \frac{1}{m} \sum_{i=1}^m
f_w\left(g_\theta\left(z^{(i)}\right)\right) \\11:
   \qquad \theta \leftarrow \theta-\alpha \cdot
\operatorname{RMSProp}\left(\theta, g_\theta\right) \\12:
   \quad \textbf{end while} \\
   \hline
\end{array}
\]</span></p>
<p>权重裁剪显然是强制实施Lipschitz约束的一种糟糕的方法。如果裁剪参数很大，则任何权重可能需要很长时间才能达到其极限，从而使critic更难训练直到最优。如果裁剪很小，则当层数较大或未使用批量归一化（例如在RNN中）时，这很容易导致渐变消失。尝试了简单的变体（例如将权重投影到球体），几乎没有区别，并且由于它的简单性和已经很好的性能，作者坚持使用权重裁剪。作者将在神经网络设置中强制执行Lipschitz约束的主题留给进一步研究，并且积极鼓励感兴趣的研究人员改进这种方法。</p>
<p>EM距离是连续且可微的事实意味着可以（并且应该）训练critic直到最优。论点很简单，训练critic的次数越多，得到的Wasserstein梯度就越可靠，这实际上很有用，因为Wasserstein几乎在所有地方都是可导的对于JS，随着判别器变得更好，梯度变得更可靠，但真正的梯度为0，因为JS是局部饱和的，得到的梯度消失。图2展示了一个概念验证，训练一个GAN判别器和一个WGAN
critic，直到最优。判别器学习非常快，可以很好地区分假和真，并且正如预期的那样，没有提供可靠的梯度信息。然而，critic无法饱和，并收敛到一个线性函数，该函数在任何地方都给出了非常干净的渐变。我们约束权重的事实限制了函数在空间的不同部分最多是线性的，迫使最优critic具有这种行为。
<img src="/2025/03/10/WGAN/WGAN\4.png" alt="image"></p>
<p>也许更重要的是，可以训练critic直到最优，这使得这样做时不会发生collapse
modes。这是因为collapse
modes来自这样一个事实，即固定判别器的最佳生成器是判别器分配最高值的点的增量之和，如[4]所观察到并在[11]中突出显示的那样。</p>
<h2 id="实验结果">实验结果</h2>
<p>使用Wasserstein-GAN算法运行图像生成实验，并表明使用它比标准GAN中使用的公式有很大的实际好处</p>
<p>声明主要有两个好处 1.
一个与生成器的收敛性和样本质量相关的有意义的损失度量 2.
提高了优化过程的稳定性</p>
<h3 id="实验设置">实验设置</h3>
<p>作者对图像生成进行实验。要学习的目标分布是LSUN-Bedrooms数据集[24]
--一个室内卧室自然图像的集合。baseline比较是DCGAN[18]，这是一个卷积结构的GAN，使用标准的GAN程序使用<span class="math inline">\(-log
D\)</span>技巧[4]进行训练。生成的样本为3通道，大小为64×64像素的图像。我们在所有的实验中都使用了算法1中规定的超参数。</p>
<h3 id="有意义的损失度量">有意义的损失度量</h3>
<p>由于WGAN算法试图在每个生成器更新(算法1中的第10行)之前相对较好地训练critic
<span class="math inline">\(f\)</span>(算法1中第2-8行)，此时的损失函数是EM距离的估计，直到与我们约束f的Lipschitz常数的方式相关的常数。</p>
<p>我们的第一个实验说明了这种估计如何与生成样本的质量很好地相关。除了卷积DCGAN架构，我们还进行了实验，使用512个隐藏单元的4层ReLU-MLP替换生成器或同时替换生成器和critic。</p>
<p>图3为3种架构的WGAN训练过程中EM距离的WGAN估计(3)的演化情况。从图中可以清楚地看出，这些曲线与生成样本的视觉质量有很好的相关性。
<img src="/2025/03/10/WGAN/WGAN\5.png" alt="image"></p>
<p>据我们所知，这是GAN文献中第一次显示这样的属性，GAN的损失显示了收敛的属性。在对抗网络中进行研究时，此属性非常有用，因为人们不需要盯着生成的样本来找出故障模式，也不需要获得哪些模型比其他模型做得更好的信息。</p>
<p>然而，我们并不认为这是一种定量评估生成模型的新方法。依赖于critic架构的恒定比例因子意味着很难比较不同critic的模型。更重要的是，在实践中critic没有无限能力的事实使得我们很难知道我们估计的EM距离究竟有多接近。也就是说，我们成功地使用了损失度量来反复验证我们的实验，并且没有失败，我们认为这是对以前没有这种设施的训练GAN的一个巨大的改进。</p>
<p>相反，图4描绘了GAN训练过程中JS距离的GAN估计的演变。更准确地说，在GAN训练过程中，对判别器进行训练最大化
<span class="math display">\[
\left.\left.L\left(D, g_\theta\right)=\mathbb{E}_{x \sim \mathbb{P}_r}
\log D(x)\right]+\mathbb{E}_{x \sim \mathbb{P}_\theta} \log
(1-D(x))\right]
\]</span></p>
<p>这是<span class="math inline">\(2 J S\left(\mathbb{P}_r,
\mathbb{P}_\theta\right)-2 \log
2\)</span>的下界。在图中，我们画出了量<span class="math inline">\(\frac{1}{2} L\left(D, g_\theta\right)+\log
2\)</span>，这是JS距离的一个下界 <img src="/2025/03/10/WGAN/WGAN\6.png" alt="image"></p>
<p>这个数量显然与样品质量相关性很差。还要注意，JS估计值通常保持不变，或者上升而不是下降。事实上，它通常非常接近<span class="math inline">\(log
2≈0.69\)</span>，这是JS距离的最高值。换句话说，JS距离饱和，鉴别器无损耗，生成的样本在某些情况下是有意义的（DCGAN生成器，右上图），在其他情况下会折叠成一个无意义的图像[4]。最后一个现象在[1]中得到了理论解释，并在[11]中得到了强调。</p>
<p>当使用<span class="math inline">\(- log
D\)</span>技巧[4]时，判别器损失和生成器损失是不同的。附录E中的图8报告了同样的图用于GAN训练，但是使用生成器损失代替判别器损失。这并不改变结论。</p>
<p>最后，作为一个负面的结果，当一个人使用基于动量的优化器(如Adam [8] (
<span class="math inline">\(\beta_1&gt;0\)</span> )
)时，或者当一个人使用高学习率时，WGAN训练变得不稳定。由于critic的损失是非平稳的，基于动量的方法似乎表现得更差。我们认为动量是一个潜在的原因，因为随着损失的增加和样本的恶化，Adam步和梯度之间的余弦通常会变成负值。这个余弦为负的唯一地方是在这些不稳定的情况下。因此，我们切换到RMSprop算法[21]，即使在非平稳问题RMSprop算法也表现良好[13]。</p>
<h3 id="提高了稳定性">提高了稳定性</h3>
<p>WGAN的一个好处是它允许我们训练critic直到最优性。当critic训练完毕时，它只是给生成器提供了一个损失，我们可以像任何其他神经网络一样进行训练。这告诉我们，我们不再需要适当地平衡生成器和判别器的容量。critic越好，我们用来训练生成器的梯度质量越高。</p>
<p>我们观察到，当生成器的结构选择发生变化时，WGANs比GANs更加稳定。我们通过在三个生成器架构上运行实验来说明这一点：(1)一个卷积DCGAN生成器，(2)一个没有批量归一化和具有固定数量过滤器的卷积DCGAN生成器，(3)一个具有512个隐藏单元的4层ReLU-MLP。最后两个已知在GANs中表现很差。我们为WGAN
critic或GAN判别器保留了卷积DCGAN架构</p>
<p>图5、图6和图7显示了同时使用WGAN和GAN算法为这三种架构生成的样本。我们将生成的样本的完整片参考附录F。样品未经过Cherry-Picked处理。</p>
<p>在没任何实验中我们都未看到WGAN算法发生模式崩溃的证据 <img src="/2025/03/10/WGAN/WGAN\7.png" alt="image">
图5：使用DCGAN生成器训练的算法。左：Wgan算法。右：标准GAN配方。两种算法都产生了高质量的样本
<img src="/2025/03/10/WGAN/WGAN\8.png" alt="image">
图6：算法用一个没有批归一化的生成器和每层固定数量的过滤器进行训练。除了去掉批量归一化，参数的数量也因此减少了一个数量级以上。左：Wgan算法。右：标准GAN配方。正如我们可以看到标准的GAN没有学习到，而WGAN仍然能够产生样本。
<img src="/2025/03/10/WGAN/WGAN\9.png" alt="image">
图7：用4层512单元ReLU非线性的MLP生成器训练的算法。参数数量与DCGAN类似，但对图像生成缺乏较强的诱导偏差。左：Wgan算法。右：标准GAN配方。WGAN方法仍然能够产生样品，质量低于DCGAN，质量高于标准GAN的MLP。注意到GAN
MLP中模式崩溃的显著程度 ## 结论</p>
<p>引入了WGAN算法，它是传统GAN训练的一种替代。在这个新的模型中，表明可以提高学习的稳定性，摆脱模式崩溃等问题，并提供有意义的学习曲线，用于调试和超参数搜索。此外，文章表明，相应的优化问题是健全的，并提供了广泛的理论工作，突出了与其他距离分布之间的深层联系。</p>
]]></content>
      <categories>
        <category>论文精读</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>图像处理的偏微分方程方法总结</title>
    <url>/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>总结一下王大凯老师的《图像处理的偏微分方程方法》 <span id="more"></span></p>
<h2 id="变分法和梯度下降流">变分法和梯度下降流</h2>
<h3 id="变分原理">变分原理</h3>
<p>所希望的解往往是由最小化某一能量泛函所确定的，在一维情况下，这一泛函可能有如下形式：</p>
<p><span class="math display">\[
E(u)=\int_{x_0}^{x_1}F(x,u,u_x)\,dx  \qquad (1)
\]</span></p>
<p><span class="math inline">\(E(u)\)</span>的极值对应于变分<span class="math inline">\(\frac{\partial E}{\partial
u}=0\)</span>所对应的函数。为了求出一阶变分<span class="math inline">\(E&#39;\)</span>，考虑对最优解<span class="math inline">\(u(x)\)</span>作一微扰，得<span class="math inline">\(u(x)+v(x)\)</span>，经推导有</p>
<p><span class="math display">\[
E(u+v)=E(u)+\int_{x_0}^{x_1}[v\frac{\partial F}{\partial
u}-v\frac{d}{dx}(\frac{\partial F}{\partial u&#39;})] \, dx \qquad(2)
\]</span></p>
<p>可见，当<span class="math inline">\(E(u)\)</span>达到极值，对<span class="math inline">\(u(x)\)</span>的任一足够小微扰<span class="math inline">\(v(x),E\)</span>的值不变，故有</p>
<p><span class="math display">\[
\frac{\partial F}{\partial u}-\frac{d}{dx}(\frac{\partial F}{\partial
u&#39;})=0
\]</span></p>
<p>此式称为变分问题式(1)的Euler方程。 二维情况：</p>
<p><span class="math display">\[
E(u)=\iint_\Omega F(x,y,u,u_x,u_y)dxdy
\]</span></p>
<p>对应的Euler方程为</p>
<p><span class="math display">\[
\frac{\partial F}{\partial u}-\frac{d}{dx}(\frac{\partial F}{\partial
u_x})-\frac{d}{dy}(\frac{\partial F}{\partial u_y})=0
\]</span></p>
<p>求解能量泛函极值问题归结为求解相应的Euler方程。</p>
<h3 id="梯度下降流">梯度下降流</h3>
<p>假定我们要求的解可随时间变化，即它可以表示为<span class="math inline">\(u(\cdot,t)\)</span>，并且这种随时间的变化总是使<span class="math inline">\(E(u(\cdot,t))\)</span>减小，那么<span class="math inline">\(u(\cdot,t)\)</span>应该怎样变化才能满足这一要求？以一维问题为例，令式（2）中的微扰项<span class="math inline">\(v(\cdot)\)</span>是由<span class="math inline">\(u(\cdot,t)\)</span>从<span class="math inline">\(t\)</span>到<span class="math inline">\(t+\Delta
t\)</span>所产生的改变量，即</p>
<p><span class="math display">\[
v=\frac{\partial u}{\partial t}\Delta t
\]</span></p>
<p>式（2）就可改写为</p>
<p><span class="math display">\[
E(\cdot,t+\Delta t)=E(\cdot ,t)+\Delta t \int^{x_1}_{x_0}\frac{\partial
u}{\partial t}[\frac{\partial F}{\partial u}-\frac{d}{dx}(\frac{\partial
F}{\partial u&#39;})]\, dx
\]</span></p>
<p>于是只要令</p>
<p><span class="math display">\[
\frac{\partial u}{\partial t}=-[\frac{\partial F}{\partial
u}-\frac{d}{dx}(\frac{\partial F}{\partial
u&#39;})]=\frac{d}{dx}(\frac{\partial F}{\partial
u&#39;})-\frac{\partial F}{\partial u} \qquad(3)
\]</span></p>
<p>就可以使<span class="math inline">\(E(u(\cdot,t))\)</span>不断减小，式（3）称为变分问题式（1）所对应的梯度下降流。</p>
<p>这样一来，我们可以从某一适当选定的初始函数<span class="math inline">\(u_0\)</span>开始，根据式（3）作迭代计算，直到<span class="math inline">\(u\)</span>达到稳态解为止。这时 <span class="math display">\[
\frac{\partial u}{\partial t}=0 \Rightarrow \frac{\partial F}{\partial
u}-\frac{\mathrm{d}}{\mathrm{~d} x}\left(\frac{\partial F}{\partial
u^{\prime}}\right)=0
\]</span> 可见梯度下降流式（3）的稳态解也就是Euler方程式的解。</p>
<p>对于二维变分问题，类似推导，可得梯度下降流 <span class="math display">\[
\frac{\partial u}{\partial t}=\frac{\mathrm{d}}{\mathrm{~d}
x}\left(\frac{\partial F}{\partial
u_x}\right)+\frac{\mathrm{d}}{\mathrm{~d} y}\left(\frac{\partial
F}{\partial u_y}\right)-\frac{\partial F}{\partial u}
\]</span></p>
<p>值得特别注意的是：只有当<span class="math inline">\(E(u)\)</span>是凸性的，它有唯一极小值，从而梯度下降流可得到与初条件无关的唯一解。而当<span class="math inline">\(E(u)\)</span>非凸性时，梯度下降流可能由于选用不同的初条件<span class="math inline">\(u_0(x)\)</span>而得到不同的局部极小值而不是全局最小值。</p>
<h2 id="曲线演化问题">曲线演化问题</h2>
<h3 id="曲线几何演化的一般方程式">曲线几何演化的一般方程式</h3>
<p>曲线几何演化的一般方程式 <span class="math display">\[
\frac{\partial C(p, t)}{\partial t}=\boldsymbol{V}=\alpha(p, t)
\boldsymbol{T}+\beta(p, t) \boldsymbol{N}, \quad C(p, 0)=C_0(p) \quad(4)
\]</span> 式中<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>分别为切向速率和法向速率。</p>
<p>曲线演化的一般方程式可以简化为 <span class="math display">\[
\frac{\partial C}{\partial t}=\beta \boldsymbol{N} \qquad(5)
\]</span></p>
<h2 id="水平集方法">水平集方法</h2>
<h3 id="基本概念">基本概念</h3>
<p>一条平面封闭曲线可以采用隐式表达方法，即将它定义为一个二维函数<span class="math inline">\(u(x,y)\)</span>的水平集 <span class="math display">\[
C=\lbrace (x,y),u(x,y)=c \rbrace
\]</span></p>
<p>这样，<span class="math inline">\(C\)</span>有某种变化，则我们可以将它归结为是由函数<span class="math inline">\(u(x,y)\)</span>发生了某种相应的变化所引起的。随时间变化的封闭曲线，可表达为随时间变化的二维函数<span class="math inline">\(u(x,y)\)</span>水平集，即 <span class="math display">\[
C(t):=\lbrace (x,y),u(x,y,t)=c \rbrace
\]</span></p>
<p>当曲线<span class="math inline">\(C(t)\)</span>按（4）式演化时，嵌入函数<span class="math inline">\(u(x,y,t)\)</span>应如何演化呢？对上式中的函数<span class="math inline">\(u\)</span>求全导数<span class="math inline">\(\frac{du}{dt}\)</span>，由复合函数求导的链式规则可得
<span class="math display">\[
\frac{du}{dt}=\frac{\partial u}{\partial t}+\nabla u \cdot
\frac{\partial(x,y)}{\partial t}=0
\]</span></p>
<p>考虑到式（4），<span class="math inline">\(\frac{\partial(x,y)}{\partial t}=\frac{\partial
C}{\partial t}=\boldsymbol{V}\)</span>,于是得 <span class="math display">\[
\frac{\partial u}{\partial t}=-\nabla u \cdot \boldsymbol{V}=-|\nabla u|
\frac{\nabla u}{|\nabla u|} \cdot \boldsymbol{V}=|\nabla u|
\boldsymbol{N} \cdot \boldsymbol{V}=\beta|\nabla u|  \qquad(6)
\]</span> 式中，<span class="math inline">\(\beta=\boldsymbol{v}\cdot
\boldsymbol{N}\)</span>是运动速度的法向分量。式（6）就是曲线演化水平集方法的基本方程式。</p>
<p>式（6）的推导与常数<span class="math inline">\(c\)</span>的取值无关，为了方便，常取<span class="math inline">\(c=0\)</span>，即我们关心的曲线是嵌入函数的零水平集。</p>
<p>至此，我们看到，对于封闭曲线<span class="math inline">\(C\)</span>在给定的初值<span class="math inline">\(C_0\)</span>条件下，按式（5）演化问题，等价于嵌入函数<span class="math inline">\(u(x,y)\)</span>在给定初值<span class="math inline">\(u_0(x,y)\)</span>条件下按（6）的演化，也就是说，只要在任何时刻<span class="math inline">\(t\)</span>取出<span class="math inline">\(u(x,y,t)=0\)</span>的水平集就可以确定当前曲线<span class="math inline">\(C(t)\)</span></p>
<h3 id="水平集方法的优点">水平集方法的优点</h3>
<p>第一，水平集方法的PDE是直接在固定坐标系<span class="math inline">\((x,y)\)</span>中给出的，因而是一种无参数的方法</p>
<p>第二，可采用迎风方案作数值实现，以得到粘滞解</p>
<p>第三，曲线在演化过程中可能发生拓扑变化，例如，一条封闭曲线演变为两条封闭曲线。如果采用标注质点法，则要求随时监测这种可能的变化。但是对于水平集方法而言，<span class="math inline">\(u(x,y,t)\)</span>只有数值上的变化而没有拓扑上的变化。曲线在拓扑上的任何变化，都将自动嵌入到<span class="math inline">\(u(x,y,t)\)</span>的数值变化之中。因此没有跟踪曲线拓扑变化和修改实现方案的必要。</p>
<h2 id="变分水平集方法">变分水平集方法</h2>
<h3 id="基本概念-1">基本概念</h3>
<p>在将曲线演化应用于图像处理问题时，曲线运动方程往往来自于最小化闭合曲线<span class="math inline">\(C\)</span>的某一“能量”泛函。例如，测地线活动轮廓模型就是最小化如下泛函：
<span class="math display">\[
E(C)=\oint_C g(C) \, ds \qquad(7)
\]</span></p>
<p>式中<span class="math inline">\(g(x,y)ds\)</span>是“加权弧长微元”。可以证明，式（7）的梯度下降流为
<span class="math display">\[
\frac{\partial C}{\partial t}=[g(C)\kappa-\nabla g \cdot
\boldsymbol{N}]\boldsymbol{N} \qquad (8)
\]</span> 采用前面讨论的水平集方法，则对应的关于嵌入函数的PDE为 <span class="math display">\[
\begin{aligned}
\frac{\partial u}{\partial t} &amp; =(g \kappa-\nabla g \cdot
\boldsymbol{N})|\nabla u| \\
&amp; =|\nabla u| \operatorname{div}\left(g \frac{\nabla u}{|\nabla
u|}\right)
\end{aligned} \qquad(9)
\]</span></p>
<p>针对这类由曲线的能量泛函最小化所导出的曲线演化问题，有一种新的水平集方法，称之为变分水平集方法。</p>
<p>首先，利用如下定义的特殊函数（Heaviside函数）： <span class="math display">\[
H(z)= \begin{cases}1, &amp; z \geqslant 0 \\ 0, &amp; z&lt;0\end{cases}
\]</span></p>
<p>可将关于沿<span class="math inline">\(C\)</span>的环路积分式（7）在形式上改写为面积分
<span class="math display">\[
\oint_C g(C) \mathrm{d} s=\iint_{\Omega} g(x, y)|\nabla H(u)| \mathrm{d}
x \mathrm{~d} y
\]</span></p>
<p>由于 <span class="math display">\[
\nabla H(u)=\delta(u) \nabla u, \quad \delta(z)=\frac{\mathrm{d}
H(z)}{\mathrm{d} z}
\]</span> 这样一来，式（7）便可改写为嵌入函数<span class="math inline">\(u\)</span>的泛函 <span class="math display">\[
E(u)=\iint_\Omega g(x, y) \delta(u)|\nabla u| \mathrm{d} x \mathrm{~d} y
\qquad(7&#39;)
\]</span> 利用变分法，可以得到上式的梯度下降流 <span class="math display">\[
\frac{\partial u}{\partial t}=\delta(u) \operatorname{div}\left(g
\frac{\nabla u}{|\nabla u|}\right) \qquad(10)
\]</span> 为了使它成为可实际计算的PDE，式中的<span class="math inline">\(\delta\)</span>函数需用正则化的<span class="math inline">\(\delta_{\epsilon}\)</span>作近似，即将（10）改写为
<span class="math display">\[
\frac{\partial u}{\partial t}=\delta_{\varepsilon}(u)
\operatorname{div}\left(g \frac{\nabla u}{|\nabla u|}\right) \qquad(11)
\]</span> 式中 <span class="math display">\[
\delta_{\varepsilon}(z):=\frac{\mathrm{d}}{\mathrm{~d} z} H_\epsilon(z)
\]</span> 这里，<span class="math inline">\(H_{\epsilon}(z)\)</span>称为正则化的Heaviside函数。原则上它可以是任意满足如下条件：
<span class="math display">\[
H_{\varepsilon}(z) \xrightarrow{\varepsilon \rightarrow 0} H(z)
\]</span> 的函数。</p>
<p>从表面上看，PDE式（11）与（9）似乎差别不大，只是用<span class="math inline">\(\delta_{\epsilon}(u)\)</span>取代了<span class="math inline">\(|\nabla
u|\)</span>。但是，两者在数学上有本质的差别。式（9）属于双曲型，式（11）属于抛物型，稳定性较前者高。因而在数值实现时，可用较大的时间步长，并且常常无需对嵌入函数进行重新初始化。</p>
<p>但是，这并不意味变分水平集方法就可取代水平集。因为能够采用变分水平集方法来求解曲线演化问题的前提是：该问题是来自最小化曲线<span class="math inline">\(C\)</span>的“能量”泛函<span class="math inline">\(E(u)\)</span>。这时通过引入嵌入函数<span class="math inline">\(u\)</span>和利用Heaviside函数，将<span class="math inline">\(E(C)\)</span>改造成<span class="math inline">\(E(u)\)</span>，通过变分法得到关于<span class="math inline">\(u\)</span>的PDE。而水平集方法是：先利用变分法最小化<span class="math inline">\(C\)</span>的“能量”泛函，得到关于<span class="math inline">\(C\)</span>的运动方程后，再引入嵌入函数，得到关于<span class="math inline">\(u\)</span>的PDE。问题在于，并不是所有的曲线和曲面的演化问题都是由“能量”泛函最小化而导出的。水平集方法是较变分水平集方法适用面更广的方法。</p>
<h2 id="曲线演化的线性热流">曲线演化的线性热流</h2>
<h3 id="线性几何热流">线性几何热流</h3>
<p>考虑一条简单封闭平面曲线</p>
<p><span class="math display">\[
C_0(p)=(x_0(p),y_0(p))
\]</span></p>
<p>为初始条件，按照热方程演化</p>
<p><span class="math display">\[
\frac{\partial C}{\partial t}=\frac{\partial^2 C}{\partial p^2}
\qquad(12)
\]</span></p>
<p>得到曲线族</p>
<p><span class="math display">\[
C(p,t)=(x(p,t),y(p,t)), \qquad C(p,0)=C_0(p)
\]</span></p>
<p>用Fourier方法求解得到</p>
<p><span class="math display">\[
x(p,t)=x_0(p)*g(p,t),\qquad y(p,t)=y_0(p)*g(p,t)
\\g(p,t)=\frac{1}{\sqrt{4\pi t}}\exp[\frac{-p^2}{4t}]
\]</span></p>
<p>由此可见，曲线按式(12)做线性热运动，等价于对曲线上的每一点坐标<span class="math inline">\((x,y)\)</span>同时作Gaussian滤波。并且Gaussian滤波器的标准偏离<span class="math inline">\(\sigma\)</span>与演化时间<span class="math inline">\(t\)</span>有以下对应关系：</p>
<p><span class="math display">\[
\sigma=\sqrt{2}t
\]</span></p>
<h2 id="非线性几何不变流">非线性几何不变流</h2>
<h3 id="euclidean不变流">Euclidean不变流</h3>
<p>线性热流式（12）提供了一个很好的平面曲线的多尺度表达，但它存在一个严重的不足之处，它不能保证一个简单的（不自相交的）闭合曲线在演化过程中始终保持为一条简单封闭曲线。</p>
<p>如果用曲线的Euclidean弧长<span class="math inline">\(s\)</span>取代参数<span class="math inline">\(p\)</span>，则式（12）变为</p>
<p><span class="math display">\[
\frac{\partial C}{\partial t}=\frac{\partial^2 C}{\partial s^2}
\qquad(13)
\]</span></p>
<p>考虑到<span class="math inline">\(C_s=T,C_{ss}=T_s=\kappa
N\)</span>，故式（13）可改写为</p>
<p><span class="math display">\[
\frac{\partial C}{\partial t}=\kappa N \qquad(14)
\]</span> 可见Euclidean几何不变流也就是（平均）曲率运动（MCM）方程。</p>
<p>如果令<span class="math inline">\(E(C)=\oint_C
g(C)\,ds\)</span>中的函数<span class="math inline">\(g(x,y)=1\)</span>，即能量泛函简化为 <span class="math display">\[
E(C)=\oint_C ds \qquad(15)
\]</span> 那么，最小化这一泛函的意义就是使闭合曲线<span class="math inline">\(C\)</span>的全弧长缩短。这时对应的梯度下降流式（8）中，令<span class="math inline">\(g=1,\nabla
g=0\)</span>，它将简化为式（14）。据此，也可将式（14）称为弧长缩短流。</p>
<p>可以从三个不同的出发点得到同一个曲线运动方程：
（1）集合论的形态学算子——中值滤波，在原盘结构元素半径<span class="math inline">\(r \to 0\)</span>的极限
（2）Euclidean不变几何流——以Euclidean弧长为参数的热流
（3）弧长缩短流——最小化式（15）的梯度下降流。</p>
<h3 id="仿射不变几何流">仿射不变几何流</h3>
<p>另一个有重要意义的曲线演化PDE是 <span class="math display">\[
\frac{\partial C}{\partial t}=\frac{\partial^2 C}{\partial v^2}
\qquad(16)
\]</span> 式中<span class="math inline">\(v\)</span>表示仿射弧长。有
<span class="math display">\[
C_{v v}=\kappa^{1 / 3} \boldsymbol{N}+f\left(\kappa, \kappa_v\right)
\boldsymbol{T} \\
f\left(\kappa, \kappa_v\right)=\frac{\mathrm{d}^2 s}{\mathrm{~d}
v^2}=\left(\kappa^{-1 / 3}\right)_v
\]</span>
考虑到运动速度的切线分量不影响曲线的几何形变，所以式（16）在几何上等价于
<span class="math display">\[
\frac{\partial C}{\partial t}=\kappa^{1 / 3} N \qquad(17)
\]</span>
此式称为仿射不变几何流。它将任何简单的非凸闭合形变为全凸的闭合曲线，在演化过程中，曲线逐渐平滑，不产生奇异性，也不发生自相交，因而它也生成一个尺度空间，称为仿射形态学尺度空间（AMSS）。</p>
<p>对（17）采用水平集方法，可以得到关于嵌入函数<span class="math inline">\(u\)</span>的PDE为 <span class="math display">\[
\frac{\partial u}{\partial t}=\kappa^{1 / 3}|\nabla u|=\left(u_{x x}
u_y^2-2 u_x u_y u_{x y}+u_{y y} u_x^2\right)^{1 / 3}
\]</span>
由于在此PDE中不含有分式，因而不会像MCM方程那样出现“被零除”的问题。</p>
<h2 id="测地线活动轮廓模型">测地线活动轮廓模型</h2>
<h3 id="活动轮廓模型的基本概念">活动轮廓模型的基本概念</h3>
<p>图像分割中的活动轮廓模型（active
contour）或“蛇”（snake）模型的基本思想是将图像分割问题归结为最小化一个封闭曲线<span class="math inline">\(C(p)\)</span>的“能量”泛函： <span class="math display">\[
E[C(p)]=\alpha \int_0^1\left|C_p(p)\right| \mathrm{d} p+\beta
\int_0^1\left|C_{p p}(p)\right|^2 \mathrm{~d} p-\lambda \int_0^1|\nabla
I[C(p)]| \mathrm{d} p \qquad(18)
\]</span> 式中第一项的积分是曲线的Euclidean弧长，第二项 <span class="math display">\[
\int_0^1\left|C_{p p}(p)\right|^2 \mathrm{~d}
p=\int_0^1|\kappa|^2\left(\frac{\mathrm{~d} p}{\mathrm{~d} s}\right)^2
\mathrm{~d} p
\]</span>
表示曲线“振荡”的能量。因而最小化这两项就是要求闭合曲线尽可能短并且尽可能光顺。但根据关于曲率运动演化性质的讨论可知，在短程化弧长的过程中，也将使曲线逐渐光顺，因而第二项可不必单独提出。如果图像中的对象与背景的分解处存在灰度值的较大差异，那么对象的轮廓就将形成明显的边缘，即图像的梯度模值<span class="math inline">\(|\nabla
I|\)</span>，在对象的边界将达到局部极大值。注意到第三项的符号为负，最小化<span class="math inline">\(E(C)\)</span>对应于最大化第三项，这就要求曲线<span class="math inline">\(C\)</span>尽可能落在<span class="math inline">\(|\nabla
I|\)</span>达到极大值的位置上。由此可见，“蛇”模型的基本出发点也是“基于边缘的”。由于这一模型的前提是<span class="math inline">\(C(p)\)</span>为封闭曲线，因而利用“蛇”模型分割时。将不会产生边缘断裂问题。</p>
<p>为了避免出现第三项的负号，可引入一个新的函数<span class="math inline">\(g(r),r \in
\mathbb{R}^+\)</span>。原则上，它可以时任何具有单调递减的非负函数，以保证<span class="math inline">\(|\nabla I|\)</span>的局部极大值对应于<span class="math inline">\(g(|\nabla
I|)\)</span>的局部极小值，从而使最大化<span class="math inline">\(\int^1_0 |\nabla I[C(p)]| \,
dp\)</span>等价于最小化<span class="math inline">\(\int g(\nabla
I[C(p)])\,
dp\)</span>。略去式（18）中的第二项，并按上述方法修改第三项，此模型便可改写成
<span class="math display">\[
E[C(p)]=\alpha_1 \int_0^1\left|C_p(p)\right| \mathrm{d} p+\alpha_2
\int_0^1 g(\nabla I[C(p)]) \mathrm{d} p \qquad(19)
\]</span></p>
<p>但是，式（19）仍然存在一个严重缺陷，即它不依赖于曲线<span class="math inline">\(C\)</span>的几何形状和位置，而且还依赖于曲线的参数<span class="math inline">\(p\)</span>。为了克服这一缺陷，提出了不含自由参数测地线活动轮廓（geodesic
active
contour，GAC）模型。GAC模型的提出是PDE方法在图像分割应用中的重大突破。</p>
<h3 id="gac模型的建立">GAC模型的建立</h3>
<p>文献[35]提出用最小化以下“能量”泛函来确定活动轮廓： <span class="math display">\[
L_R(C)=\int_0^{L(C)} g(|\nabla I[C(s)]|) \mathrm{d} s \qquad(20)
\]</span></p>
<p>式中<span class="math inline">\(L(C)\)</span>表示闭合曲线<span class="math inline">\(C\)</span>的弧长，而<span class="math inline">\(L_R
(C)\)</span>则是“加权弧长”。由于以上泛函是建立在曲线固有参数——弧长之上的，消除了经典蛇模型依赖自由参数的缺陷。</p>
<p>可以证明，最小化式（20）所对应的梯度下降流为 <span class="math display">\[
\frac{\partial C}{\partial t}=g(C) \kappa N-(\nabla g \cdot N) N
\qquad(21)
\]</span></p>
<h3 id="gac模型的行为分析">GAC模型的行为分析</h3>
<p>对（21）的行为作一定性分析。我们看到式（21）右边的前一项是平均曲率运动（MCM）乘以非负标量因子<span class="math inline">\(g(x,y)\)</span>，因而这一项与MCM的行为是一致的，即在曲率为正的局部，曲线向内部收缩，而在曲率为负的局部，曲线将向外扩张，曲线的总长度将逐渐缩短，并逐渐平滑（曲率过零点逐渐减少）。不过现在这一运动收到函数<span class="math inline">\(g(x,y)\)</span>的控制，即在平坦区，有 <span class="math display">\[
|\nabla I| \approx 0 \Rightarrow g \approx 1
\]</span> 曲线将完全按照曲率运动方程演化，但在图像边缘附近，则因为 <span class="math display">\[
|\nabla I| \gg K \Rightarrow g \approx 0
\]</span> 使第一项失去作用。</p>
<p>式（21）后一项的行为值得更仔细的考察。在图像平坦区，由于<span class="math inline">\(g \approx 1 \Rightarrow \nabla g \approx
0\)</span>，于是这一项基本失去作用。对于在图像边缘附近的情况，用图4.14来进行讨论。图4.14中的闭合曲线表示一个“对象”的边缘，假定其内部灰度值较外部低。由于梯度模值在边缘达到局部极大值，从而边缘函数<span class="math inline">\(g\)</span>达到局部极小值。在图4.14中分别用<span class="math inline">\(|\nabla I|\)</span>和<span class="math inline">\(g(|\nabla I|)\)</span>曲线表示。由于<span class="math inline">\(\nabla g\)</span>的方向总是指向<span class="math inline">\(g\)</span>增大的方向，故不论在物体的内部还是外部，<span class="math inline">\(\nabla
g\)</span>总是指向离开边缘的方向。现在假定曲线<span class="math inline">\(C(t)\)</span>已经运动到物体的边缘附近，按规定<span class="math inline">\(C(t)\)</span>的法方向<span class="math inline">\(N\)</span>总是指向曲线的内部。于是，如果当前曲线的位置是处在边界的外部，那么<span class="math inline">\(N\)</span>将与<span class="math inline">\(\nabla
g\)</span>方向相反，即<span class="math inline">\((\nabla g \cdot
N)\)</span>为负值，因而<span class="math inline">\(-(\nabla g \cdot
N)N\)</span>与<span class="math inline">\(N\)</span>相一致，可见这时第二项的作用是使<span class="math inline">\(C(t)\)</span>从边界外部。向更靠近边界的方向运动；反之，如果当前曲线的位置是处在边界的内部，那么，由于<span class="math inline">\(\nabla g\)</span>与<span class="math inline">\(N\)</span>相一致，即<span class="math inline">\(\nabla g \cdot N\)</span>取正值，故<span class="math inline">\(-(\nabla g \cdot N)N\)</span>将与<span class="math inline">\(N\)</span>方向相反，也就是说第二项的作用是使<span class="math inline">\(C(t)\)</span>从边界内部，向靠近边界的方向运动。由此可见，式（21）第二项的行为是将曲线<span class="math inline">\(C(t)\)</span>推向<span class="math inline">\(|\nabla I|\)</span>的局部极大值（也就是<span class="math inline">\(g(|\nabla
I|)\)</span>的局部极小值），并稳定平衡在<span class="math inline">\(|\nabla I|\)</span>的“屋脊”上（或者说，<span class="math inline">\(g|\nabla
I|\)</span>的“谷底”中）。但这种作用只有当<span class="math inline">\(C(t)\)</span>已经相当靠近边界，以致函数<span class="math inline">\(g(|\nabla I|)\)</span>有明显的梯度时。才能产生。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1.png" alt="image"></p>
<p>总结以上分析可知，曲线按照GAC模型（21）演化时，将受两种“力”的支配：其一是来自于曲线自身的几何形变——曲率运动，故称为内力。不过这种“力”的强弱受到由图像<span class="math inline">\(I(x,y)\)</span>的梯度所提供的标量场<span class="math inline">\(g(x,y)\)</span>的控制。在图像边缘附近，这种“力”将变得很小以至“停止”。所以也常将边缘函数<span class="math inline">\(g(|\nabla
I|)\)</span>称之为边缘停止函数。第二种“力”来自于<span class="math inline">\(g(x,y)\)</span>的梯度<span class="math inline">\(\nabla g\)</span>，由于<span class="math inline">\(g(x,y)=g(|\nabla I(x,y)|)\)</span>，故<span class="math inline">\(\nabla y\)</span>是由图像<span class="math inline">\(I(x,y)\)</span>产生的，所以第二项的力称为外力。它能使<span class="math inline">\(C\)</span>向着图像中对象的边缘靠近，并稳定在边缘上。</p>
<p>GAC模型存在一个严重局限性，即当图像中对象由较深的凹陷边界时，GAC模型可能使<span class="math inline">\(C(t)\)</span>停止在某一<span class="math inline">\(E(C)\)</span>局部极小值状态，它并不与对象的边界相一致。图4.15是一个简单的实例，图（a）表示初始曲线，图（b）表示演化达到稳态的结果。我们看到，在对象凹陷部分，曲率<span class="math inline">\(\kappa &lt;0\)</span>。若<span class="math inline">\(C(t)\)</span>靠近这一部分，<span class="math inline">\(C(t)\)</span>的这一局部的曲率也必将是负的。式（21）前一项的“力”将是使曲线向外<span class="math inline">\((-N)\)</span>运动；另一方面，这时<span class="math inline">\(C(t)\)</span>的实际位置距离物体的边界还较远，故式（21）的后一项几乎为零。于是<span class="math inline">\(C(t)\)</span>的这部分就将停止演化，形成一段接近于直线的线段。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/2.png" alt="image"></p>
<h2 id="无边缘活动轮廓模型">无边缘活动轮廓模型</h2>
<h3 id="模型的建立">模型的建立</h3>
<p>在图像中，对象与背景的区别也可能表现为平均灰度值的明显不同。图4.23是这类图像的一个例子。由于这类图像既没有明显的边缘，也缺乏明显的纹理特征，以上讨论的GAC模型将难以实现成功的分割。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/3.png" alt="image"></p>
<p>对图4.23所示的这类图形，如果我们能找到闭合曲线<span class="math inline">\(C\)</span>，它将全部图像划分为内部区和外部区两个分部<span class="math inline">\(\Omega_1\)</span>和<span class="math inline">\(\Omega_2\)</span>，使在<span class="math inline">\(\Omega_1\)</span>内的图像部分与在<span class="math inline">\(\Omega_2\)</span>的图像的平均灰度恰好反映出对象与背景之间的灰度平均值的差别，那么这一闭合曲线就可看成是对象的轮廓。基于这一思路，T.Chan和L.Vese提出了如下“能量”泛函：
<span class="math display">\[
E\left(c_1, c_2, C\right)=\mu \oint_C \mathrm{~d} s+\lambda_1
\iint_{\Omega_1}\left(I-c_1\right)^2 \mathrm{~d} x \mathrm{~d}
y+\lambda_2 \iint_{\Omega_2}\left(I-c_2\right)^2 \mathrm{~d} x
\mathrm{~d} y \qquad(22)
\]</span></p>
<p>它有三个宗量：标量<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>，以及曲线<span class="math inline">\(C\)</span>。其中第一项是<span class="math inline">\(C\)</span>的全弧长，第二和第三项分别是内部区和外部区的灰度值与标量<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>的平方误差，也就是实际图像与假定的“分片常数”图像之间的偏离。图4.24给出了同一图像，当<span class="math inline">\(C\)</span>处在四个不同状态时，式（22）中第二项(<span class="math inline">\(F_1\)</span>)和第三项(<span class="math inline">\(F_2\)</span>)的变化情况。只有当<span class="math inline">\(C\)</span>达到“正确”位置（图4.24(d)）时，这两项才能同时达到最小值。
<img src="/2025/03/03/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%81%8F%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/4.png" alt="image"></p>
<p>式（22）称为无边缘活动轮廓模型。也可按提出者姓名称之为C-V模型。实际上，其基本思想与传统的基于区域的图像分割方法是一致的。故也可称为测地线活动区域（geodesic
active region,GAR）模型。</p>
<p>采用变分水平集方法，先在式（22）引入Heaviside函数，将它修改为关于嵌入函数<span class="math inline">\(u\)</span>的泛函，即 <span class="math display">\[
\begin{aligned}
E\left(c_1, c_2, u\right)= &amp; \mu \iint_{\Omega} \delta(u)|\nabla u|
\mathrm{d} x \mathrm{~d} y \\
&amp; +\lambda_1 \iint_{\Omega}\left(I-c_1\right)^2 H(u) \mathrm{d} x
\mathrm{~d} y+\lambda_2 \iint_{\Omega}\left(I-c_2\right)^2[1-H(u)]
\mathrm{d} x \mathrm{~d} y
\end{aligned} \qquad(23)
\]</span></p>
<p>这样，在函数<span class="math inline">\(u\)</span>固定的条件下，相对<span class="math inline">\(c_1，c_2\)</span>最小化式（23），可得 <span class="math display">\[
c_i=\frac{\iint_{\Omega_i} I \mathrm{~d} x \mathrm{~d}
y}{\iint_{\Omega_i} \mathrm{~d} x \mathrm{~d} y}, i=1,2 \qquad(24)
\]</span></p>
<p>即<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>分别是输入图像<span class="math inline">\(I(x,y)\)</span>在<span class="math inline">\(\Omega_1\)</span>（当前曲线的内部）和<span class="math inline">\(\Omega_2\)</span>（当前曲线的外部）的平均值。</p>
<p>在<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>固定的条件下，相对于<span class="math inline">\(u\)</span>最小化式（23），则可得 <span class="math display">\[
\frac{\partial u}{\partial t}=\delta_{\varepsilon}\left[\mu
\operatorname{div}\left(\frac{\nabla u}{|\nabla
u|}\right)-\lambda_1\left(I-c_1\right)^2+\lambda_2\left(I-c_2\right)^2\right]  \qquad(25)
\]</span></p>
<p>于是根据C-V模型，通过方程式（24），（25）的联立，求稳态解，便得到分割结果。</p>
]]></content>
      <categories>
        <category>图书总结</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
</search>
